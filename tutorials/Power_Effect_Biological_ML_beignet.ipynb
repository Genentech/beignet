{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471a85f6",
   "metadata": {},
   "source": [
    "\n",
    "# Power & Effect Size Analysis for Biological ML with PyTorch + Lightning + `beignet`\n",
    "\n",
    "Deep learning metrics (loss, AUROC) don’t tell you whether your dataset/model can **reliably detect** biologically meaningful effects.  \n",
    "This notebook adds **effect sizes** and **power/sample-size** to your workflow so claims are reproducible and decision-relevant.\n",
    "\n",
    "**Datasets used (small/medium from `beignet`):**\n",
    "- `FreeSolvDataset` (regression; hydration free energy)\n",
    "- `ClinToxDataset` (binary classification; clinical trial toxicity)\n",
    "- `SKEMPIDataset` (regression; ΔΔG mutational effects)\n",
    "\n",
    "> If any field names differ (e.g., `X`, `y`), adjust in the marked cells below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647bc12",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, uncomment:\n",
    "# !pip install beignet torch pytorch-lightning torchmetrics scikit-learn pandas\n",
    "\n",
    "import torch, pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from lightning import LightningModule, Trainer, seed_everything\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "seed_everything(7)\n",
    "print(\"Torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8566a0",
   "metadata": {},
   "source": [
    "### Import `beignet` datasets & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Datasets (swap for others in your package if preferred)\n",
    "from beignet.datasets import FreeSolvDataset, ClinToxDataset, SKEMPIDataset\n",
    "\n",
    "# TorchMetrics-style wrappers for power/effect/sample size\n",
    "from beignet.metrics import (\n",
    "    CohensD, HedgesG, CramersV, PhiCoefficient,\n",
    "    TTestPower, CorrelationPower, ChiSquaredIndependencePower,\n",
    "    ProportionTwoSamplePower, ANOVAPower,\n",
    "    TTestSampleSize, CorrelationSampleSize, ChiSquaredIndependenceSampleSize,\n",
    "    ProportionTwoSampleSampleSize, ANOVASampleSize,\n",
    ")\n",
    "\n",
    "from torchmetrics.regression import MeanSquaredError, R2Score\n",
    "from torchmetrics.classification import Accuracy, AUROC, F1Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5abc38",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Why power/effect matters (biology-first)\n",
    "\n",
    "- **Effect size** (Cohen’s *d*, Hedges’ *g*, φ/Cramér’s *V*, Cohen’s *f/f²*): how *big* is a difference/association.\n",
    "- **Power**: probability to detect that effect at α (given *n*, noise).\n",
    "- **For DL**: plan **data needs** before training; **log** detectability alongside loss/AUC; **calibrate claims** to what your dataset can actually support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d00081d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2) Case A — **FreeSolv** (regression; hydration free energy)\n",
    "\n",
    "We’ll show correlation/t-test power and standardized effects on a compact regression task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eab9ad",
   "metadata": {},
   "source": [
    "### 2.1 Load & preprocess *(adjust field names if needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff883033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = FreeSolvDataset()\n",
    "\n",
    "# Adjust here if your dataset exposes different attributes\n",
    "X = torch.as_tensor(ds.X, dtype=torch.float32)   # [N, D]\n",
    "y = torch.as_tensor(ds.y, dtype=torch.float32)   # [N]\n",
    "\n",
    "# Standardize for stable training (optional)\n",
    "xsc = StandardScaler().fit(X.numpy())\n",
    "ysc = StandardScaler().fit(y[:, None].numpy())\n",
    "Xn = torch.from_numpy(xsc.transform(X.numpy())).float()\n",
    "yn = torch.from_numpy(ysc.transform(y[:, None].numpy()).squeeze(1)).float()\n",
    "\n",
    "train_idx, test_idx = train_test_split(torch.arange(len(Xn)), test_size=0.2, random_state=7)\n",
    "train_loader = DataLoader(TensorDataset(Xn[train_idx], yn[train_idx]), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xn[test_idx],  yn[test_idx]),  batch_size=128)\n",
    "\n",
    "Xn.shape, yn.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0356eac",
   "metadata": {},
   "source": [
    "### 2.2 Lightning model with **effect/power** logged next to loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitRegressor(LightningModule):\n",
    "    def __init__(self, in_dim, lr=1e-3, alpha=0.05):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.mse, self.r2 = MeanSquaredError(), R2Score()\n",
    "        self.d = CohensD()\n",
    "        self.power_t = TTestPower(alpha=alpha)\n",
    "        self.power_corr = CorrelationPower(alpha=alpha)\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = nn.functional.mse_loss(pred, y)\n",
    "        self.log(\"train/mse\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        self.log(\"val/mse\", self.mse(pred, y), prog_bar=True)\n",
    "        self.log(\"val/r2\",  self.r2(pred, y),  prog_bar=True)\n",
    "        # Effect size / power on predictions vs truth\n",
    "        self.log(\"val/cohens_d\", self.d(pred, y))\n",
    "        self.log(\"val/power_ttest\", self.power_t(pred, y))\n",
    "        self.log(\"val/power_corr\", self.power_corr(pred, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "model = LitRegressor(in_dim=Xn.shape[1])\n",
    "trainer = Trainer(max_epochs=10, log_every_n_steps=5, deterministic=True, enable_checkpointing=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6478ca",
   "metadata": {},
   "source": [
    "### 2.3 Plan **sample size** for a target correlation (threshold → detectability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose r >= 0.35 is \"biologically useful\"\n",
    "target_r, alpha = 0.35, 0.05\n",
    "ns = torch.arange(30, 401, 10)\n",
    "cp = CorrelationPower(alpha=alpha)\n",
    "\n",
    "powers = [float(cp(effect_size=torch.tensor(target_r), n=int(n))) for n in ns]\n",
    "pd.DataFrame({\"n\": ns.numpy(), \"power_at_r=0.35\": powers}).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6278cf8b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 3) Case B — **ClinTox** (binary classification; clinical trial toxicity)\n",
    "\n",
    "Demonstrate **χ² power** for association (truth vs predictions), **φ/Cramér’s V** effect size,\n",
    "and **two-sample proportion power** for prevalence differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9274f43",
   "metadata": {},
   "source": [
    "### 3.1 Load & split *(adjust field names if needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bff8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = ClinToxDataset()\n",
    "\n",
    "X = torch.as_tensor(ds.X, dtype=torch.float32)\n",
    "y = torch.as_tensor(ds.y, dtype=torch.long)  # 0/1 labels\n",
    "\n",
    "Xn = torch.from_numpy(StandardScaler().fit_transform(X.numpy())).float()\n",
    "tr, te = train_test_split(torch.arange(len(Xn)), test_size=0.2, stratify=y, random_state=7)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xn[tr], y[tr]), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xn[te], y[te]),  batch_size=128)\n",
    "\n",
    "Xn.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a0aa1",
   "metadata": {},
   "source": [
    "### 3.2 Lightning classifier with **χ² power** & **effect sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitClassifier(LightningModule):\n",
    "    def __init__(self, in_dim, lr=1e-3, alpha=0.05):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.acc, self.auroc, self.f1 = Accuracy(task=\"binary\"), AUROC(task=\"binary\"), F1Score(task=\"binary\")\n",
    "        self.chi_power = ChiSquaredIndependencePower(alpha=alpha)\n",
    "        self.cramersV, self.phi = CramersV(), PhiCoefficient()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def _contingency(self, logits, y, thr=0.5):\n",
    "        p = torch.sigmoid(logits)\n",
    "        pred = (p >= thr).long()\n",
    "        table = torch.zeros((2,2), device=logits.device)\n",
    "        for t, q in zip(y, pred):\n",
    "            table[int(t), int(q)] += 1\n",
    "        return pred, p, table\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logit = self(x)\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(logit, y.float())\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logit = self(x)\n",
    "        pred, p, table = self._contingency(logit, y)\n",
    "        self.log(\"val/acc\",   self.acc(pred, y),   prog_bar=True)\n",
    "        self.log(\"val/auroc\", self.auroc(p, y),    prog_bar=True)\n",
    "        self.log(\"val/f1\",    self.f1(pred, y))\n",
    "\n",
    "        # Association detectability and effect size:\n",
    "        self.log(\"val/chi_power\", self.chi_power(table))\n",
    "        self.log(\"val/cramers_v\", self.cramersV(table))\n",
    "        self.log(\"val/phi\",       self.phi(table))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "model_c = LitClassifier(in_dim=Xn.shape[1])\n",
    "trainer = Trainer(max_epochs=10, log_every_n_steps=5, deterministic=True, enable_checkpointing=False)\n",
    "trainer.fit(model_c, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff820c8",
   "metadata": {},
   "source": [
    "### 3.3 Two-sample **proportion power** for prevalence differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbc9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose positive-class prevalence differs between two subgroups by 0.10 (e.g., 0.20 vs 0.30)\n",
    "p1, p2, alpha = 0.20, 0.30, 0.05\n",
    "ns = torch.arange(50, 801, 25)\n",
    "pp = ProportionTwoSamplePower(alpha=alpha)\n",
    "powers = [float(pp(p1=torch.tensor(p1), p2=torch.tensor(p2), n1=int(n), n2=int(n))) for n in ns]\n",
    "\n",
    "pd.DataFrame({\"n_per_group\": ns.numpy(), \"power_at_diff=0.10\": powers}).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90791a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 4) Case C — **SKEMPI** (ΔΔG regression; mutational effects)\n",
    "\n",
    "Use **Cohen’s d / Hedges’ g** and **ANOVA power** across mutation classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a607caf",
   "metadata": {},
   "source": [
    "### 4.1 Load & split *(adjust field names if needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb92f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = SKEMPIDataset()\n",
    "\n",
    "X = torch.as_tensor(ds.X, dtype=torch.float32)\n",
    "y = torch.as_tensor(ds.y, dtype=torch.float32)  # ΔΔG (kcal/mol). Verify sign convention if needed.\n",
    "\n",
    "Xn = torch.from_numpy(StandardScaler().fit_transform(X.numpy())).float()\n",
    "yn = torch.from_numpy(StandardScaler().fit_transform(y[:, None].numpy()).squeeze(1)).float()\n",
    "\n",
    "tr, te = train_test_split(torch.arange(len(Xn)), test_size=0.2, random_state=7)\n",
    "train_loader = DataLoader(TensorDataset(Xn[tr], yn[tr]), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xn[te], yn[te]),  batch_size=128)\n",
    "\n",
    "Xn.shape, yn.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d39ce",
   "metadata": {},
   "source": [
    "### 4.2 Lightning regressor with grouped effects and **ANOVA power**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitRegressorGrouped(LightningModule):\n",
    "    def __init__(self, in_dim, lr=1e-3, alpha=0.05):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.mse, self.r2 = MeanSquaredError(), R2Score()\n",
    "        self.d, self.g = CohensD(), HedgesG()\n",
    "        self.anova_p = ANOVAPower(alpha=alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = nn.functional.mse_loss(pred, y)\n",
    "        self.log(\"train/mse\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        self.log(\"val/mse\", self.mse(pred, y), prog_bar=True)\n",
    "        self.log(\"val/r2\",  self.r2(pred, y),  prog_bar=True)\n",
    "        self.log(\"val/d\",   self.d(pred, y))\n",
    "        self.log(\"val/g\",   self.g(pred, y))\n",
    "\n",
    "        # If SKEMPI exposes true mutation categories, replace this tertile bucketing with ds.category[indices].\n",
    "        q = torch.quantile(y, torch.tensor([0.33, 0.66], device=y.device))\n",
    "        cats = torch.bucketize(y, q)  # 0,1,2 tertiles as a stand-in for classes\n",
    "\n",
    "        group_means = torch.stack([pred[cats==k].mean() for k in (0,1,2)])\n",
    "        group_vars  = torch.stack([pred[cats==k].var(unbiased=True) for k in (0,1,2)])\n",
    "        group_ns    = torch.tensor([int((cats==k).sum()) for k in (0,1,2)], device=pred.device)\n",
    "\n",
    "        try:\n",
    "            self.log(\"val/anova_power\", self.anova_p(group_means, group_vars, group_ns))\n",
    "        except TypeError:\n",
    "            # If ANOVAPower expects raw groups instead of summary stats, adapt here.\n",
    "            pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "model_s = LitRegressorGrouped(in_dim=Xn.shape[1])\n",
    "trainer = Trainer(max_epochs=10, log_every_n_steps=5, deterministic=True, enable_checkpointing=False)\n",
    "trainer.fit(model_s, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f2e61",
   "metadata": {},
   "source": [
    "### 4.3 Convert biological threshold → **required n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose ΔΔG = 0.5 kcal/mol is biologically meaningful.\n",
    "# Estimate Cohen's d from observed SD in original units:\n",
    "with torch.no_grad():\n",
    "    # NOTE: use original-scale y for this calculation\n",
    "    y_val = y[te]\n",
    "    sd = y_val.std()\n",
    "    d_eff = torch.tensor(0.5) / sd\n",
    "\n",
    "# Sample size for 80% power at α=0.05 using the Metric interface:\n",
    "n_needed = TTestSampleSize()(effect_size=d_eff, alpha=0.05, power=0.80)\n",
    "n_needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7375ca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5) Power-aware workflow (what to log & when)\n",
    "\n",
    "1. **Before training**: convert meaningful thresholds (ΔTm, ΔΔG, prevalence deltas) → **effect sizes**; compute **required n** with `*SampleSize` metrics.  \n",
    "2. **During validation**: log **effect sizes** and **power** alongside loss/AUC.  \n",
    "3. **After training**: frame claims like *“association is moderate (Cramér’s V≈0.3) and detectable (χ²-power≈0.86) at n=…”* rather than only AUROC/MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab8723",
   "metadata": {},
   "source": [
    "## 6) Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34215b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def power_learning_curve_regression(X, y, model_ctor, power_metric, ns=(50,100,150,200), repeats=3):\n",
    "    \"\"\"Subsampled n → power. power_metric(pred, y) should return a scalar tensor.\"\"\"\n",
    "    out, idx_all = [], torch.arange(len(X))\n",
    "    for n in ns:\n",
    "        for r in range(repeats):\n",
    "            idx = idx_all[torch.randperm(len(idx_all))[:n]]\n",
    "            tr, va = train_test_split(torch.arange(n), test_size=0.25, random_state=r)\n",
    "            train_loader = DataLoader(TensorDataset(X[idx][tr], y[idx][tr]), batch_size=64, shuffle=True)\n",
    "            val_loader   = DataLoader(TensorDataset(X[idx][va], y[idx][va]), batch_size=128)\n",
    "            model = model_ctor()\n",
    "            Trainer(max_epochs=5, logger=False, enable_checkpointing=False, deterministic=True).fit(model, train_loader, val_loader)\n",
    "            with torch.no_grad():\n",
    "                pred = model(val_loader.dataset.tensors[0])\n",
    "                pwr = float(power_metric(pred, val_loader.dataset.tensors[1]))\n",
    "            out.append({\"n\": int(n), \"repeat\": r, \"power\": pwr})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def threshold_to_d(delta_units, sd_units):\n",
    "    \"\"\"Convert a meaningful difference in original units (e.g., 2°C Tm, 0.5 kcal/mol ΔΔG) to Cohen's d.\"\"\"\n",
    "    return torch.tensor(delta_units, dtype=torch.float32) / torch.tensor(sd_units, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb7b29",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Reporting check-list\n",
    "\n",
    "- **Data adequacy**: “Power to detect r≥0.35 at n=240 is 0.82 (α=0.05).”  \n",
    "- **Effect sizes**: “φ=0.28 (weak–moderate), Cramér’s V=0.31 (moderate).”  \n",
    "- **Limits**: “Power < 0.8 for ΔΔG=0.3 kcal/mol; claims restricted to ≥0.5 kcal/mol shifts.”\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
