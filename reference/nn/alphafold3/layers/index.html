
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../../lightning/modules/">
      
      
        <link rel="next" href="../loss-functions/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Layers - Beignet</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#beignet.nn.AlphaFold3" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Beignet" class="md-header__button md-logo" aria-label="Beignet" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Beignet
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Layers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Beignet" class="md-nav__button md-logo" aria-label="Beignet" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Beignet
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    beignet
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            beignet
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Geometry
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1">
            <span class="md-nav__icon md-icon"></span>
            Geometry
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1_1" id="__nav_3_1_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_1_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1_1">
            <span class="md-nav__icon md-icon"></span>
            Transformations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/geometry/transformations/euler-angle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Euler angle
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/geometry/transformations/quaternion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quaternion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/geometry/transformations/rotation-matrix/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Rotation matrix
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/geometry/transformations/rotation-vector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Rotation vector
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/geometry/transformations/transform/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transform
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/geometry/transformations/translation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Translation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/integral-transforms.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Integral transforms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_3" >
        
          
          <label class="md-nav__link" for="__nav_3_1_3" id="__nav_3_1_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Numerical methods
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_3">
            <span class="md-nav__icon md-icon"></span>
            Numerical methods
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/numerical-methods/numerical-integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Numerical integration
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_4" >
        
          
          <label class="md-nav__link" for="__nav_3_1_4" id="__nav_3_1_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Special functions
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_4">
            <span class="md-nav__icon md-icon"></span>
            Special functions
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/special-functions/dawson-and-fresnel-integrals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dawson and Fresnel integrals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/special-functions/error-and-related-functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Error and related functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_4_3" >
        
          
          <label class="md-nav__link" for="__nav_3_1_4_3" id="__nav_3_1_4_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Orthogonal polynomials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_1_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_4_3">
            <span class="md-nav__icon md-icon"></span>
            Orthogonal polynomials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/special-functions/orthogonal-polynomials/polynomial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Polynomial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/special-functions/orthogonal-polynomials/chebyshev-polynomial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chebyshev polynomial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/special-functions/orthogonal-polynomials/physicists-hermite-polynomial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Physicists’ Hermite polynomial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/special-functions/orthogonal-polynomials/probabilists-hermite-polynomial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probabilists’ Hermite polynomial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/special-functions/orthogonal-polynomials/laguerre-polynomial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Laguerre polynomial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../operators/special-functions/orthogonal-polynomials/legendre-polynomial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Legendre polynomial
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../beignet.datasets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    beignet.datasets
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    beignet.diffusers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            beignet.diffusers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../diffusers/diffusion-pipelines/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Diffusion pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../diffusers/schedulers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedulers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    beignet.features
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            beignet.features
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../features/general-purpose/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    General-purpose
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_2" >
        
          
          <label class="md-nav__link" for="__nav_3_4_2" id="__nav_3_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Geometry
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4_2">
            <span class="md-nav__icon md-icon"></span>
            Geometry
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../features/geometry/transformations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    beignet.func
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            beignet.func
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../func/molecular-dynamics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Molecular dynamics
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    beignet.lightning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            beignet.lightning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lightning/modules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Modules
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" checked>
        
          
          <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    beignet.nn
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            beignet.nn
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_7_1" id="__nav_3_7_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AlphaFold 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_7_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_7_1">
            <span class="md-nav__icon md-icon"></span>
            AlphaFold 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Layers
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Layers
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.AlphaFold3" class="md-nav__link">
    <span class="md-ellipsis">
      AlphaFold3
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AlphaFold3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.AlphaFold3.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomAttentionDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      AtomAttentionDecoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AtomAttentionDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomAttentionDecoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomAttentionEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      AtomAttentionEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AtomAttentionEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomAttentionEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      AtomTransformer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AtomTransformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomTransformer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AttentionPairBias" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionPairBias
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AttentionPairBias">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AttentionPairBias.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.DiffusionTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      DiffusionTransformer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DiffusionTransformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.DiffusionTransformer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.MSA" class="md-nav__link">
    <span class="md-ellipsis">
      MSA
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.MSA.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.PairformerStack" class="md-nav__link">
    <span class="md-ellipsis">
      PairformerStack
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PairformerStack">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.PairformerStack.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.RelativePositionEncoding" class="md-nav__link">
    <span class="md-ellipsis">
      RelativePositionEncoding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RelativePositionEncoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.RelativePositionEncoding.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.SampleDiffusion" class="md-nav__link">
    <span class="md-ellipsis">
      SampleDiffusion
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SampleDiffusion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.SampleDiffusion.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TemplateEmbedder" class="md-nav__link">
    <span class="md-ellipsis">
      TemplateEmbedder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TemplateEmbedder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TemplateEmbedder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.Transition" class="md-nav__link">
    <span class="md-ellipsis">
      Transition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.Transition.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleAttentionEndingNode" class="md-nav__link">
    <span class="md-ellipsis">
      TriangleAttentionEndingNode
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TriangleAttentionEndingNode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleAttentionEndingNode.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleAttentionStartingNode" class="md-nav__link">
    <span class="md-ellipsis">
      TriangleAttentionStartingNode
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TriangleAttentionStartingNode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleAttentionStartingNode.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleMultiplicationIncoming" class="md-nav__link">
    <span class="md-ellipsis">
      TriangleMultiplicationIncoming
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TriangleMultiplicationIncoming">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleMultiplicationIncoming.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleMultiplicationOutgoing" class="md-nav__link">
    <span class="md-ellipsis">
      TriangleMultiplicationOutgoing
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TriangleMultiplicationOutgoing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleMultiplicationOutgoing.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loss-functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Loss functions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../beignet.io/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    beignet.io
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
        
          
          <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    beignet.transforms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_9">
            <span class="md-nav__icon md-icon"></span>
            beignet.transforms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transforms/general-purpose/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    General-purpose
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.AlphaFold3" class="md-nav__link">
    <span class="md-ellipsis">
      AlphaFold3
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AlphaFold3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.AlphaFold3.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomAttentionDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      AtomAttentionDecoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AtomAttentionDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomAttentionDecoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomAttentionEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      AtomAttentionEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AtomAttentionEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomAttentionEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      AtomTransformer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AtomTransformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AtomTransformer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AttentionPairBias" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionPairBias
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AttentionPairBias">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.AttentionPairBias.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.DiffusionTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      DiffusionTransformer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DiffusionTransformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.DiffusionTransformer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.MSA" class="md-nav__link">
    <span class="md-ellipsis">
      MSA
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MSA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.MSA.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.PairformerStack" class="md-nav__link">
    <span class="md-ellipsis">
      PairformerStack
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PairformerStack">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.PairformerStack.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.RelativePositionEncoding" class="md-nav__link">
    <span class="md-ellipsis">
      RelativePositionEncoding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RelativePositionEncoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.RelativePositionEncoding.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.SampleDiffusion" class="md-nav__link">
    <span class="md-ellipsis">
      SampleDiffusion
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SampleDiffusion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.SampleDiffusion.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TemplateEmbedder" class="md-nav__link">
    <span class="md-ellipsis">
      TemplateEmbedder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TemplateEmbedder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TemplateEmbedder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.Transition" class="md-nav__link">
    <span class="md-ellipsis">
      Transition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.Transition.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleAttentionEndingNode" class="md-nav__link">
    <span class="md-ellipsis">
      TriangleAttentionEndingNode
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TriangleAttentionEndingNode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleAttentionEndingNode.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleAttentionStartingNode" class="md-nav__link">
    <span class="md-ellipsis">
      TriangleAttentionStartingNode
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TriangleAttentionStartingNode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleAttentionStartingNode.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleMultiplicationIncoming" class="md-nav__link">
    <span class="md-ellipsis">
      TriangleMultiplicationIncoming
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TriangleMultiplicationIncoming">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleMultiplicationIncoming.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleMultiplicationOutgoing" class="md-nav__link">
    <span class="md-ellipsis">
      TriangleMultiplicationOutgoing
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TriangleMultiplicationOutgoing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beignet.nn.alphafold3.TriangleMultiplicationOutgoing.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Layers</h1>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.AlphaFold3" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.AlphaFold3</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Main Inference Loop for AlphaFold 3.</p>
<p>This module implements Algorithm 1 exactly, which is the main inference
pipeline for AlphaFold 3. It processes input features through multiple
stages including feature embedding, MSA processing, template embedding,
Pairformer stacks, diffusion sampling, and confidence prediction.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_cycle</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of recycling cycles</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_s</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Single representation dimension</p>
              </div>
            </td>
            <td>
                  <code>384</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_z</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair representation dimension</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_m</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>MSA representation dimension</p>
              </div>
            </td>
            <td>
                  <code>64</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_template</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Template feature dimension</p>
              </div>
            </td>
            <td>
                  <code>64</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_blocks_pairformer</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of blocks in PairformerStack</p>
              </div>
            </td>
            <td>
                  <code>48</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                  <code>16</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">AlphaFold3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">AlphaFold3</span><span class="p">(</span><span class="n">n_cycle</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Smaller for example</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_star</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;asym_id&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="s1">&#39;residue_index&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;entity_id&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="s1">&#39;token_index&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;sym_id&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="s1">&#39;token_bonds&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">f_star</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;x_pred&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 64, 3])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 Algorithm 1: Main Inference Loop</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_alphafold3.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AlphaFold3</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Main Inference Loop for AlphaFold 3.</span>

<span class="sd">    This module implements Algorithm 1 exactly, which is the main inference</span>
<span class="sd">    pipeline for AlphaFold 3. It processes input features through multiple</span>
<span class="sd">    stages including feature embedding, MSA processing, template embedding,</span>
<span class="sd">    Pairformer stacks, diffusion sampling, and confidence prediction.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_cycle : int, default=4</span>
<span class="sd">        Number of recycling cycles</span>
<span class="sd">    c_s : int, default=384</span>
<span class="sd">        Single representation dimension</span>
<span class="sd">    c_z : int, default=128</span>
<span class="sd">        Pair representation dimension</span>
<span class="sd">    c_m : int, default=64</span>
<span class="sd">        MSA representation dimension</span>
<span class="sd">    c_template : int, default=64</span>
<span class="sd">        Template feature dimension</span>
<span class="sd">    n_blocks_pairformer : int, default=48</span>
<span class="sd">        Number of blocks in PairformerStack</span>
<span class="sd">    n_head : int, default=16</span>
<span class="sd">        Number of attention heads</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import AlphaFold3</span>
<span class="sd">    &gt;&gt;&gt; batch_size, n_tokens = 2, 64</span>
<span class="sd">    &gt;&gt;&gt; module = AlphaFold3(n_cycle=2)  # Smaller for example</span>
<span class="sd">    &gt;&gt;&gt; f_star = {</span>
<span class="sd">    ...     &#39;asym_id&#39;: torch.randint(0, 5, (batch_size, n_tokens)),</span>
<span class="sd">    ...     &#39;residue_index&#39;: torch.arange(n_tokens).unsqueeze(0).expand(batch_size, -1),</span>
<span class="sd">    ...     &#39;entity_id&#39;: torch.randint(0, 3, (batch_size, n_tokens)),</span>
<span class="sd">    ...     &#39;token_index&#39;: torch.arange(n_tokens).unsqueeze(0).expand(batch_size, -1),</span>
<span class="sd">    ...     &#39;sym_id&#39;: torch.randint(0, 10, (batch_size, n_tokens)),</span>
<span class="sd">    ...     &#39;token_bonds&#39;: torch.randn(batch_size, n_tokens, n_tokens, 32)</span>
<span class="sd">    ... }</span>
<span class="sd">    &gt;&gt;&gt; outputs = module(f_star)</span>
<span class="sd">    &gt;&gt;&gt; outputs[&#39;x_pred&#39;].shape</span>
<span class="sd">    torch.Size([2, 64, 3])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 Algorithm 1: Main Inference Loop</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_cycle</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">c_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">384</span><span class="p">,</span>
        <span class="n">c_z</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">c_m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">c_template</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">n_blocks_pairformer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span><span class="p">,</span>
        <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_cycle</span> <span class="o">=</span> <span class="n">n_cycle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_s</span> <span class="o">=</span> <span class="n">c_s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span> <span class="o">=</span> <span class="n">c_z</span>

        <span class="c1"># Step 1: Input Feature Embedder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_feature_embedder</span> <span class="o">=</span> <span class="n">_InputFeatureEmbedder</span><span class="p">(</span>
            <span class="n">c_atom</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">c_atompair</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
            <span class="n">c_token</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 2-3: Linear projections for initial representations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># s_i^init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_linear_i</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># z_ij^init from s_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_linear_j</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># z_ij^init from s_j</span>

        <span class="c1"># Step 4: Relative Position Encoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_encoding</span> <span class="o">=</span> <span class="n">RelativePositionEncoding</span><span class="p">(</span>
            <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 5: Token bonds projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_bonds_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="mi">32</span><span class="p">,</span> <span class="n">c_z</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>  <span class="c1"># Assuming 32 bond features</span>

        <span class="c1"># Step 8: Layer norm for pair initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_z</span><span class="p">)</span>

        <span class="c1"># Step 9: Template Embedder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">template_embedder</span> <span class="o">=</span> <span class="n">TemplateEmbedder</span><span class="p">(</span>
            <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
            <span class="n">c_template</span><span class="o">=</span><span class="n">c_template</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 10: MSA Module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msa_module</span> <span class="o">=</span> <span class="n">MSA</span><span class="p">(</span>
            <span class="n">c_m</span><span class="o">=</span><span class="n">c_m</span><span class="p">,</span>
            <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
            <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 11: Single representation update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_update_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_s</span><span class="p">)</span>

        <span class="c1"># Step 12: Pairformer Stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pairformer_stack</span> <span class="o">=</span> <span class="n">PairformerStack</span><span class="p">(</span>
            <span class="n">n_block</span><span class="o">=</span><span class="n">n_blocks_pairformer</span><span class="p">,</span>
            <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span>
            <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
            <span class="n">n_head_single</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">n_head_pair</span><span class="o">=</span><span class="n">n_head</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span>  <span class="c1"># Typically fewer heads for pair attention</span>
        <span class="p">)</span>

        <span class="c1"># Step 15: Sample Diffusion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_diffusion</span> <span class="o">=</span> <span class="n">SampleDiffusion</span><span class="p">()</span>

        <span class="c1"># Step 16: Confidence Head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">confidence_head</span> <span class="o">=</span> <span class="n">_Confidence</span><span class="p">(</span>
            <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span>
            <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 17: Distogram Head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distogram_head</span> <span class="o">=</span> <span class="n">_Distogram</span><span class="p">(</span>
            <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass implementing Algorithm 1 exactly.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        f_star : dict</span>
<span class="sd">            Dictionary containing input features with keys:</span>
<span class="sd">            - &#39;asym_id&#39;: asymmetric unit IDs (batch, n_tokens)</span>
<span class="sd">            - &#39;residue_index&#39;: residue indices (batch, n_tokens)</span>
<span class="sd">            - &#39;entity_id&#39;: entity IDs (batch, n_tokens)</span>
<span class="sd">            - &#39;token_index&#39;: token indices (batch, n_tokens)</span>
<span class="sd">            - &#39;sym_id&#39;: symmetry IDs (batch, n_tokens)</span>
<span class="sd">            - &#39;token_bonds&#39;: token bond features (batch, n_tokens, n_tokens, bond_dim)</span>
<span class="sd">            - Optional: &#39;template_features&#39;, &#39;msa_features&#39;, etc.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        outputs : dict</span>
<span class="sd">            Dictionary containing:</span>
<span class="sd">            - &#39;x_pred&#39;: predicted coordinates (batch, n_tokens, 3)</span>
<span class="sd">            - &#39;p_plddt&#39;: pLDDT confidence (batch, n_tokens)</span>
<span class="sd">            - &#39;p_pae&#39;: PAE confidence (batch, n_tokens, n_tokens)</span>
<span class="sd">            - &#39;p_pde&#39;: PDE confidence (batch, n_tokens, n_tokens)</span>
<span class="sd">            - &#39;p_resolved&#39;: resolved confidence (batch, n_tokens)</span>
<span class="sd">            - &#39;p_distogram&#39;: distance distributions (batch, n_tokens, n_tokens, n_bins)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Step 1: Input Feature Embedder</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_feature_embedder</span><span class="p">(</span><span class="n">f_star</span><span class="p">)</span>
        <span class="n">s_inputs</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;single&quot;</span><span class="p">]</span>  <span class="c1"># (batch, n_tokens, c_s)</span>

        <span class="c1"># Step 2: Initialize single representation</span>
        <span class="n">s_i_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_linear</span><span class="p">(</span><span class="n">s_inputs</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens, c_s)</span>

        <span class="c1"># Step 3: Initialize pair representation</span>
        <span class="c1"># z_ij^init = LinearNoBias(s_i^inputs) + LinearNoBias(s_j^inputs)</span>
        <span class="n">pair_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_linear_i</span><span class="p">(</span><span class="n">s_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens, 1, c_z)</span>
        <span class="n">pair_j</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_linear_j</span><span class="p">(</span><span class="n">s_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># (batch, 1, n_tokens, c_z)</span>
        <span class="n">z_ij_init</span> <span class="o">=</span> <span class="n">pair_i</span> <span class="o">+</span> <span class="n">pair_j</span>  <span class="c1"># (batch, n_tokens, n_tokens, c_z)</span>

        <span class="c1"># Step 4: Add relative position encoding</span>
        <span class="n">z_ij_init</span> <span class="o">=</span> <span class="n">z_ij_init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_encoding</span><span class="p">(</span><span class="n">f_star</span><span class="p">)</span>

        <span class="c1"># Step 5: Add token bonds (if available)</span>
        <span class="k">if</span> <span class="s2">&quot;token_bonds&quot;</span> <span class="ow">in</span> <span class="n">f_star</span><span class="p">:</span>
            <span class="n">token_bonds</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;token_bonds&quot;</span><span class="p">]</span>  <span class="c1"># (batch, n_tokens, n_tokens, bond_dim)</span>
            <span class="n">z_ij_init</span> <span class="o">=</span> <span class="n">z_ij_init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_bonds_linear</span><span class="p">(</span><span class="n">token_bonds</span><span class="p">)</span>

        <span class="c1"># Step 6: Initialize accumulators</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">z_ij_init</span><span class="p">)</span>
        <span class="n">s_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">s_i_init</span><span class="p">)</span>

        <span class="c1"># Step 7-14: Main recycling loop</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cycle</span><span class="p">):</span>
            <span class="c1"># Step 8: Update pair representation</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij_init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

            <span class="c1"># Step 9: Template Embedder</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">template_embedder</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>

            <span class="c1"># Step 10: MSA Module</span>
            <span class="k">if</span> <span class="s2">&quot;msa_features&quot;</span> <span class="ow">in</span> <span class="n">f_star</span><span class="p">:</span>
                <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">msa_module</span><span class="p">(</span>
                    <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;msa_features&quot;</span><span class="p">],</span>
                    <span class="n">f_star</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;has_deletion&quot;</span><span class="p">),</span>
                    <span class="n">f_star</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;deletion_value&quot;</span><span class="p">),</span>
                    <span class="n">s_inputs</span><span class="p">,</span>
                    <span class="n">z_ij</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Step 11: Update single representation</span>
            <span class="n">s_i</span> <span class="o">=</span> <span class="n">s_i_init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_update_linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">single_layer_norm</span><span class="p">(</span><span class="n">s_i</span><span class="p">))</span>

            <span class="c1"># Step 12: Pairformer Stack</span>
            <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairformer_stack</span><span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>

            <span class="c1"># Step 13: Copy for next iteration (handled by loop)</span>

        <span class="c1"># Step 15: Sample Diffusion</span>
        <span class="n">x_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_diffusion</span><span class="p">(</span>
            <span class="n">f_star</span><span class="p">,</span> <span class="n">s_inputs</span><span class="p">,</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">,</span> <span class="n">noise_schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Step 16: Confidence Head</span>
        <span class="n">confidence_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">confidence_head</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;token_single_initial_repr&quot;</span><span class="p">:</span> <span class="n">s_inputs</span><span class="p">},</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">,</span> <span class="n">x_pred</span>
        <span class="p">)</span>

        <span class="c1"># Step 17: Distogram Head</span>
        <span class="n">p_distogram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distogram_head</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

        <span class="c1"># Step 18: Return all outputs</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;x_pred&quot;</span><span class="p">:</span> <span class="n">x_pred</span><span class="p">,</span>
            <span class="s2">&quot;p_plddt&quot;</span><span class="p">:</span> <span class="n">confidence_outputs</span><span class="p">[</span><span class="s2">&quot;p_plddt&quot;</span><span class="p">],</span>
            <span class="s2">&quot;p_pae&quot;</span><span class="p">:</span> <span class="n">confidence_outputs</span><span class="p">[</span><span class="s2">&quot;p_pae&quot;</span><span class="p">],</span>
            <span class="s2">&quot;p_pde&quot;</span><span class="p">:</span> <span class="n">confidence_outputs</span><span class="p">[</span><span class="s2">&quot;p_pde&quot;</span><span class="p">],</span>
            <span class="s2">&quot;p_resolved&quot;</span><span class="p">:</span> <span class="n">confidence_outputs</span><span class="p">[</span><span class="s2">&quot;p_resolved&quot;</span><span class="p">],</span>
            <span class="s2">&quot;p_distogram&quot;</span><span class="p">:</span> <span class="n">p_distogram</span><span class="p">,</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.AlphaFold3.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">f_star</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass implementing Algorithm 1 exactly.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>f_star</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dictionary containing input features with keys:
- 'asym_id': asymmetric unit IDs (batch, n_tokens)
- 'residue_index': residue indices (batch, n_tokens)
- 'entity_id': entity IDs (batch, n_tokens)
- 'token_index': token indices (batch, n_tokens)
- 'sym_id': symmetry IDs (batch, n_tokens)
- 'token_bonds': token bond features (batch, n_tokens, n_tokens, bond_dim)
- Optional: 'template_features', 'msa_features', etc.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>outputs</code></td>            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dictionary containing:
- 'x_pred': predicted coordinates (batch, n_tokens, 3)
- 'p_plddt': pLDDT confidence (batch, n_tokens)
- 'p_pae': PAE confidence (batch, n_tokens, n_tokens)
- 'p_pde': PDE confidence (batch, n_tokens, n_tokens)
- 'p_resolved': resolved confidence (batch, n_tokens)
- 'p_distogram': distance distributions (batch, n_tokens, n_tokens, n_bins)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_alphafold3.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass implementing Algorithm 1 exactly.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    f_star : dict</span>
<span class="sd">        Dictionary containing input features with keys:</span>
<span class="sd">        - &#39;asym_id&#39;: asymmetric unit IDs (batch, n_tokens)</span>
<span class="sd">        - &#39;residue_index&#39;: residue indices (batch, n_tokens)</span>
<span class="sd">        - &#39;entity_id&#39;: entity IDs (batch, n_tokens)</span>
<span class="sd">        - &#39;token_index&#39;: token indices (batch, n_tokens)</span>
<span class="sd">        - &#39;sym_id&#39;: symmetry IDs (batch, n_tokens)</span>
<span class="sd">        - &#39;token_bonds&#39;: token bond features (batch, n_tokens, n_tokens, bond_dim)</span>
<span class="sd">        - Optional: &#39;template_features&#39;, &#39;msa_features&#39;, etc.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    outputs : dict</span>
<span class="sd">        Dictionary containing:</span>
<span class="sd">        - &#39;x_pred&#39;: predicted coordinates (batch, n_tokens, 3)</span>
<span class="sd">        - &#39;p_plddt&#39;: pLDDT confidence (batch, n_tokens)</span>
<span class="sd">        - &#39;p_pae&#39;: PAE confidence (batch, n_tokens, n_tokens)</span>
<span class="sd">        - &#39;p_pde&#39;: PDE confidence (batch, n_tokens, n_tokens)</span>
<span class="sd">        - &#39;p_resolved&#39;: resolved confidence (batch, n_tokens)</span>
<span class="sd">        - &#39;p_distogram&#39;: distance distributions (batch, n_tokens, n_tokens, n_bins)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Step 1: Input Feature Embedder</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_feature_embedder</span><span class="p">(</span><span class="n">f_star</span><span class="p">)</span>
    <span class="n">s_inputs</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;single&quot;</span><span class="p">]</span>  <span class="c1"># (batch, n_tokens, c_s)</span>

    <span class="c1"># Step 2: Initialize single representation</span>
    <span class="n">s_i_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_linear</span><span class="p">(</span><span class="n">s_inputs</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens, c_s)</span>

    <span class="c1"># Step 3: Initialize pair representation</span>
    <span class="c1"># z_ij^init = LinearNoBias(s_i^inputs) + LinearNoBias(s_j^inputs)</span>
    <span class="n">pair_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_linear_i</span><span class="p">(</span><span class="n">s_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens, 1, c_z)</span>
    <span class="n">pair_j</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_linear_j</span><span class="p">(</span><span class="n">s_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># (batch, 1, n_tokens, c_z)</span>
    <span class="n">z_ij_init</span> <span class="o">=</span> <span class="n">pair_i</span> <span class="o">+</span> <span class="n">pair_j</span>  <span class="c1"># (batch, n_tokens, n_tokens, c_z)</span>

    <span class="c1"># Step 4: Add relative position encoding</span>
    <span class="n">z_ij_init</span> <span class="o">=</span> <span class="n">z_ij_init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_encoding</span><span class="p">(</span><span class="n">f_star</span><span class="p">)</span>

    <span class="c1"># Step 5: Add token bonds (if available)</span>
    <span class="k">if</span> <span class="s2">&quot;token_bonds&quot;</span> <span class="ow">in</span> <span class="n">f_star</span><span class="p">:</span>
        <span class="n">token_bonds</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;token_bonds&quot;</span><span class="p">]</span>  <span class="c1"># (batch, n_tokens, n_tokens, bond_dim)</span>
        <span class="n">z_ij_init</span> <span class="o">=</span> <span class="n">z_ij_init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_bonds_linear</span><span class="p">(</span><span class="n">token_bonds</span><span class="p">)</span>

    <span class="c1"># Step 6: Initialize accumulators</span>
    <span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">z_ij_init</span><span class="p">)</span>
    <span class="n">s_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">s_i_init</span><span class="p">)</span>

    <span class="c1"># Step 7-14: Main recycling loop</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cycle</span><span class="p">):</span>
        <span class="c1"># Step 8: Update pair representation</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij_init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

        <span class="c1"># Step 9: Template Embedder</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">template_embedder</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>

        <span class="c1"># Step 10: MSA Module</span>
        <span class="k">if</span> <span class="s2">&quot;msa_features&quot;</span> <span class="ow">in</span> <span class="n">f_star</span><span class="p">:</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">msa_module</span><span class="p">(</span>
                <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;msa_features&quot;</span><span class="p">],</span>
                <span class="n">f_star</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;has_deletion&quot;</span><span class="p">),</span>
                <span class="n">f_star</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;deletion_value&quot;</span><span class="p">),</span>
                <span class="n">s_inputs</span><span class="p">,</span>
                <span class="n">z_ij</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Step 11: Update single representation</span>
        <span class="n">s_i</span> <span class="o">=</span> <span class="n">s_i_init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_update_linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">single_layer_norm</span><span class="p">(</span><span class="n">s_i</span><span class="p">))</span>

        <span class="c1"># Step 12: Pairformer Stack</span>
        <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairformer_stack</span><span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>

        <span class="c1"># Step 13: Copy for next iteration (handled by loop)</span>

    <span class="c1"># Step 15: Sample Diffusion</span>
    <span class="n">x_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_diffusion</span><span class="p">(</span>
        <span class="n">f_star</span><span class="p">,</span> <span class="n">s_inputs</span><span class="p">,</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">,</span> <span class="n">noise_schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Step 16: Confidence Head</span>
    <span class="n">confidence_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">confidence_head</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">&quot;token_single_initial_repr&quot;</span><span class="p">:</span> <span class="n">s_inputs</span><span class="p">},</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">,</span> <span class="n">x_pred</span>
    <span class="p">)</span>

    <span class="c1"># Step 17: Distogram Head</span>
    <span class="n">p_distogram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distogram_head</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

    <span class="c1"># Step 18: Return all outputs</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;x_pred&quot;</span><span class="p">:</span> <span class="n">x_pred</span><span class="p">,</span>
        <span class="s2">&quot;p_plddt&quot;</span><span class="p">:</span> <span class="n">confidence_outputs</span><span class="p">[</span><span class="s2">&quot;p_plddt&quot;</span><span class="p">],</span>
        <span class="s2">&quot;p_pae&quot;</span><span class="p">:</span> <span class="n">confidence_outputs</span><span class="p">[</span><span class="s2">&quot;p_pae&quot;</span><span class="p">],</span>
        <span class="s2">&quot;p_pde&quot;</span><span class="p">:</span> <span class="n">confidence_outputs</span><span class="p">[</span><span class="s2">&quot;p_pde&quot;</span><span class="p">],</span>
        <span class="s2">&quot;p_resolved&quot;</span><span class="p">:</span> <span class="n">confidence_outputs</span><span class="p">[</span><span class="s2">&quot;p_resolved&quot;</span><span class="p">],</span>
        <span class="s2">&quot;p_distogram&quot;</span><span class="p">:</span> <span class="n">p_distogram</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.AtomAttentionDecoder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.AtomAttentionDecoder</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Atom Attention Decoder for AlphaFold 3.</p>
<p>This module broadcasts per-token activations to per-atom activations,
applies cross attention transformer, and maps to position updates.
Implements Algorithm 6 exactly.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c_token</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for token representations</p>
              </div>
            </td>
            <td>
                  <code>768</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_atom</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for atom representations</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_block</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of transformer blocks</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn.alphafold3</span><span class="w"> </span><span class="kn">import</span> <span class="n">_AtomAttentionDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_atoms</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">_AtomAttentionDecoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>  <span class="c1"># Token representations</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q_skip</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>  <span class="c1"># Query skip</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c_skip</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>  <span class="c1"># Context skip</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p_skip</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># Pair skip</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r_update</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">q_skip</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">,</span> <span class="n">p_skip</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r_update</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 1000, 3])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 Algorithm 6: Atom attention decoder</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_atom_attention_decoder.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AtomAttentionDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Atom Attention Decoder for AlphaFold 3.</span>

<span class="sd">    This module broadcasts per-token activations to per-atom activations,</span>
<span class="sd">    applies cross attention transformer, and maps to position updates.</span>
<span class="sd">    Implements Algorithm 6 exactly.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c_token : int, default=768</span>
<span class="sd">        Channel dimension for token representations</span>
<span class="sd">    c_atom : int, default=128</span>
<span class="sd">        Channel dimension for atom representations</span>
<span class="sd">    n_block : int, default=3</span>
<span class="sd">        Number of transformer blocks</span>
<span class="sd">    n_head : int, default=4</span>
<span class="sd">        Number of attention heads</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn.alphafold3 import _AtomAttentionDecoder</span>
<span class="sd">    &gt;&gt;&gt; batch_size, n_tokens, n_atoms = 2, 32, 1000</span>
<span class="sd">    &gt;&gt;&gt; module = _AtomAttentionDecoder()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; a = torch.randn(batch_size, n_tokens, 768)  # Token representations</span>
<span class="sd">    &gt;&gt;&gt; q_skip = torch.randn(batch_size, n_atoms, 768)  # Query skip</span>
<span class="sd">    &gt;&gt;&gt; c_skip = torch.randn(batch_size, n_atoms, 128)  # Context skip</span>
<span class="sd">    &gt;&gt;&gt; p_skip = torch.randn(batch_size, n_atoms, n_atoms, 16)  # Pair skip</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; r_update = module(a, q_skip, c_skip, p_skip)</span>
<span class="sd">    &gt;&gt;&gt; r_update.shape</span>
<span class="sd">    torch.Size([2, 1000, 3])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 Algorithm 6: Atom attention decoder</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">c_token</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span> <span class="n">c_atom</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">n_block</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_token</span> <span class="o">=</span> <span class="n">c_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_atom</span> <span class="o">=</span> <span class="n">c_atom</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_block</span> <span class="o">=</span> <span class="n">n_block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>

        <span class="c1"># Step 1: Broadcast per-token activations to per-atom activations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_to_atom_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_token</span><span class="p">,</span> <span class="n">c_token</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Step 2: Cross attention transformer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atom_transformer</span> <span class="o">=</span> <span class="n">AtomTransformer</span><span class="p">(</span>
            <span class="n">n_block</span><span class="o">=</span><span class="n">n_block</span><span class="p">,</span>
            <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">c_q</span><span class="o">=</span><span class="n">c_token</span><span class="p">,</span>  <span class="c1"># Query dimension</span>
            <span class="n">c_kv</span><span class="o">=</span><span class="n">c_atom</span><span class="p">,</span>  <span class="c1"># Key-value dimension</span>
            <span class="n">c_pair</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Will be inferred from p_skip</span>
        <span class="p">)</span>

        <span class="c1"># Step 3: Map to position updates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_token</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_token</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">q_skip</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">p_skip</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of Atom Attention Decoder.</span>

<span class="sd">        Implements Algorithm 6 exactly:</span>
<span class="sd">        1. q_l = LinearNoBias(a_tok_idx(l)) + q_l^skip</span>
<span class="sd">        2. {q_l} = AtomTransformer({q_l}, {c_l^skip}, {p_lm^skip}, N_block=3, N_head=4)</span>
<span class="sd">        3. r_l^update = LinearNoBias(LayerNorm(q_l))</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        a : Tensor, shape=(batch_size, n_tokens, c_token)</span>
<span class="sd">            Token-level representations</span>
<span class="sd">        q_skip : Tensor, shape=(batch_size, n_atoms, c_token)</span>
<span class="sd">            Query skip connection</span>
<span class="sd">        c_skip : Tensor, shape=(batch_size, n_atoms, c_atom)</span>
<span class="sd">            Context skip connection</span>
<span class="sd">        p_skip : Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</span>
<span class="sd">            Pair skip connection</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        r_update : Tensor, shape=(batch_size, n_atoms, 3)</span>
<span class="sd">            Position updates for atoms</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">c_token</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">q_skip</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Step 1: Broadcast per-token activations to per-atom activations and add skip connection</span>
        <span class="c1"># q_l = LinearNoBias(a_tok_idx(l)) + q_l^skip</span>

        <span class="c1"># Create token indices for each atom (simple broadcasting approach)</span>
        <span class="c1"># For simplicity, we&#39;ll map atoms to tokens cyclically</span>
        <span class="n">token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_tokens</span>

        <span class="c1"># Get corresponding token activations for each atom</span>
        <span class="n">a_tok_idx</span> <span class="o">=</span> <span class="n">a</span><span class="p">[:,</span> <span class="n">token_indices</span><span class="p">]</span>  <span class="c1"># (batch_size, n_atoms, c_token)</span>

        <span class="c1"># Apply linear projection and add skip connection</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_to_atom_proj</span><span class="p">(</span><span class="n">a_tok_idx</span><span class="p">)</span> <span class="o">+</span> <span class="n">q_skip</span>

        <span class="c1"># Step 2: Cross attention transformer</span>
        <span class="c1"># {q_l} = AtomTransformer({q_l}, {c_l^skip}, {p_lm^skip}, N_block=3, N_head=4)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atom_transformer</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">,</span> <span class="n">p_skip</span><span class="p">)</span>

        <span class="c1"># Step 3: Map to positions update</span>
        <span class="c1"># r_l^update = LinearNoBias(LayerNorm(q_l))</span>
        <span class="n">r_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">r_update</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.AtomAttentionDecoder.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">q_skip</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">,</span> <span class="n">p_skip</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of Atom Attention Decoder.</p>
<p>Implements Algorithm 6 exactly:
1. q_l = LinearNoBias(a_tok_idx(l)) + q_l^skip
2. {q_l} = AtomTransformer({q_l}, {c_l^skip}, {p_lm^skip}, N_block=3, N_head=4)
3. r_l^update = LinearNoBias(LayerNorm(q_l))</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>a</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_tokens, c_token)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Token-level representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>q_skip</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, c_token)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Query skip connection</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_skip</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, c_atom)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Context skip connection</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>p_skip</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair skip connection</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>r_update</code></td>            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, 3)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Position updates for atoms</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_atom_attention_decoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">q_skip</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">p_skip</span><span class="p">:</span> <span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of Atom Attention Decoder.</span>

<span class="sd">    Implements Algorithm 6 exactly:</span>
<span class="sd">    1. q_l = LinearNoBias(a_tok_idx(l)) + q_l^skip</span>
<span class="sd">    2. {q_l} = AtomTransformer({q_l}, {c_l^skip}, {p_lm^skip}, N_block=3, N_head=4)</span>
<span class="sd">    3. r_l^update = LinearNoBias(LayerNorm(q_l))</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : Tensor, shape=(batch_size, n_tokens, c_token)</span>
<span class="sd">        Token-level representations</span>
<span class="sd">    q_skip : Tensor, shape=(batch_size, n_atoms, c_token)</span>
<span class="sd">        Query skip connection</span>
<span class="sd">    c_skip : Tensor, shape=(batch_size, n_atoms, c_atom)</span>
<span class="sd">        Context skip connection</span>
<span class="sd">    p_skip : Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</span>
<span class="sd">        Pair skip connection</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    r_update : Tensor, shape=(batch_size, n_atoms, 3)</span>
<span class="sd">        Position updates for atoms</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">c_token</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">q_skip</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Step 1: Broadcast per-token activations to per-atom activations and add skip connection</span>
    <span class="c1"># q_l = LinearNoBias(a_tok_idx(l)) + q_l^skip</span>

    <span class="c1"># Create token indices for each atom (simple broadcasting approach)</span>
    <span class="c1"># For simplicity, we&#39;ll map atoms to tokens cyclically</span>
    <span class="n">token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_tokens</span>

    <span class="c1"># Get corresponding token activations for each atom</span>
    <span class="n">a_tok_idx</span> <span class="o">=</span> <span class="n">a</span><span class="p">[:,</span> <span class="n">token_indices</span><span class="p">]</span>  <span class="c1"># (batch_size, n_atoms, c_token)</span>

    <span class="c1"># Apply linear projection and add skip connection</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_to_atom_proj</span><span class="p">(</span><span class="n">a_tok_idx</span><span class="p">)</span> <span class="o">+</span> <span class="n">q_skip</span>

    <span class="c1"># Step 2: Cross attention transformer</span>
    <span class="c1"># {q_l} = AtomTransformer({q_l}, {c_l^skip}, {p_lm^skip}, N_block=3, N_head=4)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atom_transformer</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">,</span> <span class="n">p_skip</span><span class="p">)</span>

    <span class="c1"># Step 3: Map to positions update</span>
    <span class="c1"># r_l^update = LinearNoBias(LayerNorm(q_l))</span>
    <span class="n">r_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">r_update</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.AtomAttentionEncoder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.AtomAttentionEncoder</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Atom Attention Encoder for AlphaFold 3.</p>
<p>This module implements Algorithm 5 exactly, creating atom single conditioning,
embedding offsets and distances, running cross attention transformer, and
aggregating per-atom to per-token representations.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c_atom</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for atom representations</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_atompair</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for atom pair representations</p>
              </div>
            </td>
            <td>
                  <code>16</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_token</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for token representations</p>
              </div>
            </td>
            <td>
                  <code>384</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_block</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of transformer blocks</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">AtomAttentionEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">AtomAttentionEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Feature dictionary with all required atom features</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_star</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;ref_pos&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;ref_mask&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;ref_element&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">118</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="s1">&#39;ref_atom_name_chars&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="s1">&#39;ref_charge&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;restype&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="s1">&#39;profile&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;deletion_mean&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;ref_space_uid&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">)),</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_trunk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">384</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">,</span> <span class="n">q_skip</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">,</span> <span class="n">p_skip</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_trunk</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 32, 384])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 Algorithm 5: Atom attention encoder</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_atom_attention_encoder.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AtomAttentionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Atom Attention Encoder for AlphaFold 3.</span>

<span class="sd">    This module implements Algorithm 5 exactly, creating atom single conditioning,</span>
<span class="sd">    embedding offsets and distances, running cross attention transformer, and</span>
<span class="sd">    aggregating per-atom to per-token representations.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c_atom : int, default=128</span>
<span class="sd">        Channel dimension for atom representations</span>
<span class="sd">    c_atompair : int, default=16</span>
<span class="sd">        Channel dimension for atom pair representations</span>
<span class="sd">    c_token : int, default=384</span>
<span class="sd">        Channel dimension for token representations</span>
<span class="sd">    n_block : int, default=3</span>
<span class="sd">        Number of transformer blocks</span>
<span class="sd">    n_head : int, default=4</span>
<span class="sd">        Number of attention heads</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import AtomAttentionEncoder</span>
<span class="sd">    &gt;&gt;&gt; batch_size, n_atoms = 2, 1000</span>
<span class="sd">    &gt;&gt;&gt; module = AtomAttentionEncoder()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Feature dictionary with all required atom features</span>
<span class="sd">    &gt;&gt;&gt; f_star = {</span>
<span class="sd">    ...     &#39;ref_pos&#39;: torch.randn(batch_size, n_atoms, 3),</span>
<span class="sd">    ...     &#39;ref_mask&#39;: torch.ones(batch_size, n_atoms),</span>
<span class="sd">    ...     &#39;ref_element&#39;: torch.randint(0, 118, (batch_size, n_atoms)),</span>
<span class="sd">    ...     &#39;ref_atom_name_chars&#39;: torch.randint(0, 26, (batch_size, n_atoms, 4)),</span>
<span class="sd">    ...     &#39;ref_charge&#39;: torch.randn(batch_size, n_atoms),</span>
<span class="sd">    ...     &#39;restype&#39;: torch.randint(0, 21, (batch_size, n_atoms)),</span>
<span class="sd">    ...     &#39;profile&#39;: torch.randn(batch_size, n_atoms, 20),</span>
<span class="sd">    ...     &#39;deletion_mean&#39;: torch.randn(batch_size, n_atoms),</span>
<span class="sd">    ...     &#39;ref_space_uid&#39;: torch.randint(0, 1000, (batch_size, n_atoms)),</span>
<span class="sd">    ... }</span>
<span class="sd">    &gt;&gt;&gt; r_t = torch.randn(batch_size, n_atoms, 3)</span>
<span class="sd">    &gt;&gt;&gt; s_trunk = torch.randn(batch_size, 32, 384)</span>
<span class="sd">    &gt;&gt;&gt; z_ij = torch.randn(batch_size, n_atoms, n_atoms, 16)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; a, q_skip, c_skip, p_skip = module(f_star, r_t, s_trunk, z_ij)</span>
<span class="sd">    &gt;&gt;&gt; a.shape</span>
<span class="sd">    torch.Size([2, 32, 384])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 Algorithm 5: Atom attention encoder</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">c_atom</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">c_atompair</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">c_token</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">384</span><span class="p">,</span>
        <span class="n">n_block</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_atom</span> <span class="o">=</span> <span class="n">c_atom</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_atompair</span> <span class="o">=</span> <span class="n">c_atompair</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_token</span> <span class="o">=</span> <span class="n">c_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_block</span> <span class="o">=</span> <span class="n">n_block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>

        <span class="c1"># Step 1: Create atom single conditioning by embedding per-atom meta data</span>
        <span class="c1"># We&#39;ll concatenate all features and project them</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atom_feature_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">118</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">26</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">21</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1000</span><span class="p">,</span>  <span class="c1"># Approximate feature size</span>
            <span class="n">c_atom</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 4: Embed pairwise inverse squared distances</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist_proj_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Step 6: Additional distance embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist_proj_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Step 9-11: Trunk embedding projections (if provided)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk_single_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_token</span><span class="p">,</span> <span class="n">c_atom</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk_single_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_token</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trunk_pair_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="mi">128</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>  <span class="c1"># Assuming z_trunk has 128 dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trunk_pair_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

        <span class="c1"># Step 11: Add noisy positions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noisy_pos_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">c_atom</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Step 13: Pair representation updates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_update_proj_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_atom</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_update_proj_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_atom</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Step 14: Small MLP on pair activations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_atompair</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_atompair</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_atompair</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_atompair</span><span class="p">,</span> <span class="n">c_atompair</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># Step 15: Cross attention transformer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atom_transformer</span> <span class="o">=</span> <span class="n">AtomTransformer</span><span class="p">(</span>
            <span class="n">n_block</span><span class="o">=</span><span class="n">n_block</span><span class="p">,</span>
            <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">c_q</span><span class="o">=</span><span class="n">c_atom</span><span class="p">,</span>
            <span class="n">c_kv</span><span class="o">=</span><span class="n">c_atom</span><span class="p">,</span>
            <span class="n">c_pair</span><span class="o">=</span><span class="n">c_atompair</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 16: Aggregation to per-token representation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_atom</span><span class="p">,</span> <span class="n">c_token</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">r_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">s_trunk</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of Atom Attention Encoder implementing Algorithm 5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        f_star : dict</span>
<span class="sd">            Dictionary containing atom features with keys:</span>
<span class="sd">            - &#39;ref_pos&#39;: reference positions (batch, n_atoms, 3)</span>
<span class="sd">            - &#39;ref_mask&#39;: mask (batch, n_atoms)</span>
<span class="sd">            - &#39;ref_element&#39;: element types (batch, n_atoms)</span>
<span class="sd">            - &#39;ref_atom_name_chars&#39;: atom name characters (batch, n_atoms, 4)</span>
<span class="sd">            - &#39;ref_charge&#39;: charges (batch, n_atoms)</span>
<span class="sd">            - &#39;restype&#39;: residue types (batch, n_atoms)</span>
<span class="sd">            - &#39;profile&#39;: sequence profile (batch, n_atoms, 20)</span>
<span class="sd">            - &#39;deletion_mean&#39;: deletion statistics (batch, n_atoms)</span>
<span class="sd">            - &#39;ref_space_uid&#39;: space UIDs (batch, n_atoms)</span>
<span class="sd">        r_t : Tensor, shape=(batch_size, n_atoms, 3)</span>
<span class="sd">            Noisy atomic positions at time t</span>
<span class="sd">        s_trunk : Tensor, shape=(batch_size, n_tokens, c_token)</span>
<span class="sd">            Trunk single representations (optional, can be None)</span>
<span class="sd">        z_ij : Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</span>
<span class="sd">            Atom pair representations</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        a : Tensor, shape=(batch_size, n_tokens, c_token)</span>
<span class="sd">            Token-level representations</span>
<span class="sd">        q_skip : Tensor, shape=(batch_size, n_atoms, c_atom)</span>
<span class="sd">            Skip connection for queries</span>
<span class="sd">        c_skip : Tensor, shape=(batch_size, n_atoms, c_atom)</span>
<span class="sd">            Skip connection for atom features</span>
<span class="sd">        p_skip : Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</span>
<span class="sd">            Skip connection for pair features</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">r_t</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">r_t</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># Step 1: Create atom single conditioning by embedding per-atom meta data</span>
        <span class="c1"># For simplicity, we&#39;ll use basic features that are commonly available</span>
        <span class="c1"># In practice, you&#39;d need to handle the full feature set properly</span>

        <span class="c1"># Use reference positions from f_star if available, otherwise zeros</span>
        <span class="n">ref_pos</span> <span class="o">=</span> <span class="n">f_star</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ref_pos&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">r_t</span><span class="p">))</span>

        <span class="c1"># Create a concatenated feature vector (simplified version)</span>
        <span class="c1"># In practice, you&#39;d properly embed each feature type</span>
        <span class="n">atom_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">ref_pos</span><span class="p">,</span>  <span class="c1"># (batch, n_atoms, 3)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                    <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
                <span class="p">),</span>  <span class="c1"># placeholder for other features</span>
            <span class="p">],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Pad or project to expected input size</span>
        <span class="k">if</span> <span class="n">atom_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">118</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">26</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">21</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1000</span><span class="p">:</span>
            <span class="c1"># Pad with zeros for missing features</span>
            <span class="n">pad_size</span> <span class="o">=</span> <span class="p">(</span>
                <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">118</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">26</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">21</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1000</span> <span class="o">-</span> <span class="n">atom_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">atom_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">atom_features</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">pad_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
                <span class="p">],</span>
                <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">c_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atom_feature_proj</span><span class="p">(</span>
            <span class="n">atom_features</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">118</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">26</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">21</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1000</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Steps 2-4: Embed offsets and distances</span>
        <span class="n">d_lm</span> <span class="o">=</span> <span class="n">ref_pos</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">ref_pos</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">3</span>
        <span class="p">)</span>  <span class="c1"># (batch, n_atoms, n_atoms, 3)</span>

        <span class="c1"># Step 3: Check for same reference space (simplified)</span>
        <span class="n">same_space</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>  <span class="c1"># Simplified</span>

        <span class="c1"># Step 4: Embed pairwise inverse squared distances</span>
        <span class="n">d_lm_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
            <span class="n">d_lm</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>  <span class="c1"># (batch, n_atoms, n_atoms, 1)</span>
        <span class="n">inv_sq_dist</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">d_lm_norm</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">p_lm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_proj_1</span><span class="p">(</span><span class="n">inv_sq_dist</span><span class="p">)</span> <span class="o">*</span> <span class="n">same_space</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Steps 5-6: Additional distance embeddings</span>
        <span class="n">p_lm</span> <span class="o">=</span> <span class="n">p_lm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_proj_2</span><span class="p">(</span><span class="n">same_space</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Step 7: Initialize atom single representation</span>
        <span class="n">q_l</span> <span class="o">=</span> <span class="n">c_l</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

        <span class="c1"># Steps 8-12: Add trunk embeddings and noisy positions if provided</span>
        <span class="k">if</span> <span class="n">s_trunk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">s_trunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Step 9: Broadcast single embedding from trunk</span>
            <span class="n">n_tokens</span> <span class="o">=</span> <span class="n">s_trunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_tokens</span>
            <span class="n">s_trunk_broadcast</span> <span class="o">=</span> <span class="n">s_trunk</span><span class="p">[:,</span> <span class="n">token_indices</span><span class="p">]</span>  <span class="c1"># (batch, n_atoms, c_token)</span>
            <span class="n">c_l</span> <span class="o">=</span> <span class="n">c_l</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk_single_proj</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trunk_single_norm</span><span class="p">(</span><span class="n">s_trunk_broadcast</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># Step 10: Add trunk pair embedding (simplified)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;z_trunk&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_trunk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># In practice, you&#39;d need the actual z_trunk tensor</span>
                <span class="k">pass</span>

        <span class="c1"># Step 11: Add noisy positions</span>
        <span class="n">q_l</span> <span class="o">=</span> <span class="n">q_l</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noisy_pos_proj</span><span class="p">(</span><span class="n">r_t</span><span class="p">)</span>

        <span class="c1"># Step 13: Add combined single conditioning to pair representation</span>
        <span class="n">p_lm</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">p_lm</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_update_proj_1</span><span class="p">(</span><span class="n">c_l</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_update_proj_2</span><span class="p">(</span><span class="n">c_l</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Step 14: Run small MLP on pair activations</span>
        <span class="n">p_lm</span> <span class="o">=</span> <span class="n">p_lm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_mlp</span><span class="p">(</span><span class="n">p_lm</span><span class="p">)</span>

        <span class="c1"># Step 15: Cross attention transformer</span>
        <span class="n">q_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atom_transformer</span><span class="p">(</span><span class="n">q_l</span><span class="p">,</span> <span class="n">c_l</span><span class="p">,</span> <span class="n">p_lm</span><span class="p">)</span>

        <span class="c1"># Step 16: Aggregate per-atom to per-token representation</span>
        <span class="k">if</span> <span class="n">s_trunk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_tokens</span> <span class="o">=</span> <span class="n">s_trunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># Simple mean aggregation within token groups</span>
            <span class="n">token_assignment</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span>
                <span class="n">n_atoms</span> <span class="o">//</span> <span class="n">n_tokens</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">token_assignment</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">token_assignment</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Aggregate atoms to tokens</span>
            <span class="n">a_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_token</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q_l</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span>
            <span class="n">q_l_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_proj</span><span class="p">(</span><span class="n">q_l</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">):</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">token_assignment</span> <span class="o">==</span> <span class="n">i</span>
                <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                    <span class="n">a_i</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">q_l_projected</span><span class="p">[:,</span> <span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If no trunk, create a single token representation</span>
            <span class="n">a_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_proj</span><span class="p">(</span><span class="n">q_l</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Step 17: Skip connections</span>
        <span class="n">q_skip</span> <span class="o">=</span> <span class="n">q_l</span>
        <span class="n">c_skip</span> <span class="o">=</span> <span class="n">c_l</span>
        <span class="n">p_skip</span> <span class="o">=</span> <span class="n">p_lm</span>

        <span class="k">return</span> <span class="n">a_i</span><span class="p">,</span> <span class="n">q_skip</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">,</span> <span class="n">p_skip</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.AtomAttentionEncoder.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_trunk</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of Atom Attention Encoder implementing Algorithm 5.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>f_star</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dictionary containing atom features with keys:
- 'ref_pos': reference positions (batch, n_atoms, 3)
- 'ref_mask': mask (batch, n_atoms)
- 'ref_element': element types (batch, n_atoms)
- 'ref_atom_name_chars': atom name characters (batch, n_atoms, 4)
- 'ref_charge': charges (batch, n_atoms)
- 'restype': residue types (batch, n_atoms)
- 'profile': sequence profile (batch, n_atoms, 20)
- 'deletion_mean': deletion statistics (batch, n_atoms)
- 'ref_space_uid': space UIDs (batch, n_atoms)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>r_t</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, 3)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Noisy atomic positions at time t</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_trunk</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_tokens, c_token)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Trunk single representations (optional, can be None)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>z_ij</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Atom pair representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>a</code></td>            <td>
                  <code>Tensor, shape=(batch_size, n_tokens, c_token)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Token-level representations</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>q_skip</code></td>            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, c_atom)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Skip connection for queries</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>c_skip</code></td>            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, c_atom)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Skip connection for atom features</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>p_skip</code></td>            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Skip connection for pair features</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_atom_attention_encoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">r_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">s_trunk</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of Atom Attention Encoder implementing Algorithm 5.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    f_star : dict</span>
<span class="sd">        Dictionary containing atom features with keys:</span>
<span class="sd">        - &#39;ref_pos&#39;: reference positions (batch, n_atoms, 3)</span>
<span class="sd">        - &#39;ref_mask&#39;: mask (batch, n_atoms)</span>
<span class="sd">        - &#39;ref_element&#39;: element types (batch, n_atoms)</span>
<span class="sd">        - &#39;ref_atom_name_chars&#39;: atom name characters (batch, n_atoms, 4)</span>
<span class="sd">        - &#39;ref_charge&#39;: charges (batch, n_atoms)</span>
<span class="sd">        - &#39;restype&#39;: residue types (batch, n_atoms)</span>
<span class="sd">        - &#39;profile&#39;: sequence profile (batch, n_atoms, 20)</span>
<span class="sd">        - &#39;deletion_mean&#39;: deletion statistics (batch, n_atoms)</span>
<span class="sd">        - &#39;ref_space_uid&#39;: space UIDs (batch, n_atoms)</span>
<span class="sd">    r_t : Tensor, shape=(batch_size, n_atoms, 3)</span>
<span class="sd">        Noisy atomic positions at time t</span>
<span class="sd">    s_trunk : Tensor, shape=(batch_size, n_tokens, c_token)</span>
<span class="sd">        Trunk single representations (optional, can be None)</span>
<span class="sd">    z_ij : Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</span>
<span class="sd">        Atom pair representations</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    a : Tensor, shape=(batch_size, n_tokens, c_token)</span>
<span class="sd">        Token-level representations</span>
<span class="sd">    q_skip : Tensor, shape=(batch_size, n_atoms, c_atom)</span>
<span class="sd">        Skip connection for queries</span>
<span class="sd">    c_skip : Tensor, shape=(batch_size, n_atoms, c_atom)</span>
<span class="sd">        Skip connection for atom features</span>
<span class="sd">    p_skip : Tensor, shape=(batch_size, n_atoms, n_atoms, c_atompair)</span>
<span class="sd">        Skip connection for pair features</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">r_t</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">r_t</span><span class="o">.</span><span class="n">device</span>

    <span class="c1"># Step 1: Create atom single conditioning by embedding per-atom meta data</span>
    <span class="c1"># For simplicity, we&#39;ll use basic features that are commonly available</span>
    <span class="c1"># In practice, you&#39;d need to handle the full feature set properly</span>

    <span class="c1"># Use reference positions from f_star if available, otherwise zeros</span>
    <span class="n">ref_pos</span> <span class="o">=</span> <span class="n">f_star</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ref_pos&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">r_t</span><span class="p">))</span>

    <span class="c1"># Create a concatenated feature vector (simplified version)</span>
    <span class="c1"># In practice, you&#39;d properly embed each feature type</span>
    <span class="n">atom_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">ref_pos</span><span class="p">,</span>  <span class="c1"># (batch, n_atoms, 3)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
            <span class="p">),</span>  <span class="c1"># placeholder for other features</span>
        <span class="p">],</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Pad or project to expected input size</span>
    <span class="k">if</span> <span class="n">atom_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">118</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">26</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">21</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1000</span><span class="p">:</span>
        <span class="c1"># Pad with zeros for missing features</span>
        <span class="n">pad_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">118</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">26</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">21</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1000</span> <span class="o">-</span> <span class="n">atom_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">atom_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">atom_features</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">pad_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
            <span class="p">],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">c_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atom_feature_proj</span><span class="p">(</span>
        <span class="n">atom_features</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">118</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">26</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">21</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1000</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># Steps 2-4: Embed offsets and distances</span>
    <span class="n">d_lm</span> <span class="o">=</span> <span class="n">ref_pos</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">ref_pos</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
        <span class="o">-</span><span class="mi">3</span>
    <span class="p">)</span>  <span class="c1"># (batch, n_atoms, n_atoms, 3)</span>

    <span class="c1"># Step 3: Check for same reference space (simplified)</span>
    <span class="n">same_space</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>  <span class="c1"># Simplified</span>

    <span class="c1"># Step 4: Embed pairwise inverse squared distances</span>
    <span class="n">d_lm_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
        <span class="n">d_lm</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>  <span class="c1"># (batch, n_atoms, n_atoms, 1)</span>
    <span class="n">inv_sq_dist</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">d_lm_norm</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">p_lm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_proj_1</span><span class="p">(</span><span class="n">inv_sq_dist</span><span class="p">)</span> <span class="o">*</span> <span class="n">same_space</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Steps 5-6: Additional distance embeddings</span>
    <span class="n">p_lm</span> <span class="o">=</span> <span class="n">p_lm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_proj_2</span><span class="p">(</span><span class="n">same_space</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Step 7: Initialize atom single representation</span>
    <span class="n">q_l</span> <span class="o">=</span> <span class="n">c_l</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

    <span class="c1"># Steps 8-12: Add trunk embeddings and noisy positions if provided</span>
    <span class="k">if</span> <span class="n">s_trunk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">s_trunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Step 9: Broadcast single embedding from trunk</span>
        <span class="n">n_tokens</span> <span class="o">=</span> <span class="n">s_trunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_tokens</span>
        <span class="n">s_trunk_broadcast</span> <span class="o">=</span> <span class="n">s_trunk</span><span class="p">[:,</span> <span class="n">token_indices</span><span class="p">]</span>  <span class="c1"># (batch, n_atoms, c_token)</span>
        <span class="n">c_l</span> <span class="o">=</span> <span class="n">c_l</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk_single_proj</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trunk_single_norm</span><span class="p">(</span><span class="n">s_trunk_broadcast</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Step 10: Add trunk pair embedding (simplified)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;z_trunk&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_trunk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># In practice, you&#39;d need the actual z_trunk tensor</span>
            <span class="k">pass</span>

    <span class="c1"># Step 11: Add noisy positions</span>
    <span class="n">q_l</span> <span class="o">=</span> <span class="n">q_l</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noisy_pos_proj</span><span class="p">(</span><span class="n">r_t</span><span class="p">)</span>

    <span class="c1"># Step 13: Add combined single conditioning to pair representation</span>
    <span class="n">p_lm</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">p_lm</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_update_proj_1</span><span class="p">(</span><span class="n">c_l</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_update_proj_2</span><span class="p">(</span><span class="n">c_l</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Step 14: Run small MLP on pair activations</span>
    <span class="n">p_lm</span> <span class="o">=</span> <span class="n">p_lm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_mlp</span><span class="p">(</span><span class="n">p_lm</span><span class="p">)</span>

    <span class="c1"># Step 15: Cross attention transformer</span>
    <span class="n">q_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atom_transformer</span><span class="p">(</span><span class="n">q_l</span><span class="p">,</span> <span class="n">c_l</span><span class="p">,</span> <span class="n">p_lm</span><span class="p">)</span>

    <span class="c1"># Step 16: Aggregate per-atom to per-token representation</span>
    <span class="k">if</span> <span class="n">s_trunk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_tokens</span> <span class="o">=</span> <span class="n">s_trunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Simple mean aggregation within token groups</span>
        <span class="n">token_assignment</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span>
            <span class="n">n_atoms</span> <span class="o">//</span> <span class="n">n_tokens</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">token_assignment</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">token_assignment</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Aggregate atoms to tokens</span>
        <span class="n">a_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_token</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q_l</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">q_l_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_proj</span><span class="p">(</span><span class="n">q_l</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">token_assignment</span> <span class="o">==</span> <span class="n">i</span>
            <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">a_i</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">q_l_projected</span><span class="p">[:,</span> <span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># If no trunk, create a single token representation</span>
        <span class="n">a_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_proj</span><span class="p">(</span><span class="n">q_l</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Step 17: Skip connections</span>
    <span class="n">q_skip</span> <span class="o">=</span> <span class="n">q_l</span>
    <span class="n">c_skip</span> <span class="o">=</span> <span class="n">c_l</span>
    <span class="n">p_skip</span> <span class="o">=</span> <span class="n">p_lm</span>

    <span class="k">return</span> <span class="n">a_i</span><span class="p">,</span> <span class="n">q_skip</span><span class="p">,</span> <span class="n">c_skip</span><span class="p">,</span> <span class="n">p_skip</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.AtomTransformer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.AtomTransformer</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Atom Transformer for AlphaFold 3.</p>
<p>This module implements sequence-local atom attention using rectangular blocks
along the diagonal. It applies the DiffusionTransformer with sequence-local
attention masking based on query and key positions.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_block</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of transformer blocks</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_queries</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of queries per block</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_keys</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of keys per block</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subset_centres</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Centers for subset selection</p>
              </div>
            </td>
            <td>
                  <code>[15.5, 47.5, 79.5, ...]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_q</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Query dimension (inferred from input if None)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_kv</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Key-value dimension (inferred from input if None)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_pair</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair dimension (inferred from input if None)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">AtomTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">AtomTransformer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 1000, 128])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 Algorithm 7: Atom Transformer</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_atom_transformer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AtomTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Atom Transformer for AlphaFold 3.</span>

<span class="sd">    This module implements sequence-local atom attention using rectangular blocks</span>
<span class="sd">    along the diagonal. It applies the DiffusionTransformer with sequence-local</span>
<span class="sd">    attention masking based on query and key positions.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_block : int, default=3</span>
<span class="sd">        Number of transformer blocks</span>
<span class="sd">    n_head : int, default=4</span>
<span class="sd">        Number of attention heads</span>
<span class="sd">    n_queries : int, default=32</span>
<span class="sd">        Number of queries per block</span>
<span class="sd">    n_keys : int, default=128</span>
<span class="sd">        Number of keys per block</span>
<span class="sd">    subset_centres : list, default=[15.5, 47.5, 79.5, ...]</span>
<span class="sd">        Centers for subset selection</span>
<span class="sd">    c_q : int, default=None</span>
<span class="sd">        Query dimension (inferred from input if None)</span>
<span class="sd">    c_kv : int, default=None</span>
<span class="sd">        Key-value dimension (inferred from input if None)</span>
<span class="sd">    c_pair : int, default=None</span>
<span class="sd">        Pair dimension (inferred from input if None)</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import AtomTransformer</span>
<span class="sd">    &gt;&gt;&gt; batch_size, n_atoms = 2, 1000</span>
<span class="sd">    &gt;&gt;&gt; module = AtomTransformer()</span>
<span class="sd">    &gt;&gt;&gt; q = torch.randn(batch_size, n_atoms, 128)</span>
<span class="sd">    &gt;&gt;&gt; c = torch.randn(batch_size, n_atoms, 64)</span>
<span class="sd">    &gt;&gt;&gt; p = torch.randn(batch_size, n_atoms, n_atoms, 16)</span>
<span class="sd">    &gt;&gt;&gt; output = module(q, c, p)</span>
<span class="sd">    &gt;&gt;&gt; output.shape</span>
<span class="sd">    torch.Size([2, 1000, 128])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 Algorithm 7: Atom Transformer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_block</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">n_queries</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">n_keys</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">subset_centres</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">c_q</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">c_kv</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">c_pair</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_block</span> <span class="o">=</span> <span class="n">n_block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_queries</span> <span class="o">=</span> <span class="n">n_queries</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_keys</span> <span class="o">=</span> <span class="n">n_keys</span>

        <span class="k">if</span> <span class="n">subset_centres</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Default subset centers as specified in the algorithm</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">subset_centres</span> <span class="o">=</span> <span class="p">[</span><span class="mf">15.5</span><span class="p">,</span> <span class="mf">47.5</span><span class="p">,</span> <span class="mf">79.5</span><span class="p">]</span>  <span class="c1"># Can be extended as needed</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">subset_centres</span> <span class="o">=</span> <span class="n">subset_centres</span>

        <span class="c1"># Store dimensions (will be inferred from input if not provided)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span> <span class="o">=</span> <span class="n">c_q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_kv</span> <span class="o">=</span> <span class="n">c_kv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_pair</span> <span class="o">=</span> <span class="n">c_pair</span>

        <span class="c1"># Will be initialized in first forward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_sequence_local_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">beta_lm</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create sequence-local attention mask based on Algorithm 7.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        q : Tensor, shape=(batch_size, n_atoms, c_q)</span>
<span class="sd">            Query tensor</span>
<span class="sd">        beta_lm : Tensor, shape=(batch_size, n_atoms, n_atoms, n_head)</span>
<span class="sd">            Base attention bias</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        beta_lm : Tensor, shape=(batch_size, n_atoms, n_atoms, n_head)</span>
<span class="sd">            Modified attention bias with sequence-local masking</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># Create position indices</span>
        <span class="n">l_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># (n_atoms,)</span>
        <span class="n">m_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># (n_atoms,)</span>

        <span class="c1"># Create meshgrid for all pairs</span>
        <span class="n">l_grid</span><span class="p">,</span> <span class="n">m_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
            <span class="n">l_idx</span><span class="p">,</span> <span class="n">m_idx</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;ij&quot;</span>
        <span class="p">)</span>  <span class="c1"># (n_atoms, n_atoms)</span>

        <span class="c1"># Initialize mask with -10^10 (effectively -inf)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">beta_lm</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e10</span><span class="p">)</span>

        <span class="c1"># For each subset center, create rectangular blocks along diagonal</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subset_centres</span><span class="p">:</span>
            <span class="c1"># Condition: |l - c| &lt; n_queries/2 ∧ |m - c| &lt; n_keys/2</span>
            <span class="n">l_condition</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">l_grid</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span> <span class="o">&lt;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_queries</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">m_condition</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">m_grid</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span> <span class="o">&lt;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_keys</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Combined condition for this subset</span>
            <span class="n">subset_condition</span> <span class="o">=</span> <span class="n">l_condition</span> <span class="o">&amp;</span> <span class="n">m_condition</span>  <span class="c1"># (n_atoms, n_atoms)</span>

            <span class="c1"># Expand to match beta_lm shape</span>
            <span class="n">subset_condition</span> <span class="o">=</span> <span class="n">subset_condition</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># (1, n_atoms, n_atoms, 1)</span>
            <span class="n">subset_condition</span> <span class="o">=</span> <span class="n">subset_condition</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">)</span>

            <span class="c1"># Set mask to 0 where condition is satisfied</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">subset_condition</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="c1"># Apply mask to beta_lm</span>
        <span class="n">beta_lm</span> <span class="o">=</span> <span class="n">beta_lm</span> <span class="o">+</span> <span class="n">mask</span>

        <span class="k">return</span> <span class="n">beta_lm</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of Atom Transformer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        q : Tensor, shape=(batch_size, n_atoms, c_q)</span>
<span class="sd">            Query representations</span>
<span class="sd">        c : Tensor, shape=(batch_size, n_atoms, c_kv)</span>
<span class="sd">            Context (single) representations</span>
<span class="sd">        p : Tensor, shape=(batch_size, n_atoms, n_atoms, c_pair)</span>
<span class="sd">            Pair representations</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        q : Tensor, shape=(batch_size, n_atoms, c_q)</span>
<span class="sd">            Updated query representations</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Infer dimensions from input if not provided</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">c_q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span>
            <span class="n">c_kv</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_kv</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_kv</span>
            <span class="n">c_pair</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_pair</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_pair</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span> <span class="o">=</span> <span class="n">DiffusionTransformer</span><span class="p">(</span>
                <span class="n">c_a</span><span class="o">=</span><span class="n">c_q</span><span class="p">,</span>  <span class="c1"># Use query dimension as token dimension</span>
                <span class="n">c_s</span><span class="o">=</span><span class="n">c_kv</span><span class="p">,</span>  <span class="c1"># Use context dimension as single dimension</span>
                <span class="n">c_z</span><span class="o">=</span><span class="n">c_pair</span><span class="p">,</span>  <span class="c1"># Use pair dimension</span>
                <span class="n">n_head</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                <span class="n">n_block</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_block</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Move to same device as input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Create initial beta_lm (starts as zeros, will be modified by masking)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">beta_lm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

        <span class="c1"># Apply sequence-local masking</span>
        <span class="n">beta_lm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_sequence_local_mask</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">beta_lm</span><span class="p">)</span>

        <span class="c1"># Apply DiffusionTransformer</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">beta_lm</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">q</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.AtomTransformer.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of Atom Transformer.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>q</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, c_q)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Query representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, c_kv)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Context (single) representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>p</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, n_atoms, c_pair)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>q</code></td>            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, c_q)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated query representations</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_atom_transformer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of Atom Transformer.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    q : Tensor, shape=(batch_size, n_atoms, c_q)</span>
<span class="sd">        Query representations</span>
<span class="sd">    c : Tensor, shape=(batch_size, n_atoms, c_kv)</span>
<span class="sd">        Context (single) representations</span>
<span class="sd">    p : Tensor, shape=(batch_size, n_atoms, n_atoms, c_pair)</span>
<span class="sd">        Pair representations</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    q : Tensor, shape=(batch_size, n_atoms, c_q)</span>
<span class="sd">        Updated query representations</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Infer dimensions from input if not provided</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">c_q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_q</span>
        <span class="n">c_kv</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_kv</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_kv</span>
        <span class="n">c_pair</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_pair</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_pair</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span> <span class="o">=</span> <span class="n">DiffusionTransformer</span><span class="p">(</span>
            <span class="n">c_a</span><span class="o">=</span><span class="n">c_q</span><span class="p">,</span>  <span class="c1"># Use query dimension as token dimension</span>
            <span class="n">c_s</span><span class="o">=</span><span class="n">c_kv</span><span class="p">,</span>  <span class="c1"># Use context dimension as single dimension</span>
            <span class="n">c_z</span><span class="o">=</span><span class="n">c_pair</span><span class="p">,</span>  <span class="c1"># Use pair dimension</span>
            <span class="n">n_head</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">n_block</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_block</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Move to same device as input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Create initial beta_lm (starts as zeros, will be modified by masking)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">beta_lm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span>
    <span class="p">)</span>

    <span class="c1"># Apply sequence-local masking</span>
    <span class="n">beta_lm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_sequence_local_mask</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">beta_lm</span><span class="p">)</span>

    <span class="c1"># Apply DiffusionTransformer</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_transformer</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">beta_lm</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">q</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.AttentionPairBias" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.AttentionPairBias</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Attention with pair bias and mask from AlphaFold 3 Algorithm 24.</p>
<p>This implements the AttentionPairBias operation with conditioning signal support
for diffusion models. It uses AdaLN when conditioning is provided, or standard
LayerNorm when not. Includes proper gating and output projection.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c_a</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for input representation 'a'</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_s</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for conditioning signal 's' (can be None if no conditioning)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_z</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for pair representation 'z'</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">AttentionPairBias</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span><span class="p">,</span> <span class="n">n_head</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">AttentionPairBias</span><span class="p">(</span><span class="n">c_a</span><span class="o">=</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_s</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_head</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_out</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 10, 256])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 Algorithm 24: AttentionPairBias with pair bias and mask</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_attention_pair_bias.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AttentionPairBias</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attention with pair bias and mask from AlphaFold 3 Algorithm 24.</span>

<span class="sd">    This implements the AttentionPairBias operation with conditioning signal support</span>
<span class="sd">    for diffusion models. It uses AdaLN when conditioning is provided, or standard</span>
<span class="sd">    LayerNorm when not. Includes proper gating and output projection.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c_a : int</span>
<span class="sd">        Channel dimension for input representation &#39;a&#39;</span>
<span class="sd">    c_s : int</span>
<span class="sd">        Channel dimension for conditioning signal &#39;s&#39; (can be None if no conditioning)</span>
<span class="sd">    c_z : int</span>
<span class="sd">        Channel dimension for pair representation &#39;z&#39;</span>
<span class="sd">    n_head : int</span>
<span class="sd">        Number of attention heads</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import AttentionPairBias</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len = 2, 10</span>
<span class="sd">    &gt;&gt;&gt; c_a, c_s, c_z, n_head = 256, 384, 128, 16</span>
<span class="sd">    &gt;&gt;&gt; module = AttentionPairBias(c_a=c_a, c_s=c_s, c_z=c_z, n_head=n_head)</span>
<span class="sd">    &gt;&gt;&gt; a = torch.randn(batch_size, seq_len, c_a)</span>
<span class="sd">    &gt;&gt;&gt; s = torch.randn(batch_size, seq_len, c_s)</span>
<span class="sd">    &gt;&gt;&gt; z = torch.randn(batch_size, seq_len, seq_len, c_z)</span>
<span class="sd">    &gt;&gt;&gt; beta = torch.randn(batch_size, seq_len, seq_len, n_head)</span>
<span class="sd">    &gt;&gt;&gt; a_out = module(a, s, z, beta)</span>
<span class="sd">    &gt;&gt;&gt; a_out.shape</span>
<span class="sd">    torch.Size([2, 10, 256])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 Algorithm 24: AttentionPairBias with pair bias and mask</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">c_s</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">c_z</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_a</span> <span class="o">=</span> <span class="n">c_a</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_s</span> <span class="o">=</span> <span class="n">c_s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span> <span class="o">=</span> <span class="n">c_z</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">c_a</span> <span class="o">//</span> <span class="n">n_head</span>

        <span class="k">if</span> <span class="n">c_a</span> <span class="o">%</span> <span class="n">n_head</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Channel dimension </span><span class="si">{</span><span class="n">c_a</span><span class="si">}</span><span class="s2"> must be divisible by number of heads </span><span class="si">{</span><span class="n">n_head</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Input projections - Algorithm 24 steps 1-4</span>
        <span class="c1"># Step 1-2: If {si} ≠ ∅ then ai ← AdaLN(ai, si) else ai ← LayerNorm(ai)</span>
        <span class="k">if</span> <span class="n">c_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ada_ln</span> <span class="o">=</span> <span class="n">AdaptiveLayerNorm</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_a</span><span class="p">)</span>

        <span class="c1"># Step 6: q_i^h = Linear(ai)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_a</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Step 7: k_i^h, v_i^h = LinearNoBias(ai)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_a</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_a</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Step 8: b_ij^h ← LinearNoBias(LayerNorm(zij)) + βij</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_z</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_z</span><span class="p">)</span>

        <span class="c1"># Step 9: g_i^h ← sigmoid(LinearNoBias(ai))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_a</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Step 11: Output projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_a</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Scale factor for attention (Step 10: 1/√c where c = ca/Nhead)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Output projection with adaLN-Zero pattern (Steps 12-13)</span>
        <span class="k">if</span> <span class="n">c_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Step 13: sigmoid(Linear(si, biasinit=-2.0)) ⊙ ai</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear_s_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_a</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Initialize bias to -2.0 as specified</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">linear_s_gate</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of attention with pair bias.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        a : Tensor, shape=(..., seq_len, c_a)</span>
<span class="sd">            Input representation</span>
<span class="sd">        s : Tensor, shape=(..., seq_len, c_s), optional</span>
<span class="sd">            Conditioning signal (if None, uses LayerNorm instead of AdaLN)</span>
<span class="sd">        z : Tensor, shape=(..., seq_len, seq_len, c_z), optional</span>
<span class="sd">            Pair representation for computing attention bias</span>
<span class="sd">        beta : Tensor, shape=(..., seq_len, seq_len, n_head), optional</span>
<span class="sd">            Additional bias terms</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        a_out : Tensor, shape=(..., seq_len, c_a)</span>
<span class="sd">            Updated representation after attention with pair bias</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># Algorithm 24 Steps 1-4: Input projections</span>
        <span class="c1"># Step 1: if {si} ≠ ∅ then</span>
        <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;ada_ln&quot;</span><span class="p">):</span>
            <span class="c1"># Step 2: ai ← AdaLN(ai, si)</span>
            <span class="n">a_normed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ada_ln</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Step 4: ai ← LayerNorm(ai)</span>
            <span class="n">a_normed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="c1"># Step 6: q_i^h = Linear(ai)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">a_normed</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Step 7: k_i^h, v_i^h = LinearNoBias(ai)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">a_normed</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">a_normed</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Step 8: b_ij^h ← LinearNoBias(LayerNorm(zij)) + βij</span>
        <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">z_normed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_z</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">b_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_normed</span><span class="p">)</span>  <span class="c1"># Shape: (..., seq_len, seq_len, n_head)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span>
                <span class="n">seq_len</span><span class="p">,</span>
                <span class="n">seq_len</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">b_z</span> <span class="o">+</span> <span class="n">beta</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">b_z</span>

        <span class="c1"># Step 9: g_i^h ← sigmoid(LinearNoBias(ai))</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">a_normed</span><span class="p">))</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Step 10: A_ij^h ← softmax_j(1/√c * q_i^h * k_j^h + b_ij^h)</span>
        <span class="c1"># Compute attention scores: q @ k.T / √d + bias</span>
        <span class="c1"># q: (..., seq_len, n_head, head_dim)</span>
        <span class="c1"># k: (..., seq_len, n_head, head_dim)</span>
        <span class="c1"># Want: (..., n_head, seq_len, seq_len)</span>
        <span class="n">attn_logits</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...ihd,...jhd-&gt;...hij&quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="p">)</span>  <span class="c1"># Shape: (..., n_head, seq_len, seq_len)</span>

        <span class="c1"># Add bias: b has shape (..., seq_len, seq_len, n_head)</span>
        <span class="c1"># We need to transpose to match attention shape</span>
        <span class="n">b_transposed</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., n_head, seq_len, seq_len)</span>
        <span class="n">attn_logits</span> <span class="o">=</span> <span class="n">attn_logits</span> <span class="o">+</span> <span class="n">b_transposed</span>

        <span class="c1"># Apply softmax over the last dimension (keys)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Step 11: ai ← LinearNoBias(concat_h(g_i^h ⊙ Σ_j A_ij^h v_j^h))</span>
        <span class="c1"># Apply attention to values and gate</span>
        <span class="c1"># attn_weights: (..., n_head, seq_len, seq_len)</span>
        <span class="c1"># v: (..., seq_len, n_head, head_dim)</span>
        <span class="n">attended_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="s2">&quot;...hij,...jhd-&gt;...hid&quot;</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span>
        <span class="p">)</span>  <span class="c1"># Shape: (..., n_head, seq_len, head_dim)</span>

        <span class="c1"># Reshape g to match attended_v shape: (..., seq_len, n_head, head_dim) -&gt; (..., n_head, seq_len, head_dim)</span>
        <span class="n">g_reshaped</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Shape: (..., n_head, seq_len, head_dim)</span>

        <span class="c1"># Apply gating: g ⊙ attended_v</span>
        <span class="n">gated_output</span> <span class="o">=</span> <span class="n">g_reshaped</span> <span class="o">*</span> <span class="n">attended_v</span>  <span class="c1"># Element-wise multiplication</span>

        <span class="c1"># Concatenate heads: reshape to (..., seq_len, c_a)</span>
        <span class="n">concat_output</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">gated_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_a</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Linear projection</span>
        <span class="n">a_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span><span class="n">concat_output</span><span class="p">)</span>

        <span class="c1"># Algorithm 24 Steps 12-14: Output projection (from adaLN-Zero)</span>
        <span class="c1"># Step 12: if {si} ≠ ∅ then</span>
        <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;linear_s_gate&quot;</span><span class="p">):</span>
            <span class="c1"># Step 13: ai ← sigmoid(Linear(si, biasinit=-2.0)) ⊙ ai</span>
            <span class="n">s_gate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_s_gate</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
            <span class="n">a_out</span> <span class="o">=</span> <span class="n">s_gate</span> <span class="o">*</span> <span class="n">a_out</span>

        <span class="c1"># Step 15: return {ai}</span>
        <span class="k">return</span> <span class="n">a_out</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.AttentionPairBias.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of attention with pair bias.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>a</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., seq_len, c_a)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input representation</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., seq_len, c_s)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Conditioning signal (if None, uses LayerNorm instead of AdaLN)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>z</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., seq_len, seq_len, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair representation for computing attention bias</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>beta</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., seq_len, seq_len, n_head)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional bias terms</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>a_out</code></td>            <td>
                  <code>Tensor, shape=(..., seq_len, c_a)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated representation after attention with pair bias</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_attention_pair_bias.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of attention with pair bias.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : Tensor, shape=(..., seq_len, c_a)</span>
<span class="sd">        Input representation</span>
<span class="sd">    s : Tensor, shape=(..., seq_len, c_s), optional</span>
<span class="sd">        Conditioning signal (if None, uses LayerNorm instead of AdaLN)</span>
<span class="sd">    z : Tensor, shape=(..., seq_len, seq_len, c_z), optional</span>
<span class="sd">        Pair representation for computing attention bias</span>
<span class="sd">    beta : Tensor, shape=(..., seq_len, seq_len, n_head), optional</span>
<span class="sd">        Additional bias terms</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    a_out : Tensor, shape=(..., seq_len, c_a)</span>
<span class="sd">        Updated representation after attention with pair bias</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># Algorithm 24 Steps 1-4: Input projections</span>
    <span class="c1"># Step 1: if {si} ≠ ∅ then</span>
    <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;ada_ln&quot;</span><span class="p">):</span>
        <span class="c1"># Step 2: ai ← AdaLN(ai, si)</span>
        <span class="n">a_normed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ada_ln</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Step 4: ai ← LayerNorm(ai)</span>
        <span class="n">a_normed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="c1"># Step 6: q_i^h = Linear(ai)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">a_normed</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

    <span class="c1"># Step 7: k_i^h, v_i^h = LinearNoBias(ai)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">a_normed</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">a_normed</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

    <span class="c1"># Step 8: b_ij^h ← LinearNoBias(LayerNorm(zij)) + βij</span>
    <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">z_normed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_z</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">b_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_normed</span><span class="p">)</span>  <span class="c1"># Shape: (..., seq_len, seq_len, n_head)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span>
            <span class="n">seq_len</span><span class="p">,</span>
            <span class="n">seq_len</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b_z</span> <span class="o">+</span> <span class="n">beta</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b_z</span>

    <span class="c1"># Step 9: g_i^h ← sigmoid(LinearNoBias(ai))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">a_normed</span><span class="p">))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

    <span class="c1"># Step 10: A_ij^h ← softmax_j(1/√c * q_i^h * k_j^h + b_ij^h)</span>
    <span class="c1"># Compute attention scores: q @ k.T / √d + bias</span>
    <span class="c1"># q: (..., seq_len, n_head, head_dim)</span>
    <span class="c1"># k: (..., seq_len, n_head, head_dim)</span>
    <span class="c1"># Want: (..., n_head, seq_len, seq_len)</span>
    <span class="n">attn_logits</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...ihd,...jhd-&gt;...hij&quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
    <span class="p">)</span>  <span class="c1"># Shape: (..., n_head, seq_len, seq_len)</span>

    <span class="c1"># Add bias: b has shape (..., seq_len, seq_len, n_head)</span>
    <span class="c1"># We need to transpose to match attention shape</span>
    <span class="n">b_transposed</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., n_head, seq_len, seq_len)</span>
    <span class="n">attn_logits</span> <span class="o">=</span> <span class="n">attn_logits</span> <span class="o">+</span> <span class="n">b_transposed</span>

    <span class="c1"># Apply softmax over the last dimension (keys)</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Step 11: ai ← LinearNoBias(concat_h(g_i^h ⊙ Σ_j A_ij^h v_j^h))</span>
    <span class="c1"># Apply attention to values and gate</span>
    <span class="c1"># attn_weights: (..., n_head, seq_len, seq_len)</span>
    <span class="c1"># v: (..., seq_len, n_head, head_dim)</span>
    <span class="n">attended_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
        <span class="s2">&quot;...hij,...jhd-&gt;...hid&quot;</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span>
    <span class="p">)</span>  <span class="c1"># Shape: (..., n_head, seq_len, head_dim)</span>

    <span class="c1"># Reshape g to match attended_v shape: (..., seq_len, n_head, head_dim) -&gt; (..., n_head, seq_len, head_dim)</span>
    <span class="n">g_reshaped</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Shape: (..., n_head, seq_len, head_dim)</span>

    <span class="c1"># Apply gating: g ⊙ attended_v</span>
    <span class="n">gated_output</span> <span class="o">=</span> <span class="n">g_reshaped</span> <span class="o">*</span> <span class="n">attended_v</span>  <span class="c1"># Element-wise multiplication</span>

    <span class="c1"># Concatenate heads: reshape to (..., seq_len, c_a)</span>
    <span class="n">concat_output</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">gated_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_a</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Linear projection</span>
    <span class="n">a_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span><span class="n">concat_output</span><span class="p">)</span>

    <span class="c1"># Algorithm 24 Steps 12-14: Output projection (from adaLN-Zero)</span>
    <span class="c1"># Step 12: if {si} ≠ ∅ then</span>
    <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;linear_s_gate&quot;</span><span class="p">):</span>
        <span class="c1"># Step 13: ai ← sigmoid(Linear(si, biasinit=-2.0)) ⊙ ai</span>
        <span class="n">s_gate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_s_gate</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
        <span class="n">a_out</span> <span class="o">=</span> <span class="n">s_gate</span> <span class="o">*</span> <span class="n">a_out</span>

    <span class="c1"># Step 15: return {ai}</span>
    <span class="k">return</span> <span class="n">a_out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.DiffusionTransformer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.DiffusionTransformer</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Diffusion Transformer from AlphaFold 3 Algorithm 23.</p>
<p>This implements a transformer block for diffusion models that alternates between
AttentionPairBias and ConditionedTransitionBlock operations. The module processes
single representations {ai} conditioned on {si}, pair representations {zij},
and bias terms {βij}.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c_a</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for single representation 'a'</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_s</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for conditioning signal 's'</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_z</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for pair representation 'z'</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_block</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of transformer blocks</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Expansion factor for ConditionedTransitionBlock</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_a</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_block</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">DiffusionTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">c_a</span><span class="o">=</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span> <span class="n">n_block</span><span class="o">=</span><span class="n">n_block</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_s</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_head</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_out</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 32, 256])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 Algorithm 23: Diffusion Transformer</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_diffusion_transformer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DiffusionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Diffusion Transformer from AlphaFold 3 Algorithm 23.</span>

<span class="sd">    This implements a transformer block for diffusion models that alternates between</span>
<span class="sd">    AttentionPairBias and ConditionedTransitionBlock operations. The module processes</span>
<span class="sd">    single representations {ai} conditioned on {si}, pair representations {zij},</span>
<span class="sd">    and bias terms {βij}.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c_a : int</span>
<span class="sd">        Channel dimension for single representation &#39;a&#39;</span>
<span class="sd">    c_s : int</span>
<span class="sd">        Channel dimension for conditioning signal &#39;s&#39;</span>
<span class="sd">    c_z : int</span>
<span class="sd">        Channel dimension for pair representation &#39;z&#39;</span>
<span class="sd">    n_head : int</span>
<span class="sd">        Number of attention heads</span>
<span class="sd">    n_block : int</span>
<span class="sd">        Number of transformer blocks</span>
<span class="sd">    n : int, default=2</span>
<span class="sd">        Expansion factor for ConditionedTransitionBlock</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import DiffusionTransformer</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len, c_a, c_s, c_z = 2, 32, 256, 384, 128</span>
<span class="sd">    &gt;&gt;&gt; n_head, n_block = 16, 4</span>
<span class="sd">    &gt;&gt;&gt; module = DiffusionTransformer(</span>
<span class="sd">    ...     c_a=c_a, c_s=c_s, c_z=c_z,</span>
<span class="sd">    ...     n_head=n_head, n_block=n_block</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; a = torch.randn(batch_size, seq_len, c_a)</span>
<span class="sd">    &gt;&gt;&gt; s = torch.randn(batch_size, seq_len, c_s)</span>
<span class="sd">    &gt;&gt;&gt; z = torch.randn(batch_size, seq_len, seq_len, c_z)</span>
<span class="sd">    &gt;&gt;&gt; beta = torch.randn(batch_size, seq_len, seq_len, n_head)</span>
<span class="sd">    &gt;&gt;&gt; a_out = module(a, s, z, beta)</span>
<span class="sd">    &gt;&gt;&gt; a_out.shape</span>
<span class="sd">    torch.Size([2, 32, 256])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 Algorithm 23: Diffusion Transformer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">c_a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">c_s</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">c_z</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_a</span> <span class="o">=</span> <span class="n">c_a</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_s</span> <span class="o">=</span> <span class="n">c_s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span> <span class="o">=</span> <span class="n">c_z</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_block</span> <span class="o">=</span> <span class="n">n_block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span>

        <span class="c1"># Create n_block pairs of (AttentionPairBias, ConditionedTransitionBlock)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_block</span><span class="p">):</span>
            <span class="c1"># Each block contains:</span>
            <span class="c1"># 1. AttentionPairBias for step 2</span>
            <span class="c1"># 2. ConditionedTransitionBlock for step 3</span>
            <span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="n">AttentionPairBias</span><span class="p">(</span>
                        <span class="n">c_a</span><span class="o">=</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;transition&quot;</span><span class="p">:</span> <span class="n">_ConditionedTransitionBlock</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c_a</span><span class="p">,</span> <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">),</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of Diffusion Transformer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        a : Tensor, shape=(batch_size, seq_len, c_a)</span>
<span class="sd">            Single representations</span>
<span class="sd">        s : Tensor, shape=(batch_size, seq_len, c_s)</span>
<span class="sd">            Conditioning signal</span>
<span class="sd">        z : Tensor, shape=(batch_size, seq_len, seq_len, c_z)</span>
<span class="sd">            Pair representations</span>
<span class="sd">        beta : Tensor, shape=(batch_size, seq_len, seq_len, n_head)</span>
<span class="sd">            Bias terms for attention</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        a_out : Tensor, shape=(batch_size, seq_len, c_a)</span>
<span class="sd">            Updated single representations after diffusion transformer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Algorithm 23: for all n ∈ [1, ..., N_block] do</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="c1"># Algorithm 23 Step 2: {bi} = AttentionPairBias({ai}, {si}, {zij}, {βij}, N_head)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s2">&quot;attention&quot;</span><span class="p">](</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

            <span class="c1"># Algorithm 23 Step 3: ai ← bi + ConditionedTransitionBlock(ai, si)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="n">block</span><span class="p">[</span><span class="s2">&quot;transition&quot;</span><span class="p">](</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

        <span class="c1"># Algorithm 23 Step 5: return {ai}</span>
        <span class="k">return</span> <span class="n">a</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.DiffusionTransformer.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of Diffusion Transformer.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>a</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, seq_len, c_a)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Single representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, seq_len, c_s)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Conditioning signal</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>z</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, seq_len, seq_len, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>beta</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, seq_len, seq_len, n_head)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Bias terms for attention</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>a_out</code></td>            <td>
                  <code>Tensor, shape=(batch_size, seq_len, c_a)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated single representations after diffusion transformer</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_diffusion_transformer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of Diffusion Transformer.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : Tensor, shape=(batch_size, seq_len, c_a)</span>
<span class="sd">        Single representations</span>
<span class="sd">    s : Tensor, shape=(batch_size, seq_len, c_s)</span>
<span class="sd">        Conditioning signal</span>
<span class="sd">    z : Tensor, shape=(batch_size, seq_len, seq_len, c_z)</span>
<span class="sd">        Pair representations</span>
<span class="sd">    beta : Tensor, shape=(batch_size, seq_len, seq_len, n_head)</span>
<span class="sd">        Bias terms for attention</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    a_out : Tensor, shape=(batch_size, seq_len, c_a)</span>
<span class="sd">        Updated single representations after diffusion transformer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Algorithm 23: for all n ∈ [1, ..., N_block] do</span>
    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
        <span class="c1"># Algorithm 23 Step 2: {bi} = AttentionPairBias({ai}, {si}, {zij}, {βij}, N_head)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">block</span><span class="p">[</span><span class="s2">&quot;attention&quot;</span><span class="p">](</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

        <span class="c1"># Algorithm 23 Step 3: ai ← bi + ConditionedTransitionBlock(ai, si)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="n">block</span><span class="p">[</span><span class="s2">&quot;transition&quot;</span><span class="p">](</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

    <span class="c1"># Algorithm 23 Step 5: return {ai}</span>
    <span class="k">return</span> <span class="n">a</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.MSA" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.MSA</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Multiple Sequence Alignment Module from AlphaFold 3.</p>
<p>This implements Algorithm 8 from AlphaFold 3, which is a complete MSA processing
module that combines multiple sub-modules in a structured way:</p>
<ol>
<li>MSA representation initialization and random sampling</li>
<li>Communication block with OuterProductMean</li>
<li>MSA stack with MSAPairWeightedAveraging and Transition</li>
<li>Pair stack with triangle updates and attention</li>
</ol>
<p>The module processes MSA features, single representations, and pair representations
through multiple blocks to capture complex evolutionary and structural patterns.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_block</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of processing blocks</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_m</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for MSA representation</p>
              </div>
            </td>
            <td>
                  <code>64</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_z</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for pair representation</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_s</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for single representation</p>
              </div>
            </td>
            <td>
                  <code>256</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head_msa</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads for MSA operations</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head_pair</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads for pair operations</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout_rate</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout rate for MSA operations</p>
              </div>
            </td>
            <td>
                  <code>0.15</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">MSA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_seq</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c_m</span><span class="p">,</span> <span class="n">c_z</span><span class="p">,</span> <span class="n">c_s</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">MSA</span><span class="p">(</span><span class="n">n_block</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c_m</span><span class="o">=</span><span class="n">c_m</span><span class="p">,</span> <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span> <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Input features</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_msa</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_seq</span><span class="p">,</span> <span class="mi">23</span><span class="p">)</span>  <span class="c1"># MSA features</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_has_deletion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_seq</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_deletion_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_seq</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_s</span><span class="p">)</span>  <span class="c1"># Single inputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_z</span><span class="p">)</span>  <span class="c1"># Pair representation</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Forward pass</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updated_z_ij</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">f_msa</span><span class="p">,</span> <span class="n">f_has_deletion</span><span class="p">,</span> <span class="n">f_deletion_value</span><span class="p">,</span> <span class="n">s_inputs</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updated_z_ij</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 32, 32, 128])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 paper, Algorithm 8: MSA Module</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_msa.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MSA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple Sequence Alignment Module from AlphaFold 3.</span>

<span class="sd">    This implements Algorithm 8 from AlphaFold 3, which is a complete MSA processing</span>
<span class="sd">    module that combines multiple sub-modules in a structured way:</span>

<span class="sd">    1. MSA representation initialization and random sampling</span>
<span class="sd">    2. Communication block with OuterProductMean</span>
<span class="sd">    3. MSA stack with MSAPairWeightedAveraging and Transition</span>
<span class="sd">    4. Pair stack with triangle updates and attention</span>

<span class="sd">    The module processes MSA features, single representations, and pair representations</span>
<span class="sd">    through multiple blocks to capture complex evolutionary and structural patterns.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_block : int, default=4</span>
<span class="sd">        Number of processing blocks</span>
<span class="sd">    c_m : int, default=64</span>
<span class="sd">        Channel dimension for MSA representation</span>
<span class="sd">    c_z : int, default=128</span>
<span class="sd">        Channel dimension for pair representation</span>
<span class="sd">    c_s : int, default=256</span>
<span class="sd">        Channel dimension for single representation</span>
<span class="sd">    n_head_msa : int, default=8</span>
<span class="sd">        Number of attention heads for MSA operations</span>
<span class="sd">    n_head_pair : int, default=4</span>
<span class="sd">        Number of attention heads for pair operations</span>
<span class="sd">    dropout_rate : float, default=0.15</span>
<span class="sd">        Dropout rate for MSA operations</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import MSA</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len, n_seq = 2, 32, 16</span>
<span class="sd">    &gt;&gt;&gt; c_m, c_z, c_s = 64, 128, 256</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; module = MSA(n_block=2, c_m=c_m, c_z=c_z, c_s=c_s)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Input features</span>
<span class="sd">    &gt;&gt;&gt; f_msa = torch.randn(batch_size, seq_len, n_seq, 23)  # MSA features</span>
<span class="sd">    &gt;&gt;&gt; f_has_deletion = torch.randn(batch_size, seq_len, n_seq, 1)</span>
<span class="sd">    &gt;&gt;&gt; f_deletion_value = torch.randn(batch_size, seq_len, n_seq, 1)</span>
<span class="sd">    &gt;&gt;&gt; s_inputs = torch.randn(batch_size, seq_len, c_s)  # Single inputs</span>
<span class="sd">    &gt;&gt;&gt; z_ij = torch.randn(batch_size, seq_len, seq_len, c_z)  # Pair representation</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Forward pass</span>
<span class="sd">    &gt;&gt;&gt; updated_z_ij = module(f_msa, f_has_deletion, f_deletion_value, s_inputs, z_ij)</span>
<span class="sd">    &gt;&gt;&gt; updated_z_ij.shape</span>
<span class="sd">    torch.Size([2, 32, 32, 128])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 paper, Algorithm 8: MSA Module</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_block</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">c_m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">c_z</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">c_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">n_head_msa</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">n_head_pair</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_block</span> <span class="o">=</span> <span class="n">n_block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_m</span> <span class="o">=</span> <span class="n">c_m</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span> <span class="o">=</span> <span class="n">c_z</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_s</span> <span class="o">=</span> <span class="n">c_s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head_msa</span> <span class="o">=</span> <span class="n">n_head_msa</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head_pair</span> <span class="o">=</span> <span class="n">n_head_pair</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>

        <span class="c1"># Step 3: Initial linear projection for MSA (concatenated features -&gt; c_m)</span>
        <span class="c1"># Input features: f_msa (23) + f_has_deletion (1) + f_deletion_value (1) = 25 channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msa_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">c_m</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Step 4: Linear projection for single inputs (s_inputs -&gt; c_m)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_m</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Communication: OuterProductMean (step 6)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_product_mean</span> <span class="o">=</span> <span class="n">_OuterProductMean</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c_m</span><span class="p">,</span> <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">)</span>

        <span class="c1"># MSA stack components (step 7-8)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msa_pair_weighted_averaging</span> <span class="o">=</span> <span class="n">_MSAPairWeightedAveraging</span><span class="p">(</span>
            <span class="n">c_m</span><span class="o">=</span><span class="n">c_m</span><span class="p">,</span> <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head_msa</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msa_transition</span> <span class="o">=</span> <span class="n">Transition</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c_m</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msa_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

        <span class="c1"># Pair stack components (step 9-13)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">triangle_mult_outgoing</span> <span class="o">=</span> <span class="n">TriangleMultiplicationOutgoing</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c_z</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">triangle_mult_incoming</span> <span class="o">=</span> <span class="n">TriangleMultiplicationIncoming</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c_z</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">triangle_attention_starting</span> <span class="o">=</span> <span class="n">TriangleAttentionStartingNode</span><span class="p">(</span>
            <span class="n">c</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head_pair</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">triangle_attention_ending</span> <span class="o">=</span> <span class="n">TriangleAttentionEndingNode</span><span class="p">(</span>
            <span class="n">c</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head_pair</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_transition</span> <span class="o">=</span> <span class="n">Transition</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

        <span class="c1"># Dropout layers for pair operations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_rowwise</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>  <span class="c1"># For steps 9,10,11</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_columnwise</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>  <span class="c1"># For step 12</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">f_msa</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">f_has_deletion</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">f_deletion_value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">s_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of the MSA Module.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        f_msa : Tensor, shape=(..., s, n_seq, 23)</span>
<span class="sd">            MSA features (amino acid profiles)</span>
<span class="sd">        f_has_deletion : Tensor, shape=(..., s, n_seq, 1)</span>
<span class="sd">            Has deletion features</span>
<span class="sd">        f_deletion_value : Tensor, shape=(..., s, n_seq, 1)</span>
<span class="sd">            Deletion value features</span>
<span class="sd">        s_inputs : Tensor, shape=(..., s, c_s)</span>
<span class="sd">            Single representation inputs</span>
<span class="sd">        z_ij : Tensor, shape=(..., s, s, c_z)</span>
<span class="sd">            Pair representation</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z_ij : Tensor, shape=(..., s, s, c_z)</span>
<span class="sd">            Updated pair representation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Input validation</span>
        <span class="n">seq_len_msa</span> <span class="o">=</span> <span class="n">f_msa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">seq_len_single</span> <span class="o">=</span> <span class="n">s_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">n_seq_msa</span> <span class="o">=</span> <span class="n">f_msa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">seq_len_msa</span> <span class="o">!=</span> <span class="n">seq_len_single</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Sequence length mismatch: MSA has </span><span class="si">{</span><span class="n">seq_len_msa</span><span class="si">}</span><span class="s2"> residues, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;single representation has </span><span class="si">{</span><span class="n">seq_len_single</span><span class="si">}</span><span class="s2"> residues&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check MSA feature consistency</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">f_has_deletion</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="n">seq_len_msa</span>
            <span class="ow">or</span> <span class="n">f_deletion_value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="n">seq_len_msa</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All MSA features must have the same sequence length&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">f_has_deletion</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_seq_msa</span>
            <span class="ow">or</span> <span class="n">f_deletion_value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_seq_msa</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All MSA features must have the same number of sequences&quot;</span><span class="p">)</span>

        <span class="c1"># Check pair representation compatibility</span>
        <span class="k">if</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">seq_len_msa</span> <span class="ow">or</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="n">seq_len_msa</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Pair representation shape mismatch: expected (</span><span class="si">{</span><span class="n">seq_len_msa</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">seq_len_msa</span><span class="si">}</span><span class="s2">), &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;got (</span><span class="si">{</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="p">)</span>

        <span class="n">m_si</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msa_linear</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">f_msa</span><span class="p">,</span>
                    <span class="n">f_has_deletion</span><span class="p">,</span>
                    <span class="n">f_deletion_value</span><span class="p">,</span>
                <span class="p">],</span>
                <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">single_linear</span><span class="p">(</span><span class="n">s_inputs</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Step 5: Process through N_block iterations</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_block</span><span class="p">):</span>
            <span class="c1"># Step 6: Communication - OuterProductMean</span>
            <span class="c1"># OuterProductMean now properly handles MSA sequences and computes mean over outer products</span>
            <span class="c1"># Pass the full MSA representation to capture coevolutionary information</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_product_mean</span><span class="p">(</span><span class="n">m_si</span><span class="p">)</span>  <span class="c1"># m_si: (..., s, n_seq, c_m)</span>

            <span class="c1"># MSA stack (steps 7-8)</span>
            <span class="c1"># Step 7: MSA Pair Weighted Averaging with dropout</span>
            <span class="n">m_si</span> <span class="o">=</span> <span class="n">m_si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">msa_dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msa_pair_weighted_averaging</span><span class="p">(</span><span class="n">m_si</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">))</span>

            <span class="c1"># Step 8: MSA Transition</span>
            <span class="n">m_si</span> <span class="o">=</span> <span class="n">m_si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">msa_transition</span><span class="p">(</span><span class="n">m_si</span><span class="p">)</span>

            <span class="c1"># Pair stack (steps 9-13)</span>
            <span class="c1"># Step 9: Triangle Multiplication Outgoing with rowwise dropout</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_rowwise</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">triangle_mult_outgoing</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span>

            <span class="c1"># Step 10: Triangle Multiplication Incoming with rowwise dropout</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_rowwise</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">triangle_mult_incoming</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span>

            <span class="c1"># Step 11: Triangle Attention Starting Node with rowwise dropout</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_rowwise</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">triangle_attention_starting</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># Step 12: Triangle Attention Ending Node with columnwise dropout</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_columnwise</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">triangle_attention_ending</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># Step 13: Pair Transition</span>
            <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_transition</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">z_ij</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.MSA.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">f_msa</span><span class="p">,</span> <span class="n">f_has_deletion</span><span class="p">,</span> <span class="n">f_deletion_value</span><span class="p">,</span> <span class="n">s_inputs</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of the MSA Module.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>f_msa</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, n_seq, 23)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>MSA features (amino acid profiles)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>f_has_deletion</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, n_seq, 1)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Has deletion features</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>f_deletion_value</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, n_seq, 1)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deletion value features</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_inputs</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, c_s)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Single representation inputs</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>z_ij</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, s, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair representation</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>z_ij</code></td>            <td>
                  <code>Tensor, shape=(..., s, s, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated pair representation</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_msa.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">f_msa</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">f_has_deletion</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">f_deletion_value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">s_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of the MSA Module.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    f_msa : Tensor, shape=(..., s, n_seq, 23)</span>
<span class="sd">        MSA features (amino acid profiles)</span>
<span class="sd">    f_has_deletion : Tensor, shape=(..., s, n_seq, 1)</span>
<span class="sd">        Has deletion features</span>
<span class="sd">    f_deletion_value : Tensor, shape=(..., s, n_seq, 1)</span>
<span class="sd">        Deletion value features</span>
<span class="sd">    s_inputs : Tensor, shape=(..., s, c_s)</span>
<span class="sd">        Single representation inputs</span>
<span class="sd">    z_ij : Tensor, shape=(..., s, s, c_z)</span>
<span class="sd">        Pair representation</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z_ij : Tensor, shape=(..., s, s, c_z)</span>
<span class="sd">        Updated pair representation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Input validation</span>
    <span class="n">seq_len_msa</span> <span class="o">=</span> <span class="n">f_msa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">seq_len_single</span> <span class="o">=</span> <span class="n">s_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">n_seq_msa</span> <span class="o">=</span> <span class="n">f_msa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">seq_len_msa</span> <span class="o">!=</span> <span class="n">seq_len_single</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Sequence length mismatch: MSA has </span><span class="si">{</span><span class="n">seq_len_msa</span><span class="si">}</span><span class="s2"> residues, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;single representation has </span><span class="si">{</span><span class="n">seq_len_single</span><span class="si">}</span><span class="s2"> residues&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Check MSA feature consistency</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">f_has_deletion</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="n">seq_len_msa</span>
        <span class="ow">or</span> <span class="n">f_deletion_value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="n">seq_len_msa</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All MSA features must have the same sequence length&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">f_has_deletion</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_seq_msa</span>
        <span class="ow">or</span> <span class="n">f_deletion_value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_seq_msa</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All MSA features must have the same number of sequences&quot;</span><span class="p">)</span>

    <span class="c1"># Check pair representation compatibility</span>
    <span class="k">if</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">seq_len_msa</span> <span class="ow">or</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="n">seq_len_msa</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Pair representation shape mismatch: expected (</span><span class="si">{</span><span class="n">seq_len_msa</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">seq_len_msa</span><span class="si">}</span><span class="s2">), &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;got (</span><span class="si">{</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

    <span class="n">m_si</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msa_linear</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">f_msa</span><span class="p">,</span>
                <span class="n">f_has_deletion</span><span class="p">,</span>
                <span class="n">f_deletion_value</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">single_linear</span><span class="p">(</span><span class="n">s_inputs</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Step 5: Process through N_block iterations</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_block</span><span class="p">):</span>
        <span class="c1"># Step 6: Communication - OuterProductMean</span>
        <span class="c1"># OuterProductMean now properly handles MSA sequences and computes mean over outer products</span>
        <span class="c1"># Pass the full MSA representation to capture coevolutionary information</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">outer_product_mean</span><span class="p">(</span><span class="n">m_si</span><span class="p">)</span>  <span class="c1"># m_si: (..., s, n_seq, c_m)</span>

        <span class="c1"># MSA stack (steps 7-8)</span>
        <span class="c1"># Step 7: MSA Pair Weighted Averaging with dropout</span>
        <span class="n">m_si</span> <span class="o">=</span> <span class="n">m_si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">msa_dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msa_pair_weighted_averaging</span><span class="p">(</span><span class="n">m_si</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">))</span>

        <span class="c1"># Step 8: MSA Transition</span>
        <span class="n">m_si</span> <span class="o">=</span> <span class="n">m_si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">msa_transition</span><span class="p">(</span><span class="n">m_si</span><span class="p">)</span>

        <span class="c1"># Pair stack (steps 9-13)</span>
        <span class="c1"># Step 9: Triangle Multiplication Outgoing with rowwise dropout</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_rowwise</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">triangle_mult_outgoing</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span>

        <span class="c1"># Step 10: Triangle Multiplication Incoming with rowwise dropout</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_rowwise</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">triangle_mult_incoming</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span>

        <span class="c1"># Step 11: Triangle Attention Starting Node with rowwise dropout</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_rowwise</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">triangle_attention_starting</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Step 12: Triangle Attention Ending Node with columnwise dropout</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_dropout_columnwise</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">triangle_attention_ending</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Step 13: Pair Transition</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="n">z_ij</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pair_transition</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">z_ij</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.PairformerStack" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.PairformerStack</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Pairformer stack from AlphaFold 3 Algorithm 17.</p>
<p>This is the exact implementation of the Pairformer stack as specified
in Algorithm 17, which processes single and pair representations through
N_block iterations of triangle operations and attention mechanisms.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_block</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of Pairformer blocks (N_block in Algorithm 17)</p>
              </div>
            </td>
            <td>
                  <code>48</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_s</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for single representation</p>
              </div>
            </td>
            <td>
                  <code>384</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_z</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for pair representation</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head_single</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads for single representation</p>
              </div>
            </td>
            <td>
                  <code>16</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head_pair</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads for pair representation</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout_rate</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout rate as specified in Algorithm 17</p>
              </div>
            </td>
            <td>
                  <code>0.25</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transition_n</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiplier for transition layer hidden dimension</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">PairformerStack</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_block</span><span class="p">,</span> <span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">PairformerStack</span><span class="p">(</span><span class="n">n_block</span><span class="o">=</span><span class="n">n_block</span><span class="p">,</span> <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span> <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_s</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c_z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_out</span><span class="p">,</span> <span class="n">z_out</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 10, 384])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 10, 10, 128])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] Abramson, J., Adler, J., Dunger, J. et al. Accurate structure prediction
       of biomolecular interactions with AlphaFold 3. Nature 630, 493–500 (2024).
       Algorithm 17: Pairformer stack</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_pairformer_stack.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PairformerStack</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pairformer stack from AlphaFold 3 Algorithm 17.</span>

<span class="sd">    This is the exact implementation of the Pairformer stack as specified</span>
<span class="sd">    in Algorithm 17, which processes single and pair representations through</span>
<span class="sd">    N_block iterations of triangle operations and attention mechanisms.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_block : int, default=48</span>
<span class="sd">        Number of Pairformer blocks (N_block in Algorithm 17)</span>
<span class="sd">    c_s : int, default=384</span>
<span class="sd">        Channel dimension for single representation</span>
<span class="sd">    c_z : int, default=128</span>
<span class="sd">        Channel dimension for pair representation</span>
<span class="sd">    n_head_single : int, default=16</span>
<span class="sd">        Number of attention heads for single representation</span>
<span class="sd">    n_head_pair : int, default=4</span>
<span class="sd">        Number of attention heads for pair representation</span>
<span class="sd">    dropout_rate : float, default=0.25</span>
<span class="sd">        Dropout rate as specified in Algorithm 17</span>
<span class="sd">    transition_n : int, default=4</span>
<span class="sd">        Multiplier for transition layer hidden dimension</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import PairformerStack</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len = 2, 10</span>
<span class="sd">    &gt;&gt;&gt; n_block, c_s, c_z = 4, 384, 128</span>
<span class="sd">    &gt;&gt;&gt; module = PairformerStack(n_block=n_block, c_s=c_s, c_z=c_z)</span>
<span class="sd">    &gt;&gt;&gt; s_i = torch.randn(batch_size, seq_len, c_s)</span>
<span class="sd">    &gt;&gt;&gt; z_ij = torch.randn(batch_size, seq_len, seq_len, c_z)</span>
<span class="sd">    &gt;&gt;&gt; s_out, z_out = module(s_i, z_ij)</span>
<span class="sd">    &gt;&gt;&gt; s_out.shape</span>
<span class="sd">    torch.Size([2, 10, 384])</span>
<span class="sd">    &gt;&gt;&gt; z_out.shape</span>
<span class="sd">    torch.Size([2, 10, 10, 128])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Abramson, J., Adler, J., Dunger, J. et al. Accurate structure prediction</span>
<span class="sd">           of biomolecular interactions with AlphaFold 3. Nature 630, 493–500 (2024).</span>
<span class="sd">           Algorithm 17: Pairformer stack</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_block</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span><span class="p">,</span>
        <span class="n">c_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">384</span><span class="p">,</span>
        <span class="n">c_z</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">n_head_single</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">n_head_pair</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">transition_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_block</span> <span class="o">=</span> <span class="n">n_block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_s</span> <span class="o">=</span> <span class="n">c_s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span> <span class="o">=</span> <span class="n">c_z</span>

        <span class="c1"># Create n_block Pairformer stack blocks (each with its own parameters)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">_PairformerStackBlock</span><span class="p">(</span>
                    <span class="n">c_s</span><span class="o">=</span><span class="n">c_s</span><span class="p">,</span>
                    <span class="n">c_z</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
                    <span class="n">n_head_single</span><span class="o">=</span><span class="n">n_head_single</span><span class="p">,</span>
                    <span class="n">n_head_pair</span><span class="o">=</span><span class="n">n_head_pair</span><span class="p">,</span>
                    <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
                    <span class="n">transition_n</span><span class="o">=</span><span class="n">transition_n</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_block</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_i</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of Pairformer stack.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        s_i : Tensor, shape=(..., s, c_s)</span>
<span class="sd">            Single representation where s is sequence length</span>
<span class="sd">        z_ij : Tensor, shape=(..., s, s, c_z)</span>
<span class="sd">            Pair representation</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        s_out : Tensor, shape=(..., s, c_s)</span>
<span class="sd">            Updated single representation after all blocks</span>
<span class="sd">        z_out : Tensor, shape=(..., s, s, c_z)</span>
<span class="sd">            Updated pair representation after all blocks</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Validate input shapes</span>
        <span class="k">if</span> <span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="ow">or</span> <span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Sequence length mismatch: single representation has </span><span class="si">{</span><span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;residues but pair representation has shape </span><span class="si">{</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_s</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Single representation has </span><span class="si">{</span><span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> channels, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">c_s</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Pair representation has </span><span class="si">{</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> channels, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Algorithm 17: for all l ∈ [1, ..., N_block] do</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>

        <span class="c1"># Algorithm 17 step 10: return {s_i}, {z_ij}</span>
        <span class="k">return</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.PairformerStack.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of Pairformer stack.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>s_i</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, c_s)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Single representation where s is sequence length</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>z_ij</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, s, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair representation</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>s_out</code></td>            <td>
                  <code>Tensor, shape=(..., s, c_s)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated single representation after all blocks</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>z_out</code></td>            <td>
                  <code>Tensor, shape=(..., s, s, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated pair representation after all blocks</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_pairformer_stack.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_i</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of Pairformer stack.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    s_i : Tensor, shape=(..., s, c_s)</span>
<span class="sd">        Single representation where s is sequence length</span>
<span class="sd">    z_ij : Tensor, shape=(..., s, s, c_z)</span>
<span class="sd">        Pair representation</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    s_out : Tensor, shape=(..., s, c_s)</span>
<span class="sd">        Updated single representation after all blocks</span>
<span class="sd">    z_out : Tensor, shape=(..., s, s, c_z)</span>
<span class="sd">        Updated pair representation after all blocks</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Validate input shapes</span>
    <span class="k">if</span> <span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="ow">or</span> <span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Sequence length mismatch: single representation has </span><span class="si">{</span><span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;residues but pair representation has shape </span><span class="si">{</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_s</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Single representation has </span><span class="si">{</span><span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> channels, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">c_s</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Pair representation has </span><span class="si">{</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> channels, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Algorithm 17: for all l ∈ [1, ..., N_block] do</span>
    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
        <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>

    <span class="c1"># Algorithm 17 step 10: return {s_i}, {z_ij}</span>
    <span class="k">return</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">z_ij</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.RelativePositionEncoding" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.RelativePositionEncoding</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Relative Position Encoding for AlphaFold 3.</p>
<p>This module implements Algorithm 3 exactly, computing relative position
encodings based on asymmetric ID, residue index, entity ID, token index,
and chain ID information.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>r_max</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum residue separation for clipping</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_max</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum chain separation for clipping</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_z</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Output channel dimension</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">RelativePositionEncoding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">RelativePositionEncoding</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_star</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;asym_id&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="s1">&#39;residue_index&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;entity_id&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="s1">&#39;token_index&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s1">&#39;sym_id&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">)),</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p_ij</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">f_star</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p_ij</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 100, 100, 128])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 Algorithm 3: Relative position encoding</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_relative_position_encoding.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">RelativePositionEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Relative Position Encoding for AlphaFold 3.</span>

<span class="sd">    This module implements Algorithm 3 exactly, computing relative position</span>
<span class="sd">    encodings based on asymmetric ID, residue index, entity ID, token index,</span>
<span class="sd">    and chain ID information.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    r_max : int, default=32</span>
<span class="sd">        Maximum residue separation for clipping</span>
<span class="sd">    s_max : int, default=2</span>
<span class="sd">        Maximum chain separation for clipping</span>
<span class="sd">    c_z : int, default=128</span>
<span class="sd">        Output channel dimension</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import RelativePositionEncoding</span>
<span class="sd">    &gt;&gt;&gt; batch_size, n_tokens = 2, 100</span>
<span class="sd">    &gt;&gt;&gt; module = RelativePositionEncoding()</span>
<span class="sd">    &gt;&gt;&gt; f_star = {</span>
<span class="sd">    ...     &#39;asym_id&#39;: torch.randint(0, 5, (batch_size, n_tokens)),</span>
<span class="sd">    ...     &#39;residue_index&#39;: torch.arange(n_tokens).unsqueeze(0).expand(batch_size, -1),</span>
<span class="sd">    ...     &#39;entity_id&#39;: torch.randint(0, 3, (batch_size, n_tokens)),</span>
<span class="sd">    ...     &#39;token_index&#39;: torch.arange(n_tokens).unsqueeze(0).expand(batch_size, -1),</span>
<span class="sd">    ...     &#39;sym_id&#39;: torch.randint(0, 10, (batch_size, n_tokens)),</span>
<span class="sd">    ... }</span>
<span class="sd">    &gt;&gt;&gt; p_ij = module(f_star)</span>
<span class="sd">    &gt;&gt;&gt; p_ij.shape</span>
<span class="sd">    torch.Size([2, 100, 100, 128])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 Algorithm 3: Relative position encoding</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">s_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c_z</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">=</span> <span class="n">r_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">=</span> <span class="n">s_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span> <span class="o">=</span> <span class="n">c_z</span>

        <span class="c1"># Final linear projection</span>
        <span class="c1"># 2 rel distance features (residue, token) * (2*r_max+2) + 1 same_entity + 1 chain feature * (2*s_max+2)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">r_max</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">s_max</span> <span class="o">+</span> <span class="mi">2</span><span class="p">),</span> <span class="n">c_z</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass implementing Algorithm 3 exactly.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        f_star : dict</span>
<span class="sd">            Dictionary containing features with keys:</span>
<span class="sd">            - &#39;asym_id&#39;: asymmetric unit IDs (batch, n_tokens)</span>
<span class="sd">            - &#39;residue_index&#39;: residue indices (batch, n_tokens)</span>
<span class="sd">            - &#39;entity_id&#39;: entity IDs (batch, n_tokens)</span>
<span class="sd">            - &#39;token_index&#39;: token indices (batch, n_tokens)</span>
<span class="sd">            - &#39;sym_id&#39;: symmetry IDs (batch, n_tokens)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p_ij : Tensor, shape=(batch, n_tokens, n_tokens, c_z)</span>
<span class="sd">            Relative position encodings</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Extract features</span>
        <span class="n">asym_id_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;asym_id&quot;</span><span class="p">]</span>  <span class="c1"># (batch, n_tokens)</span>
        <span class="n">residue_index_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;residue_index&quot;</span><span class="p">]</span>
        <span class="n">entity_id_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;entity_id&quot;</span><span class="p">]</span>
        <span class="n">token_index_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;token_index&quot;</span><span class="p">]</span>
        <span class="n">sym_id_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;sym_id&quot;</span><span class="p">]</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">=</span> <span class="n">asym_id_i</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">asym_id_i</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># Create pairwise comparisons</span>
        <span class="n">asym_id_j</span> <span class="o">=</span> <span class="n">asym_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens, 1)</span>
        <span class="n">asym_id_i</span> <span class="o">=</span> <span class="n">asym_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, 1, n_tokens)</span>

        <span class="n">residue_index_j</span> <span class="o">=</span> <span class="n">residue_index_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">residue_index_i</span> <span class="o">=</span> <span class="n">residue_index_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">entity_id_j</span> <span class="o">=</span> <span class="n">entity_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">entity_id_i</span> <span class="o">=</span> <span class="n">entity_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">token_index_j</span> <span class="o">=</span> <span class="n">token_index_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">token_index_i</span> <span class="o">=</span> <span class="n">token_index_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">sym_id_j</span> <span class="o">=</span> <span class="n">sym_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sym_id_i</span> <span class="o">=</span> <span class="n">sym_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Step 1: b_ij^same_chain = (f_i^asym_id == f_j^asym_id)</span>
        <span class="n">b_same_chain</span> <span class="o">=</span> <span class="p">(</span><span class="n">asym_id_i</span> <span class="o">==</span> <span class="n">asym_id_j</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="c1"># Step 2: b_ij^same_residue = (f_i^residue_index == f_j^residue_index)</span>
        <span class="n">b_same_residue</span> <span class="o">=</span> <span class="p">(</span><span class="n">residue_index_i</span> <span class="o">==</span> <span class="n">residue_index_j</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="c1"># Step 3: b_ij^same_entity = (f_i^entity_id == f_j^entity_id)</span>
        <span class="n">b_same_entity</span> <span class="o">=</span> <span class="p">(</span><span class="n">entity_id_i</span> <span class="o">==</span> <span class="n">entity_id_j</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="c1"># Step 4: Relative residue distance</span>
        <span class="n">d_residue</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">b_same_chain</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
                <span class="n">residue_index_i</span> <span class="o">-</span> <span class="n">residue_index_j</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span>
            <span class="p">),</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="c1"># Step 5: One-hot encode residue distance</span>
        <span class="n">a_rel_pos</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span>
            <span class="n">d_residue</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="p">)</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*r_max+2)</span>

        <span class="c1"># Step 6: Relative token distance</span>
        <span class="n">d_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="p">(</span><span class="n">b_same_chain</span><span class="o">.</span><span class="n">bool</span><span class="p">())</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">b_same_residue</span><span class="o">.</span><span class="n">bool</span><span class="p">()),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">token_index_i</span> <span class="o">-</span> <span class="n">token_index_j</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span><span class="p">),</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="c1"># Step 7: One-hot encode token distance</span>
        <span class="n">a_rel_token</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span>
            <span class="n">d_token</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="p">)</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*r_max+2)</span>

        <span class="c1"># Step 8: Relative chain distance</span>
        <span class="n">d_chain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="o">~</span><span class="n">b_same_chain</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">sym_id_i</span> <span class="o">-</span> <span class="n">sym_id_j</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">),</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="c1"># Step 9: One-hot encode chain distance</span>
        <span class="n">a_rel_chain</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span>
            <span class="n">d_chain</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="p">)</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*s_max+2)</span>

        <span class="c1"># Step 10: Concatenate all features and apply linear projection</span>
        <span class="n">all_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">a_rel_pos</span><span class="p">,</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*r_max+2)</span>
                <span class="n">a_rel_token</span><span class="p">,</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*r_max+2)</span>
                <span class="n">b_same_entity</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># (batch, n_tokens, n_tokens, 1)</span>
                <span class="n">a_rel_chain</span><span class="p">,</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*s_max+2)</span>
            <span class="p">],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 11: Linear projection</span>
        <span class="n">p_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">all_features</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">p_ij</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.RelativePositionEncoding.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">f_star</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass implementing Algorithm 3 exactly.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>f_star</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dictionary containing features with keys:
- 'asym_id': asymmetric unit IDs (batch, n_tokens)
- 'residue_index': residue indices (batch, n_tokens)
- 'entity_id': entity IDs (batch, n_tokens)
- 'token_index': token indices (batch, n_tokens)
- 'sym_id': symmetry IDs (batch, n_tokens)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>p_ij</code></td>            <td>
                  <code>Tensor, shape=(batch, n_tokens, n_tokens, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Relative position encodings</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_relative_position_encoding.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass implementing Algorithm 3 exactly.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    f_star : dict</span>
<span class="sd">        Dictionary containing features with keys:</span>
<span class="sd">        - &#39;asym_id&#39;: asymmetric unit IDs (batch, n_tokens)</span>
<span class="sd">        - &#39;residue_index&#39;: residue indices (batch, n_tokens)</span>
<span class="sd">        - &#39;entity_id&#39;: entity IDs (batch, n_tokens)</span>
<span class="sd">        - &#39;token_index&#39;: token indices (batch, n_tokens)</span>
<span class="sd">        - &#39;sym_id&#39;: symmetry IDs (batch, n_tokens)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    p_ij : Tensor, shape=(batch, n_tokens, n_tokens, c_z)</span>
<span class="sd">        Relative position encodings</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Extract features</span>
    <span class="n">asym_id_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;asym_id&quot;</span><span class="p">]</span>  <span class="c1"># (batch, n_tokens)</span>
    <span class="n">residue_index_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;residue_index&quot;</span><span class="p">]</span>
    <span class="n">entity_id_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;entity_id&quot;</span><span class="p">]</span>
    <span class="n">token_index_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;token_index&quot;</span><span class="p">]</span>
    <span class="n">sym_id_i</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span><span class="s2">&quot;sym_id&quot;</span><span class="p">]</span>

    <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">=</span> <span class="n">asym_id_i</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">asym_id_i</span><span class="o">.</span><span class="n">device</span>

    <span class="c1"># Create pairwise comparisons</span>
    <span class="n">asym_id_j</span> <span class="o">=</span> <span class="n">asym_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens, 1)</span>
    <span class="n">asym_id_i</span> <span class="o">=</span> <span class="n">asym_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, 1, n_tokens)</span>

    <span class="n">residue_index_j</span> <span class="o">=</span> <span class="n">residue_index_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">residue_index_i</span> <span class="o">=</span> <span class="n">residue_index_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">entity_id_j</span> <span class="o">=</span> <span class="n">entity_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">entity_id_i</span> <span class="o">=</span> <span class="n">entity_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">token_index_j</span> <span class="o">=</span> <span class="n">token_index_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">token_index_i</span> <span class="o">=</span> <span class="n">token_index_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">sym_id_j</span> <span class="o">=</span> <span class="n">sym_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sym_id_i</span> <span class="o">=</span> <span class="n">sym_id_i</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Step 1: b_ij^same_chain = (f_i^asym_id == f_j^asym_id)</span>
    <span class="n">b_same_chain</span> <span class="o">=</span> <span class="p">(</span><span class="n">asym_id_i</span> <span class="o">==</span> <span class="n">asym_id_j</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="c1"># Step 2: b_ij^same_residue = (f_i^residue_index == f_j^residue_index)</span>
    <span class="n">b_same_residue</span> <span class="o">=</span> <span class="p">(</span><span class="n">residue_index_i</span> <span class="o">==</span> <span class="n">residue_index_j</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="c1"># Step 3: b_ij^same_entity = (f_i^entity_id == f_j^entity_id)</span>
    <span class="n">b_same_entity</span> <span class="o">=</span> <span class="p">(</span><span class="n">entity_id_i</span> <span class="o">==</span> <span class="n">entity_id_j</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="c1"># Step 4: Relative residue distance</span>
    <span class="n">d_residue</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">b_same_chain</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
            <span class="n">residue_index_i</span> <span class="o">-</span> <span class="n">residue_index_j</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span>
        <span class="p">),</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

    <span class="c1"># Step 5: One-hot encode residue distance</span>
    <span class="n">a_rel_pos</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span>
        <span class="n">d_residue</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="p">)</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*r_max+2)</span>

    <span class="c1"># Step 6: Relative token distance</span>
    <span class="n">d_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="p">(</span><span class="n">b_same_chain</span><span class="o">.</span><span class="n">bool</span><span class="p">())</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">b_same_residue</span><span class="o">.</span><span class="n">bool</span><span class="p">()),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">token_index_i</span> <span class="o">-</span> <span class="n">token_index_j</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span><span class="p">),</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

    <span class="c1"># Step 7: One-hot encode token distance</span>
    <span class="n">a_rel_token</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span>
        <span class="n">d_token</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_max</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="p">)</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*r_max+2)</span>

    <span class="c1"># Step 8: Relative chain distance</span>
    <span class="n">d_chain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="o">~</span><span class="n">b_same_chain</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">sym_id_i</span> <span class="o">-</span> <span class="n">sym_id_j</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span><span class="p">),</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

    <span class="c1"># Step 9: One-hot encode chain distance</span>
    <span class="n">a_rel_chain</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span>
        <span class="n">d_chain</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_max</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="p">)</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*s_max+2)</span>

    <span class="c1"># Step 10: Concatenate all features and apply linear projection</span>
    <span class="n">all_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">a_rel_pos</span><span class="p">,</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*r_max+2)</span>
            <span class="n">a_rel_token</span><span class="p">,</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*r_max+2)</span>
            <span class="n">b_same_entity</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># (batch, n_tokens, n_tokens, 1)</span>
            <span class="n">a_rel_chain</span><span class="p">,</span>  <span class="c1"># (batch, n_tokens, n_tokens, 2*s_max+2)</span>
        <span class="p">],</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Step 11: Linear projection</span>
    <span class="n">p_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">all_features</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p_ij</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.SampleDiffusion" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.SampleDiffusion</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Sample Diffusion for AlphaFold 3.</p>
<p>This module implements Algorithm 18 exactly, performing iterative denoising
sampling for structure generation using a diffusion model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>gamma_0</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Initial gamma parameter for augmentation</p>
              </div>
            </td>
            <td>
                  <code>0.8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gamma_min</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum gamma threshold</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>noise_scale</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Noise scale lambda parameter</p>
              </div>
            </td>
            <td>
                  <code>1.003</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>step_scale</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Step scale eta parameter</p>
              </div>
            </td>
            <td>
                  <code>1.5</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_trans</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Translation scale for augmentation</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn.alphafold3</span><span class="w"> </span><span class="kn">import</span> <span class="n">SampleDiffusion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">SampleDiffusion</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Input features</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_star</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;ref_pos&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s_trunk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="mi">384</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_trunk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">noise_schedule</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_t</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">s_inputs</span><span class="p">,</span> <span class="n">s_trunk</span><span class="p">,</span> <span class="n">z_trunk</span><span class="p">,</span> <span class="n">noise_schedule</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_t</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 1000, 3])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 Algorithm 18: Sample Diffusion</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_sample_diffusion.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SampleDiffusion</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample Diffusion for AlphaFold 3.</span>

<span class="sd">    This module implements Algorithm 18 exactly, performing iterative denoising</span>
<span class="sd">    sampling for structure generation using a diffusion model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    gamma_0 : float, default=0.8</span>
<span class="sd">        Initial gamma parameter for augmentation</span>
<span class="sd">    gamma_min : float, default=1.0</span>
<span class="sd">        Minimum gamma threshold</span>
<span class="sd">    noise_scale : float, default=1.003</span>
<span class="sd">        Noise scale lambda parameter</span>
<span class="sd">    step_scale : float, default=1.5</span>
<span class="sd">        Step scale eta parameter</span>
<span class="sd">    s_trans : float, default=1.0</span>
<span class="sd">        Translation scale for augmentation</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn.alphafold3 import SampleDiffusion</span>
<span class="sd">    &gt;&gt;&gt; batch_size, n_atoms, n_tokens = 2, 1000, 32</span>
<span class="sd">    &gt;&gt;&gt; module = SampleDiffusion()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Input features</span>
<span class="sd">    &gt;&gt;&gt; f_star = {&#39;ref_pos&#39;: torch.randn(batch_size, n_atoms, 3)}</span>
<span class="sd">    &gt;&gt;&gt; s_inputs = torch.randn(batch_size, n_atoms, 100)</span>
<span class="sd">    &gt;&gt;&gt; s_trunk = torch.randn(batch_size, n_tokens, 384)</span>
<span class="sd">    &gt;&gt;&gt; z_trunk = torch.randn(batch_size, n_tokens, n_tokens, 128)</span>
<span class="sd">    &gt;&gt;&gt; noise_schedule = [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; x_t = module(f_star, s_inputs, s_trunk, z_trunk, noise_schedule)</span>
<span class="sd">    &gt;&gt;&gt; x_t.shape</span>
<span class="sd">    torch.Size([2, 1000, 3])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 Algorithm 18: Sample Diffusion</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">gamma_0</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>
        <span class="n">gamma_min</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">noise_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.003</span><span class="p">,</span>
        <span class="n">step_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span>
        <span class="n">s_trans</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gamma_0</span> <span class="o">=</span> <span class="n">gamma_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma_min</span> <span class="o">=</span> <span class="n">gamma_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise_scale</span> <span class="o">=</span> <span class="n">noise_scale</span>  <span class="c1"># lambda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_scale</span> <span class="o">=</span> <span class="n">step_scale</span>  <span class="c1"># eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s_trans</span> <span class="o">=</span> <span class="n">s_trans</span>

        <span class="c1"># Diffusion module for denoising</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_module</span> <span class="o">=</span> <span class="n">_Diffusion</span><span class="p">()</span>

        <span class="c1"># Centre random augmentation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">centre_random_augmentation</span> <span class="o">=</span> <span class="n">_CentreRandomAugmentation</span><span class="p">(</span><span class="n">s_trans</span><span class="o">=</span><span class="n">s_trans</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">s_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">s_trunk</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">z_trunk</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">noise_schedule</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass implementing Algorithm 18 exactly.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        f_star : dict</span>
<span class="sd">            Reference structure features</span>
<span class="sd">        s_inputs : Tensor, shape=(batch_size, n_atoms, c_s_inputs)</span>
<span class="sd">            Input single representations</span>
<span class="sd">        s_trunk : Tensor, shape=(batch_size, n_tokens, c_s)</span>
<span class="sd">            Trunk single representations</span>
<span class="sd">        z_trunk : Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</span>
<span class="sd">            Trunk pair representations</span>
<span class="sd">        noise_schedule : list</span>
<span class="sd">            Noise schedule [c0, c1, ..., cT]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x_t : Tensor, shape=(batch_size, n_atoms, 3)</span>
<span class="sd">            Final denoised positions</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">s_inputs</span><span class="o">.</span><span class="n">device</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">s_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">s_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Step 1: x̃_t ∼ c_0 · N(0̃, I_3)</span>
        <span class="n">c_0</span> <span class="o">=</span> <span class="n">noise_schedule</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="n">c_0</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Step 2: for all c_τ ∈ [c_1, ..., c_T] do</span>
        <span class="k">for</span> <span class="n">tau</span><span class="p">,</span> <span class="n">c_tau</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">noise_schedule</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># Step 3: {x̃_t} ← CentreRandomAugmentation({x̃_t})</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">centre_random_augmentation</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>

            <span class="c1"># Step 4: γ = γ_0 if c_τ &gt; γ_min else 0</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_0</span> <span class="k">if</span> <span class="n">c_tau</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_min</span> <span class="k">else</span> <span class="mf">0.0</span>

            <span class="c1"># Step 5: t̂ = c_{τ-1}(γ + 1)</span>
            <span class="n">c_tau_minus_1</span> <span class="o">=</span> <span class="n">noise_schedule</span><span class="p">[</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">t_hat</span> <span class="o">=</span> <span class="n">c_tau_minus_1</span> <span class="o">*</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Step 6: ζ̃_t = λ√(t̂^2 - c^2_{τ-1}) · N(0̃, I_3)</span>
            <span class="n">variance</span> <span class="o">=</span> <span class="n">t_hat</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">c_tau_minus_1</span><span class="o">**</span><span class="mi">2</span>
            <span class="k">if</span> <span class="n">variance</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">zeta_t</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">noise_scale</span>
                    <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">variance</span><span class="p">))</span>
                    <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">zeta_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>

            <span class="c1"># Step 7: x̃_t^noisy = x̃_t + ζ̃_t</span>
            <span class="n">x_t_noisy</span> <span class="o">=</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">zeta_t</span>

            <span class="c1"># Step 8: {x̃_t^denoised} = AlphaFold3Diffusion({x̃_t^noisy}, t̂, {f*}, {s_i^inputs}, {s_i^trunk}, {z_{ij}^trunk})</span>
            <span class="c1"># Create timestep tensor</span>
            <span class="n">t_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">t_hat</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_t</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span>

            <span class="c1"># Get reference positions from f_star</span>
            <span class="n">f_star_pos</span> <span class="o">=</span> <span class="n">f_star</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ref_pos&quot;</span><span class="p">,</span> <span class="n">x_t</span> <span class="o">*</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Use zeros if not available</span>

            <span class="c1"># Create dummy z_atom for the diffusion module</span>
            <span class="n">z_atom</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_t</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span>

            <span class="n">x_t_denoised</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_module</span><span class="p">(</span>
                <span class="n">x_noisy</span><span class="o">=</span><span class="n">x_t_noisy</span><span class="p">,</span>
                <span class="n">t</span><span class="o">=</span><span class="n">t_tensor</span><span class="p">,</span>
                <span class="n">f_star</span><span class="o">=</span><span class="n">f_star_pos</span><span class="p">,</span>
                <span class="n">s_inputs</span><span class="o">=</span><span class="n">s_inputs</span><span class="p">,</span>
                <span class="n">s_trunk</span><span class="o">=</span><span class="n">s_trunk</span><span class="p">,</span>
                <span class="n">z_trunk</span><span class="o">=</span><span class="n">z_trunk</span><span class="p">,</span>
                <span class="n">z_atom</span><span class="o">=</span><span class="n">z_atom</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Step 9: δ̃_t = (x̃_t - x̃_t^denoised) / t̂</span>
            <span class="n">delta_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_t</span> <span class="o">-</span> <span class="n">x_t_denoised</span><span class="p">)</span> <span class="o">/</span> <span class="n">t_hat</span>

            <span class="c1"># Step 10: dt = c_τ - t̂</span>
            <span class="n">dt</span> <span class="o">=</span> <span class="n">c_tau</span> <span class="o">-</span> <span class="n">t_hat</span>

            <span class="c1"># Step 11: x̃_t ← x̃_t^noisy + η · dt · δ̃_t</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_t_noisy</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_scale</span> <span class="o">*</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">delta_t</span>

        <span class="c1"># Step 12: end for</span>
        <span class="c1"># Step 13: return {x̃_t}</span>
        <span class="k">return</span> <span class="n">x_t</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.SampleDiffusion.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">s_inputs</span><span class="p">,</span> <span class="n">s_trunk</span><span class="p">,</span> <span class="n">z_trunk</span><span class="p">,</span> <span class="n">noise_schedule</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass implementing Algorithm 18 exactly.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>f_star</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Reference structure features</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_inputs</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, c_s_inputs)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input single representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s_trunk</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_tokens, c_s)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Trunk single representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>z_trunk</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Trunk pair representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>noise_schedule</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Noise schedule [c0, c1, ..., cT]</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>x_t</code></td>            <td>
                  <code>Tensor, shape=(batch_size, n_atoms, 3)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Final denoised positions</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_sample_diffusion.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">s_inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">s_trunk</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">z_trunk</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">noise_schedule</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass implementing Algorithm 18 exactly.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    f_star : dict</span>
<span class="sd">        Reference structure features</span>
<span class="sd">    s_inputs : Tensor, shape=(batch_size, n_atoms, c_s_inputs)</span>
<span class="sd">        Input single representations</span>
<span class="sd">    s_trunk : Tensor, shape=(batch_size, n_tokens, c_s)</span>
<span class="sd">        Trunk single representations</span>
<span class="sd">    z_trunk : Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</span>
<span class="sd">        Trunk pair representations</span>
<span class="sd">    noise_schedule : list</span>
<span class="sd">        Noise schedule [c0, c1, ..., cT]</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    x_t : Tensor, shape=(batch_size, n_atoms, 3)</span>
<span class="sd">        Final denoised positions</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">s_inputs</span><span class="o">.</span><span class="n">device</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">s_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">s_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Step 1: x̃_t ∼ c_0 · N(0̃, I_3)</span>
    <span class="n">c_0</span> <span class="o">=</span> <span class="n">noise_schedule</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">c_0</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Step 2: for all c_τ ∈ [c_1, ..., c_T] do</span>
    <span class="k">for</span> <span class="n">tau</span><span class="p">,</span> <span class="n">c_tau</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">noise_schedule</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Step 3: {x̃_t} ← CentreRandomAugmentation({x̃_t})</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">centre_random_augmentation</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>

        <span class="c1"># Step 4: γ = γ_0 if c_τ &gt; γ_min else 0</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_0</span> <span class="k">if</span> <span class="n">c_tau</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_min</span> <span class="k">else</span> <span class="mf">0.0</span>

        <span class="c1"># Step 5: t̂ = c_{τ-1}(γ + 1)</span>
        <span class="n">c_tau_minus_1</span> <span class="o">=</span> <span class="n">noise_schedule</span><span class="p">[</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">t_hat</span> <span class="o">=</span> <span class="n">c_tau_minus_1</span> <span class="o">*</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Step 6: ζ̃_t = λ√(t̂^2 - c^2_{τ-1}) · N(0̃, I_3)</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">t_hat</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">c_tau_minus_1</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">if</span> <span class="n">variance</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">zeta_t</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">noise_scale</span>
                <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">variance</span><span class="p">))</span>
                <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">zeta_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>

        <span class="c1"># Step 7: x̃_t^noisy = x̃_t + ζ̃_t</span>
        <span class="n">x_t_noisy</span> <span class="o">=</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">zeta_t</span>

        <span class="c1"># Step 8: {x̃_t^denoised} = AlphaFold3Diffusion({x̃_t^noisy}, t̂, {f*}, {s_i^inputs}, {s_i^trunk}, {z_{ij}^trunk})</span>
        <span class="c1"># Create timestep tensor</span>
        <span class="n">t_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
            <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">t_hat</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_t</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

        <span class="c1"># Get reference positions from f_star</span>
        <span class="n">f_star_pos</span> <span class="o">=</span> <span class="n">f_star</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ref_pos&quot;</span><span class="p">,</span> <span class="n">x_t</span> <span class="o">*</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Use zeros if not available</span>

        <span class="c1"># Create dummy z_atom for the diffusion module</span>
        <span class="n">z_atom</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_t</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

        <span class="n">x_t_denoised</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diffusion_module</span><span class="p">(</span>
            <span class="n">x_noisy</span><span class="o">=</span><span class="n">x_t_noisy</span><span class="p">,</span>
            <span class="n">t</span><span class="o">=</span><span class="n">t_tensor</span><span class="p">,</span>
            <span class="n">f_star</span><span class="o">=</span><span class="n">f_star_pos</span><span class="p">,</span>
            <span class="n">s_inputs</span><span class="o">=</span><span class="n">s_inputs</span><span class="p">,</span>
            <span class="n">s_trunk</span><span class="o">=</span><span class="n">s_trunk</span><span class="p">,</span>
            <span class="n">z_trunk</span><span class="o">=</span><span class="n">z_trunk</span><span class="p">,</span>
            <span class="n">z_atom</span><span class="o">=</span><span class="n">z_atom</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Step 9: δ̃_t = (x̃_t - x̃_t^denoised) / t̂</span>
        <span class="n">delta_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_t</span> <span class="o">-</span> <span class="n">x_t_denoised</span><span class="p">)</span> <span class="o">/</span> <span class="n">t_hat</span>

        <span class="c1"># Step 10: dt = c_τ - t̂</span>
        <span class="n">dt</span> <span class="o">=</span> <span class="n">c_tau</span> <span class="o">-</span> <span class="n">t_hat</span>

        <span class="c1"># Step 11: x̃_t ← x̃_t^noisy + η · dt · δ̃_t</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_t_noisy</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_scale</span> <span class="o">*</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">delta_t</span>

    <span class="c1"># Step 12: end for</span>
    <span class="c1"># Step 13: return {x̃_t}</span>
    <span class="k">return</span> <span class="n">x_t</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.TemplateEmbedder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.TemplateEmbedder</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Template Embedder for AlphaFold 3.</p>
<p>This module processes template structural information and adds it to the pair
representation. Templates provide structural constraints from homologous
structures that guide the prediction.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c_z</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pair representation dimension</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>c_template</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Template feature dimension</p>
              </div>
            </td>
            <td>
                  <code>64</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TemplateEmbedder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">TemplateEmbedder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_star</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;template_features&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="mi">64</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 64, 64, 128])</span>
</code></pre></div>








              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_template_embedder.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TemplateEmbedder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Template Embedder for AlphaFold 3.</span>

<span class="sd">    This module processes template structural information and adds it to the pair</span>
<span class="sd">    representation. Templates provide structural constraints from homologous</span>
<span class="sd">    structures that guide the prediction.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c_z : int, default=128</span>
<span class="sd">        Pair representation dimension</span>
<span class="sd">    c_template : int, default=64</span>
<span class="sd">        Template feature dimension</span>
<span class="sd">    n_head : int, default=4</span>
<span class="sd">        Number of attention heads</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import TemplateEmbedder</span>
<span class="sd">    &gt;&gt;&gt; batch_size, n_tokens = 2, 64</span>
<span class="sd">    &gt;&gt;&gt; module = TemplateEmbedder()</span>
<span class="sd">    &gt;&gt;&gt; f_star = {&#39;template_features&#39;: torch.randn(batch_size, n_tokens, n_tokens, 64)}</span>
<span class="sd">    &gt;&gt;&gt; z_ij = torch.randn(batch_size, n_tokens, n_tokens, 128)</span>
<span class="sd">    &gt;&gt;&gt; output = module(f_star, z_ij)</span>
<span class="sd">    &gt;&gt;&gt; output.shape</span>
<span class="sd">    torch.Size([2, 64, 64, 128])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">c_z</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">c_template</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span> <span class="o">=</span> <span class="n">c_z</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_template</span> <span class="o">=</span> <span class="n">c_template</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>

        <span class="c1"># Template processing layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">template_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_template</span><span class="p">,</span> <span class="n">c_z</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c_z</span><span class="p">)</span>

        <span class="c1"># Attention mechanism for template integration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="n">c_z</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Final projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c_z</span><span class="p">,</span> <span class="n">c_z</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of Template Embedder.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        f_star : dict</span>
<span class="sd">            Dictionary containing template features with key &#39;template_features&#39;</span>
<span class="sd">        z_ij : Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</span>
<span class="sd">            Current pair representations</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z_ij : Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</span>
<span class="sd">            Updated pair representations with template information</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Extract template features (if available)</span>
        <span class="k">if</span> <span class="s2">&quot;template_features&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">f_star</span><span class="p">:</span>
            <span class="c1"># No templates available, return unchanged</span>
            <span class="k">return</span> <span class="n">z_ij</span>

        <span class="n">template_features</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span>
            <span class="s2">&quot;template_features&quot;</span>
        <span class="p">]</span>  <span class="c1"># (batch, n_tokens, n_tokens, c_template)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">c_template</span> <span class="o">=</span> <span class="n">template_features</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project template features to pair dimension</span>
        <span class="n">template_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">template_proj</span><span class="p">(</span>
            <span class="n">template_features</span>
        <span class="p">)</span>  <span class="c1"># (batch, n_tokens, n_tokens, c_z)</span>

        <span class="c1"># Reshape for attention: (batch, n_tokens^2, c_z)</span>
        <span class="n">template_flat</span> <span class="o">=</span> <span class="n">template_proj</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">*</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="p">)</span>
        <span class="n">z_flat</span> <span class="o">=</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">*</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="p">)</span>

        <span class="c1"># Apply layer norm</span>
        <span class="n">template_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">template_flat</span><span class="p">)</span>

        <span class="c1"># Self-attention to integrate template information</span>
        <span class="n">template_attended</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="n">template_flat</span><span class="p">,</span> <span class="n">template_flat</span><span class="p">,</span> <span class="n">template_flat</span>
        <span class="p">)</span>

        <span class="c1"># Add template information to pair representations</span>
        <span class="n">z_updated</span> <span class="o">=</span> <span class="n">z_flat</span> <span class="o">+</span> <span class="n">template_attended</span>

        <span class="c1"># Final projection and reshape back</span>
        <span class="n">z_updated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_proj</span><span class="p">(</span><span class="n">z_updated</span><span class="p">)</span>
        <span class="n">z_updated</span> <span class="o">=</span> <span class="n">z_updated</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">z_updated</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.TemplateEmbedder.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">f_star</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of Template Embedder.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>f_star</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dictionary containing template features with key 'template_features'</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>z_ij</code>
            </td>
            <td>
                  <code>Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Current pair representations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>z_ij</code></td>            <td>
                  <code>Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated pair representations with template information</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_template_embedder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f_star</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of Template Embedder.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    f_star : dict</span>
<span class="sd">        Dictionary containing template features with key &#39;template_features&#39;</span>
<span class="sd">    z_ij : Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</span>
<span class="sd">        Current pair representations</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z_ij : Tensor, shape=(batch_size, n_tokens, n_tokens, c_z)</span>
<span class="sd">        Updated pair representations with template information</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Extract template features (if available)</span>
    <span class="k">if</span> <span class="s2">&quot;template_features&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">f_star</span><span class="p">:</span>
        <span class="c1"># No templates available, return unchanged</span>
        <span class="k">return</span> <span class="n">z_ij</span>

    <span class="n">template_features</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">[</span>
        <span class="s2">&quot;template_features&quot;</span>
    <span class="p">]</span>  <span class="c1"># (batch, n_tokens, n_tokens, c_template)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">c_template</span> <span class="o">=</span> <span class="n">template_features</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Project template features to pair dimension</span>
    <span class="n">template_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">template_proj</span><span class="p">(</span>
        <span class="n">template_features</span>
    <span class="p">)</span>  <span class="c1"># (batch, n_tokens, n_tokens, c_z)</span>

    <span class="c1"># Reshape for attention: (batch, n_tokens^2, c_z)</span>
    <span class="n">template_flat</span> <span class="o">=</span> <span class="n">template_proj</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">*</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="p">)</span>
    <span class="n">z_flat</span> <span class="o">=</span> <span class="n">z_ij</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">*</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="p">)</span>

    <span class="c1"># Apply layer norm</span>
    <span class="n">template_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">template_flat</span><span class="p">)</span>

    <span class="c1"># Self-attention to integrate template information</span>
    <span class="n">template_attended</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
        <span class="n">template_flat</span><span class="p">,</span> <span class="n">template_flat</span><span class="p">,</span> <span class="n">template_flat</span>
    <span class="p">)</span>

    <span class="c1"># Add template information to pair representations</span>
    <span class="n">z_updated</span> <span class="o">=</span> <span class="n">z_flat</span> <span class="o">+</span> <span class="n">template_attended</span>

    <span class="c1"># Final projection and reshape back</span>
    <span class="n">z_updated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_proj</span><span class="p">(</span><span class="n">z_updated</span><span class="p">)</span>
    <span class="n">z_updated</span> <span class="o">=</span> <span class="n">z_updated</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_z</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">z_updated</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.Transition" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.Transition</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Transition layer from AlphaFold 3.</p>
<p>This implements Algorithm 11 from AlphaFold 3, which is a simple
transition layer with layer normalization, two linear projections,
and a SwiGLU activation function for enhanced non-linearity.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input and output channel dimension</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Expansion factor for the hidden dimension</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Transition</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">Transition</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_out</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 10, 128])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 paper, Algorithm 11: Transition layer</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_transition.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Transition</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transition layer from AlphaFold 3.</span>

<span class="sd">    This implements Algorithm 11 from AlphaFold 3, which is a simple</span>
<span class="sd">    transition layer with layer normalization, two linear projections,</span>
<span class="sd">    and a SwiGLU activation function for enhanced non-linearity.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c : int, default=128</span>
<span class="sd">        Input and output channel dimension</span>
<span class="sd">    n : int, default=4</span>
<span class="sd">        Expansion factor for the hidden dimension</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import Transition</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len, c = 2, 10, 128</span>
<span class="sd">    &gt;&gt;&gt; n = 4</span>
<span class="sd">    &gt;&gt;&gt; module = Transition(c=c, n=n)</span>
<span class="sd">    &gt;&gt;&gt; x = torch.randn(batch_size, seq_len, c)</span>
<span class="sd">    &gt;&gt;&gt; x_out = module(x)</span>
<span class="sd">    &gt;&gt;&gt; x_out.shape</span>
<span class="sd">    torch.Size([2, 10, 128])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 paper, Algorithm 11: Transition layer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">c</span>

        <span class="c1"># Layer normalization (step 1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="c1"># First linear projection (step 2)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Second linear projection (step 3)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Final output projection (step 4)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of transition layer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : Tensor, shape=(..., c)</span>
<span class="sd">            Input tensor where c is the channel dimension.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : Tensor, shape=(..., c)</span>
<span class="sd">            Output tensor after transition layer processing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="p">)</span>  <span class="c1"># (..., c)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.Transition.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of transition layer.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input tensor where c is the channel dimension.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>x</code></td>            <td>
                  <code>Tensor, shape=(..., c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Output tensor after transition layer processing.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_transition.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of transition layer.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : Tensor, shape=(..., c)</span>
<span class="sd">        Input tensor where c is the channel dimension.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    x : Tensor, shape=(..., c)</span>
<span class="sd">        Output tensor after transition layer processing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
    <span class="p">)</span>  <span class="c1"># (..., c)</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.TriangleAttentionEndingNode" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.TriangleAttentionEndingNode</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Triangular gated self-attention around ending node from AlphaFold 3.</p>
<p>This implements Algorithm 15 from AlphaFold 3, which performs triangular
gated self-attention where the attention is computed around the ending
node of each edge in the triangle. The key differences from Algorithm 14
are in steps 5 and 6 where k^h_kj and v^h_kj are used instead of k^h_jk and v^h_jk.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for the pair representation</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TriangleAttentionEndingNode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_head</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">TriangleAttentionEndingNode</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_tilde_ij</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_tilde_ij</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 10, 10, 32])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 paper, Algorithm 15: Triangular gated self-attention around ending node</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_triangle_attention_ending_node.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TriangleAttentionEndingNode</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Triangular gated self-attention around ending node from AlphaFold 3.</span>

<span class="sd">    This implements Algorithm 15 from AlphaFold 3, which performs triangular</span>
<span class="sd">    gated self-attention where the attention is computed around the ending</span>
<span class="sd">    node of each edge in the triangle. The key differences from Algorithm 14</span>
<span class="sd">    are in steps 5 and 6 where k^h_kj and v^h_kj are used instead of k^h_jk and v^h_jk.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c : int, default=32</span>
<span class="sd">        Channel dimension for the pair representation</span>
<span class="sd">    n_head : int, default=4</span>
<span class="sd">        Number of attention heads</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import TriangleAttentionEndingNode</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len, c = 2, 10, 32</span>
<span class="sd">    &gt;&gt;&gt; n_head = 4</span>
<span class="sd">    &gt;&gt;&gt; module = TriangleAttentionEndingNode(c=c, n_head=n_head)</span>
<span class="sd">    &gt;&gt;&gt; z_ij = torch.randn(batch_size, seq_len, seq_len, c)</span>
<span class="sd">    &gt;&gt;&gt; z_tilde_ij = module(z_ij)</span>
<span class="sd">    &gt;&gt;&gt; z_tilde_ij.shape</span>
<span class="sd">    torch.Size([2, 10, 10, 32])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 paper, Algorithm 15: Triangular gated self-attention around ending node</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">n_head</span>

        <span class="k">if</span> <span class="n">c</span> <span class="o">%</span> <span class="n">n_head</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Channel dimension </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2"> must be divisible by number of heads </span><span class="si">{</span><span class="n">n_head</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Layer normalization for input (step 1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="c1"># Linear projections for queries, keys, values (step 2)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Bias projection (step 3)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Gate projection (step 4)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Output projection (step 7)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Scale factor for attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of triangular gated self-attention around ending node.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        z_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">            Input pair representation where s is sequence length</span>
<span class="sd">            and c is channel dimension.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z_tilde_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">            Updated pair representation after triangular attention.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Step 1: Layer normalization</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                    <span class="s2">&quot;...ijhk,...kjhd-&gt;...ijhd&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                                <span class="s2">&quot;...ijhd,...kjhd-&gt;...ijhk&quot;</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                                    <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                                <span class="p">),</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                                    <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                                <span class="p">),</span>
                            <span class="p">)</span>
                            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
                            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_ij</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                        <span class="p">),</span>
                        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                        <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                        <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                        <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.TriangleAttentionEndingNode.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of triangular gated self-attention around ending node.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>z_ij</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, s, c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input pair representation where s is sequence length
and c is channel dimension.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>z_tilde_ij</code></td>            <td>
                  <code>Tensor, shape=(..., s, s, c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated pair representation after triangular attention.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_triangle_attention_ending_node.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of triangular gated self-attention around ending node.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">        Input pair representation where s is sequence length</span>
<span class="sd">        and c is channel dimension.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z_tilde_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">        Updated pair representation after triangular attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Step 1: Layer normalization</span>
    <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                <span class="s2">&quot;...ijhk,...kjhd-&gt;...ijhd&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                            <span class="s2">&quot;...ijhd,...kjhd-&gt;...ijhk&quot;</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                                <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                            <span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                                <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                            <span class="p">),</span>
                        <span class="p">)</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
                        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_ij</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="p">),</span>
                    <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
            <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
            <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.TriangleAttentionStartingNode" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.TriangleAttentionStartingNode</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Triangular gated self-attention around starting node from AlphaFold 3.</p>
<p>This implements Algorithm 14 from AlphaFold 3, which performs triangular
gated self-attention where the attention is computed around the starting
node of each edge in the triangle.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for the pair representation</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_head</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TriangleAttentionStartingNode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_head</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">TriangleAttentionStartingNode</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_tilde_ij</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_tilde_ij</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 10, 10, 32])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 paper, Algorithm 14: Triangular gated self-attention around starting node</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_triangle_attention_starting_node.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TriangleAttentionStartingNode</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Triangular gated self-attention around starting node from AlphaFold 3.</span>

<span class="sd">    This implements Algorithm 14 from AlphaFold 3, which performs triangular</span>
<span class="sd">    gated self-attention where the attention is computed around the starting</span>
<span class="sd">    node of each edge in the triangle.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c : int, default=32</span>
<span class="sd">        Channel dimension for the pair representation</span>
<span class="sd">    n_head : int, default=4</span>
<span class="sd">        Number of attention heads</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import TriangleAttentionStartingNode</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len, c = 2, 10, 32</span>
<span class="sd">    &gt;&gt;&gt; n_head = 4</span>
<span class="sd">    &gt;&gt;&gt; module = TriangleAttentionStartingNode(c=c, n_head=n_head)</span>
<span class="sd">    &gt;&gt;&gt; z_ij = torch.randn(batch_size, seq_len, seq_len, c)</span>
<span class="sd">    &gt;&gt;&gt; z_tilde_ij = module(z_ij)</span>
<span class="sd">    &gt;&gt;&gt; z_tilde_ij.shape</span>
<span class="sd">    torch.Size([2, 10, 10, 32])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 paper, Algorithm 14: Triangular gated self-attention around starting node</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">n_head</span>

        <span class="k">if</span> <span class="n">c</span> <span class="o">%</span> <span class="n">n_head</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Channel dimension </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2"> must be divisible by number of heads </span><span class="si">{</span><span class="n">n_head</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Layer normalization for input (step 1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="c1"># Linear projections for queries, keys, values (step 2)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Bias projection (step 3)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Gate projection (step 4)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Output projection (step 7)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Scale factor for attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of triangular gated self-attention around starting node.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        z_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">            Input pair representation where s is sequence length</span>
<span class="sd">            and c is channel dimension.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z_tilde_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">            Updated pair representation after triangular attention.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                    <span class="s2">&quot;...ijhk,...jkhd-&gt;...ijhd&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                                <span class="s2">&quot;...ijhd,...jkhd-&gt;...ijhk&quot;</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                                    <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                                <span class="p">),</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                                    <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                                <span class="p">),</span>
                            <span class="p">)</span>
                            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
                            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_ij</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                        <span class="p">),</span>
                        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                        <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                        <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                        <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.TriangleAttentionStartingNode.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of triangular gated self-attention around starting node.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>z_ij</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, s, c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input pair representation where s is sequence length
and c is channel dimension.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>z_tilde_ij</code></td>            <td>
                  <code>Tensor, shape=(..., s, s, c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated pair representation after triangular attention.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_triangle_attention_starting_node.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of triangular gated self-attention around starting node.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">        Input pair representation where s is sequence length</span>
<span class="sd">        and c is channel dimension.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z_tilde_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">        Updated pair representation after triangular attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                <span class="s2">&quot;...ijhk,...jkhd-&gt;...ijhd&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                            <span class="s2">&quot;...ijhd,...jkhd-&gt;...ijhk&quot;</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">linear_q</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                                <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                            <span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">linear_k</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                                <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                            <span class="p">),</span>
                        <span class="p">)</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
                        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_ij</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="p">),</span>
                    <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">linear_v</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                    <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="o">*</span><span class="p">(</span><span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]),</span>
            <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
            <span class="n">z_ij</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.TriangleMultiplicationIncoming" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.TriangleMultiplicationIncoming</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Triangular multiplicative update using "incoming" edges from AlphaFold 3.</p>
<p>This implements Algorithm 13 from AlphaFold 3, which performs triangular
multiplicative updates on pair representations using incoming edges.
The key difference from the outgoing version is in step 4 where
a_ki ⊙ b_kj is computed instead of a_ik ⊙ b_jk.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for the pair representation</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TriangleMultiplicationIncoming</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">TriangleMultiplicationIncoming</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_tilde_ij</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_tilde_ij</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 10, 10, 128])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 paper, Algorithm 13: Triangular multiplicative update using "incoming" edges</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_triangle_multiplication_incoming.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TriangleMultiplicationIncoming</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Triangular multiplicative update using &quot;incoming&quot; edges from AlphaFold 3.</span>

<span class="sd">    This implements Algorithm 13 from AlphaFold 3, which performs triangular</span>
<span class="sd">    multiplicative updates on pair representations using incoming edges.</span>
<span class="sd">    The key difference from the outgoing version is in step 4 where</span>
<span class="sd">    a_ki ⊙ b_kj is computed instead of a_ik ⊙ b_jk.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c : int, default=128</span>
<span class="sd">        Channel dimension for the pair representation</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import TriangleMultiplicationIncoming</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len, c = 2, 10, 128</span>
<span class="sd">    &gt;&gt;&gt; module = TriangleMultiplicationIncoming(c=c)</span>
<span class="sd">    &gt;&gt;&gt; z_ij = torch.randn(batch_size, seq_len, seq_len, c)</span>
<span class="sd">    &gt;&gt;&gt; z_tilde_ij = module(z_ij)</span>
<span class="sd">    &gt;&gt;&gt; z_tilde_ij.shape</span>
<span class="sd">    torch.Size([2, 10, 10, 128])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 paper, Algorithm 13: Triangular multiplicative update using &quot;incoming&quot; edges</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>

        <span class="c1"># Layer normalization (step 1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="c1"># Linear projections without bias for a and b (step 2)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Linear projection without bias for g (step 3)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Final linear projection without bias with layer norm (step 4)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of triangular multiplicative update with incoming edges.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        z_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">            Input pair representation where s is sequence length</span>
<span class="sd">            and c is channel dimension.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z_tilde_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">            Updated pair representation after triangular multiplicative update.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                    <span class="s2">&quot;...kic,...kjc-&gt;...ijc&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_a</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)),</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.TriangleMultiplicationIncoming.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of triangular multiplicative update with incoming edges.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>z_ij</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, s, c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input pair representation where s is sequence length
and c is channel dimension.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>z_tilde_ij</code></td>            <td>
                  <code>Tensor, shape=(..., s, s, c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated pair representation after triangular multiplicative update.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_triangle_multiplication_incoming.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of triangular multiplicative update with incoming edges.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">        Input pair representation where s is sequence length</span>
<span class="sd">        and c is channel dimension.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z_tilde_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">        Updated pair representation after triangular multiplicative update.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_linear</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                <span class="s2">&quot;...kic,...kjc-&gt;...ijc&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_a</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)),</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h4 id="beignet.nn.alphafold3.TriangleMultiplicationOutgoing" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">beignet.nn.alphafold3.TriangleMultiplicationOutgoing</span>


</h4>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Triangular multiplicative update using "outgoing" edges from AlphaFold 3.</p>
<p>This implements Algorithm 12 from AlphaFold 3, which performs triangular
multiplicative updates on pair representations. The algorithm establishes
direct communication between edges that connect 3 nodes in a triangle,
allowing the network to detect inconsistencies in spatial relationships.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>c</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Channel dimension for the pair representation</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">beignet.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TriangleMultiplicationOutgoing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">TriangleMultiplicationOutgoing</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_tilde_ij</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z_tilde_ij</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 10, 10, 128])</span>
</code></pre></div>


<details class="references" open>
  <summary>References</summary>
  <p>.. [1] AlphaFold 3 paper, Algorithm 12: Triangular multiplicative update using "outgoing" edges</p>
</details>







              <details class="quote">
                <summary>Source code in <code>src/beignet/nn/alphafold3/_triangle_multiplication_outgoing.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TriangleMultiplicationOutgoing</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Triangular multiplicative update using &quot;outgoing&quot; edges from AlphaFold 3.</span>

<span class="sd">    This implements Algorithm 12 from AlphaFold 3, which performs triangular</span>
<span class="sd">    multiplicative updates on pair representations. The algorithm establishes</span>
<span class="sd">    direct communication between edges that connect 3 nodes in a triangle,</span>
<span class="sd">    allowing the network to detect inconsistencies in spatial relationships.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    c : int, default=128</span>
<span class="sd">        Channel dimension for the pair representation</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from beignet.nn import TriangleMultiplicationOutgoing</span>
<span class="sd">    &gt;&gt;&gt; batch_size, seq_len, c = 2, 10, 128</span>
<span class="sd">    &gt;&gt;&gt; module = TriangleMultiplicationOutgoing(c=c)</span>
<span class="sd">    &gt;&gt;&gt; z_ij = torch.randn(batch_size, seq_len, seq_len, c)</span>
<span class="sd">    &gt;&gt;&gt; z_tilde_ij = module(z_ij)</span>
<span class="sd">    &gt;&gt;&gt; z_tilde_ij.shape</span>
<span class="sd">    torch.Size([2, 10, 10, 128])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] AlphaFold 3 paper, Algorithm 12: Triangular multiplicative update using &quot;outgoing&quot; edges</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>

        <span class="c1"># Layer normalization (step 1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="c1"># Linear projections without bias for a and b (step 2)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Linear projection without bias for g (step 3)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Final linear projection without bias with layer norm (step 4)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of triangular multiplicative update with outgoing edges.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        z_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">            Input pair representation where s is sequence length</span>
<span class="sd">            and c is channel dimension.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z_tilde_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">            Updated pair representation after triangular multiplicative update.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Step 1: Layer normalization</span>
        <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                    <span class="s2">&quot;...ikc,...jkc-&gt;...ijc&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_a</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)),</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="beignet.nn.alphafold3.TriangleMultiplicationOutgoing.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


</h5>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of triangular multiplicative update with outgoing edges.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>z_ij</code>
            </td>
            <td>
                  <code>Tensor, shape=(..., s, s, c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input pair representation where s is sequence length
and c is channel dimension.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>z_tilde_ij</code></td>            <td>
                  <code>Tensor, shape=(..., s, s, c)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Updated pair representation after triangular multiplicative update.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>src/beignet/nn/alphafold3/_triangle_multiplication_outgoing.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_ij</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of triangular multiplicative update with outgoing edges.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    z_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">        Input pair representation where s is sequence length</span>
<span class="sd">        and c is channel dimension.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z_tilde_ij : Tensor, shape=(..., s, s, c)</span>
<span class="sd">        Updated pair representation after triangular multiplicative update.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Step 1: Layer normalization</span>
    <span class="n">z_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_g</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_linear</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                <span class="s2">&quot;...ikc,...jkc-&gt;...ijc&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_a</span><span class="p">(</span><span class="n">z_ij</span><span class="p">)),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_b</span><span class="p">(</span><span class="n">z_ij</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.sections"], "search": "../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>