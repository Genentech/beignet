{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f830e83",
   "metadata": {},
   "source": [
    "\n",
    "# Tutorial (Deep Dive): Power & Effect Size Analysis for Biological ML with PyTorch + Lightning + `beignet`\n",
    "\n",
    "> **Goal.** Teach you how to design, train, and *interpret* biological ML models using **effect sizes** and **power / sample-size analysis**—so your results are *detectable*, *reproducible*, and *biologically actionable*.\n",
    "\n",
    "**Who is this for?**\n",
    "- ML researchers/engineers working with biological data (drug discovery, protein design, single-cell, ADMET).\n",
    "- Biologists validating ML models and planning experiments.\n",
    "- Anyone shipping models where *small effects* still matter.\n",
    "\n",
    "**What you'll learn**\n",
    "1. Map *biological questions* to the **right statistical tests** and **effect sizes**.\n",
    "2. Estimate **required sample sizes** *before* training.\n",
    "3. Instrument PyTorch Lightning to log **effect sizes** and **power** alongside loss/AUROC.\n",
    "4. Interpret results and write **power-aware claims** for papers and reports.\n",
    "\n",
    "**Prerequisites**\n",
    "- Comfortable with PyTorch/Lightning training loops.\n",
    "- Basic stats (mean/variance/correlation, t-tests, chi-squared, ANOVA).  \n",
    "  A concise refresher is included below.\n",
    "\n",
    "**Datasets used (small/medium from `beignet`):**\n",
    "- `FreeSolvDataset` (regression; hydration free energy)\n",
    "- `ClinToxDataset` (binary classification; clinical trial toxicity)\n",
    "- `SKEMPIDataset` (regression; ΔΔG mutational effects)\n",
    "\n",
    "**Navigation**\n",
    "- §1 Foundations: definitions, formulae, and when to use what  \n",
    "- §2–§4 Hands-on case studies (FreeSolv, ClinTox, SKEMPI)  \n",
    "- §5 Workflow patterns you can reuse  \n",
    "- §6 Utilities and helper functions  \n",
    "- §7 Reporting templates (what to claim & how)  \n",
    "- Appendix: FAQs and pitfalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a85f6",
   "metadata": {},
   "source": [
    "\n",
    "# Power & Effect Size Analysis for Biological ML with PyTorch + Lightning + `beignet`\n",
    "\n",
    "Deep learning metrics (loss, AUROC) don’t tell you whether your dataset/model can **reliably detect** biologically meaningful effects.  \n",
    "This notebook adds **effect sizes** and **power/sample-size** to your workflow so claims are reproducible and decision-relevant.\n",
    "\n",
    "**Datasets used (small/medium from `beignet`):**\n",
    "- `FreeSolvDataset` (regression; hydration free energy)\n",
    "- `ClinToxDataset` (binary classification; clinical trial toxicity)\n",
    "- `SKEMPIDataset` (regression; ΔΔG mutational effects)\n",
    "\n",
    "> If any field names differ (e.g., `X`, `y`), adjust in the marked cells below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647bc12",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, uncomment:\n",
    "# !pip install beignet torch pytorch-lightning torchmetrics scikit-learn pandas\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lightning import LightningModule, Trainer, seed_everything\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seed_everything(7)\n",
    "print(\"Torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8566a0",
   "metadata": {},
   "source": [
    "### Import `beignet` datasets & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets (swap for others in your package if preferred)\n",
    "from torchmetrics.classification import AUROC, Accuracy, F1Score\n",
    "from torchmetrics.regression import MeanSquaredError, R2Score\n",
    "\n",
    "from beignet.datasets import ClinToxDataset, FreeSolvDataset, SKEMPIDataset\n",
    "\n",
    "# TorchMetrics-style wrappers for power/effect/sample size\n",
    "from beignet.metrics import (\n",
    "    ANOVAPower,\n",
    "    ChiSquaredIndependencePower,\n",
    "    CohensD,\n",
    "    CorrelationPower,\n",
    "    CramersV,\n",
    "    HedgesG,\n",
    "    PhiCoefficient,\n",
    "    ProportionTwoSamplePower,\n",
    "    TTestPower,\n",
    "    TTestSampleSize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf38e5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1) Statistical foundations (concise refresher)\n",
    "\n",
    "### 1.1 Effect sizes (how *big* is the effect?)\n",
    "- **Cohen's d (two means)**:  \n",
    "  \\[ d \\;=\\; \\frac{\\mu_1 - \\mu_2}{s_\\text{pooled}}, \\quad\n",
    "     s_\\text{pooled} = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} \\]\n",
    "  Interpretable in SD units. Use **Hedges' g** for small-sample bias correction.\n",
    "- **Correlation r (association strength)**: absolute value indicates effect magnitude; can be converted to *f* or *d* if needed.\n",
    "- **Cramér’s V / Phi (φ)** for association in contingency tables:  \n",
    "  \\[ \\phi = \\sqrt{\\frac{\\chi^2}{n}} \\quad (\\text{for 2x2}) \\qquad\n",
    "     V = \\sqrt{\\frac{\\chi^2}{n \\cdot (k-1)}} \\quad (\\text{for } r \\times k) \\]\n",
    "- **Cohen’s f (ANOVA)** relates group-separation to within-group variance:  \n",
    "  \\[ f = \\sqrt{\\frac{\\eta^2}{1-\\eta^2}} \\]\n",
    "\n",
    "### 1.2 Power and sample size (can we **detect** it reliably?)\n",
    "- **Power (1-β)**: probability of rejecting the null when the effect is real. In practice we target **0.8** or **0.9**.\n",
    "- **Inputs**: effect size, alpha (type-I error, usually 0.05), sample size, variance/df.\n",
    "- **Outputs**: (a) compute power for given *n*, (b) compute **required n** for target power.\n",
    "\n",
    "### 1.3 Mapping biological questions → tests\n",
    "- Regression (e.g., FreeSolv, SKEMPI): use **correlation power**, **t-test power**, **ANOVA power** for grouped effects.\n",
    "- Binary classification (e.g., ClinTox): **χ² independence power**, **two-sample proportion power** for prevalence deltas.\n",
    "- Multi-group categorical biology (e.g., assay tiers, mutation classes): **ANOVA power** and **Cohen’s f**.\n",
    "\n",
    "> **Why not only AUROC/MSE?** Because AUROC/MSE don't tell you whether your dataset size + noise profile can detect the effect you *care* about. Power does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5abc38",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Why power/effect matters (biology-first)\n",
    "\n",
    "- **Effect size** (Cohen’s *d*, Hedges’ *g*, φ/Cramér’s *V*, Cohen’s *f/f²*): how *big* is a difference/association.\n",
    "- **Power**: probability to detect that effect at α (given *n*, noise).\n",
    "- **For DL**: plan **data needs** before training; **log** detectability alongside loss/AUC; **calibrate claims** to what your dataset can actually support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d00081d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2) Case A — **FreeSolv** (regression; hydration free energy)\n",
    "\n",
    "We’ll show correlation/t-test power and standardized effects on a compact regression task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eab9ad",
   "metadata": {},
   "source": [
    "### 2.1 Load & preprocess *(adjust field names if needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff883033",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = FreeSolvDataset()\n",
    "\n",
    "# Adjust here if your dataset exposes different attributes\n",
    "X = torch.as_tensor(ds.X, dtype=torch.float32)  # [N, D]\n",
    "y = torch.as_tensor(ds.y, dtype=torch.float32)  # [N]\n",
    "\n",
    "# Standardize for stable training (optional)\n",
    "xsc = StandardScaler().fit(X.numpy())\n",
    "ysc = StandardScaler().fit(y[:, None].numpy())\n",
    "Xn = torch.from_numpy(xsc.transform(X.numpy())).float()\n",
    "yn = torch.from_numpy(ysc.transform(y[:, None].numpy()).squeeze(1)).float()\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    torch.arange(len(Xn)),\n",
    "    test_size=0.2,\n",
    "    random_state=7,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(Xn[train_idx], yn[train_idx]),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(TensorDataset(Xn[test_idx], yn[test_idx]), batch_size=128)\n",
    "\n",
    "Xn.shape, yn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0356eac",
   "metadata": {},
   "source": [
    "### 2.2 Lightning model with **effect/power** logged next to loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitRegressor(LightningModule):\n",
    "    def __init__(self, in_dim, lr=1e-3, alpha=0.05):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        self.mse, self.r2 = MeanSquaredError(), R2Score()\n",
    "        self.d = CohensD()\n",
    "        self.power_t = TTestPower(alpha=alpha)\n",
    "        self.power_corr = CorrelationPower(alpha=alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = nn.functional.mse_loss(pred, y)\n",
    "        self.log(\"train/mse\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        self.log(\"val/mse\", self.mse(pred, y), prog_bar=True)\n",
    "        self.log(\"val/r2\", self.r2(pred, y), prog_bar=True)\n",
    "        # Effect size / power on predictions vs truth\n",
    "        self.log(\"val/cohens_d\", self.d(pred, y))\n",
    "        self.log(\"val/power_ttest\", self.power_t(pred, y))\n",
    "        self.log(\"val/power_corr\", self.power_corr(pred, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "model = LitRegressor(in_dim=Xn.shape[1])\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    log_every_n_steps=5,\n",
    "    deterministic=True,\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6478ca",
   "metadata": {},
   "source": [
    "### 2.3 Plan **sample size** for a target correlation (threshold → detectability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose r >= 0.35 is \"biologically useful\"\n",
    "target_r, alpha = 0.35, 0.05\n",
    "ns = torch.arange(30, 401, 10)\n",
    "cp = CorrelationPower(alpha=alpha)\n",
    "\n",
    "powers = [float(cp(effect_size=torch.tensor(target_r), n=int(n))) for n in ns]\n",
    "pd.DataFrame({\"n\": ns.numpy(), \"power_at_r=0.35\": powers}).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6278cf8b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 3) Case B — **ClinTox** (binary classification; clinical trial toxicity)\n",
    "\n",
    "Demonstrate **χ² power** for association (truth vs predictions), **φ/Cramér’s V** effect size,\n",
    "and **two-sample proportion power** for prevalence differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9274f43",
   "metadata": {},
   "source": [
    "### 3.1 Load & split *(adjust field names if needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bff8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ClinToxDataset()\n",
    "\n",
    "X = torch.as_tensor(ds.X, dtype=torch.float32)\n",
    "y = torch.as_tensor(ds.y, dtype=torch.long)  # 0/1 labels\n",
    "\n",
    "Xn = torch.from_numpy(StandardScaler().fit_transform(X.numpy())).float()\n",
    "tr, te = train_test_split(\n",
    "    torch.arange(len(Xn)),\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=7,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xn[tr], y[tr]), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(Xn[te], y[te]), batch_size=128)\n",
    "\n",
    "Xn.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a0aa1",
   "metadata": {},
   "source": [
    "### 3.2 Lightning classifier with **χ² power** & **effect sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitClassifier(LightningModule):\n",
    "    def __init__(self, in_dim, lr=1e-3, alpha=0.05):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        self.acc, self.auroc, self.f1 = (\n",
    "            Accuracy(task=\"binary\"),\n",
    "            AUROC(task=\"binary\"),\n",
    "            F1Score(task=\"binary\"),\n",
    "        )\n",
    "        self.chi_power = ChiSquaredIndependencePower(alpha=alpha)\n",
    "        self.cramersV, self.phi = CramersV(), PhiCoefficient()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def _contingency(self, logits, y, thr=0.5):\n",
    "        p = torch.sigmoid(logits)\n",
    "        pred = (p >= thr).long()\n",
    "        table = torch.zeros((2, 2), device=logits.device)\n",
    "        for t, q in zip(y, pred, strict=False):\n",
    "            table[int(t), int(q)] += 1\n",
    "        return pred, p, table\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logit = self(x)\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(logit, y.float())\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logit = self(x)\n",
    "        pred, p, table = self._contingency(logit, y)\n",
    "        self.log(\"val/acc\", self.acc(pred, y), prog_bar=True)\n",
    "        self.log(\"val/auroc\", self.auroc(p, y), prog_bar=True)\n",
    "        self.log(\"val/f1\", self.f1(pred, y))\n",
    "\n",
    "        # Association detectability and effect size:\n",
    "        self.log(\"val/chi_power\", self.chi_power(table))\n",
    "        self.log(\"val/cramers_v\", self.cramersV(table))\n",
    "        self.log(\"val/phi\", self.phi(table))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "model_c = LitClassifier(in_dim=Xn.shape[1])\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    log_every_n_steps=5,\n",
    "    deterministic=True,\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "trainer.fit(model_c, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994ceac",
   "metadata": {},
   "source": [
    "\n",
    "> **Interpretation (ClinTox).**\n",
    "> - *val/chi_power*: detects association between true and predicted labels (beyond raw accuracy).\n",
    "> - *val/cramers_v* / *val/phi*: effect-size of association—0.1 (small), 0.3 (moderate), 0.5 (large) as rough guides.\n",
    "> - *ProportionTwoSamplePower*: plan cohort sizes for minimum clinically meaningful prevalence differences (e.g., 10%).\n",
    "> - If χ² power is high but AUROC is modest, the model captures useful association—optimize thresholding/costs.\n",
    "> - If AUROC is high but χ² power is low, you may be underpowered at this *n* or imbalanced in subgroups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff820c8",
   "metadata": {},
   "source": [
    "### 3.3 Two-sample **proportion power** for prevalence differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbc9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose positive-class prevalence differs between two subgroups by 0.10 (e.g., 0.20 vs 0.30)\n",
    "p1, p2, alpha = 0.20, 0.30, 0.05\n",
    "ns = torch.arange(50, 801, 25)\n",
    "pp = ProportionTwoSamplePower(alpha=alpha)\n",
    "powers = [\n",
    "    float(pp(p1=torch.tensor(p1), p2=torch.tensor(p2), n1=int(n), n2=int(n)))\n",
    "    for n in ns\n",
    "]\n",
    "\n",
    "pd.DataFrame({\"n_per_group\": ns.numpy(), \"power_at_diff=0.10\": powers}).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90791a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 4) Case C — **SKEMPI** (ΔΔG regression; mutational effects)\n",
    "\n",
    "Use **Cohen’s d / Hedges’ g** and **ANOVA power** across mutation classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a607caf",
   "metadata": {},
   "source": [
    "### 4.1 Load & split *(adjust field names if needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb92f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SKEMPIDataset()\n",
    "\n",
    "X = torch.as_tensor(ds.X, dtype=torch.float32)\n",
    "y = torch.as_tensor(\n",
    "    ds.y,\n",
    "    dtype=torch.float32,\n",
    ")  # ΔΔG (kcal/mol). Verify sign convention if needed.\n",
    "\n",
    "Xn = torch.from_numpy(StandardScaler().fit_transform(X.numpy())).float()\n",
    "yn = torch.from_numpy(\n",
    "    StandardScaler().fit_transform(y[:, None].numpy()).squeeze(1),\n",
    ").float()\n",
    "\n",
    "tr, te = train_test_split(torch.arange(len(Xn)), test_size=0.2, random_state=7)\n",
    "train_loader = DataLoader(TensorDataset(Xn[tr], yn[tr]), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(Xn[te], yn[te]), batch_size=128)\n",
    "\n",
    "Xn.shape, yn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d39ce",
   "metadata": {},
   "source": [
    "### 4.2 Lightning regressor with grouped effects and **ANOVA power**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitRegressorGrouped(LightningModule):\n",
    "    def __init__(self, in_dim, lr=1e-3, alpha=0.05):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        self.mse, self.r2 = MeanSquaredError(), R2Score()\n",
    "        self.d, self.g = CohensD(), HedgesG()\n",
    "        self.anova_p = ANOVAPower(alpha=alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = nn.functional.mse_loss(pred, y)\n",
    "        self.log(\"train/mse\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        self.log(\"val/mse\", self.mse(pred, y), prog_bar=True)\n",
    "        self.log(\"val/r2\", self.r2(pred, y), prog_bar=True)\n",
    "        self.log(\"val/d\", self.d(pred, y))\n",
    "        self.log(\"val/g\", self.g(pred, y))\n",
    "\n",
    "        # If SKEMPI exposes true mutation categories, replace this tertile bucketing with ds.category[indices].\n",
    "        q = torch.quantile(y, torch.tensor([0.33, 0.66], device=y.device))\n",
    "        cats = torch.bucketize(y, q)  # 0,1,2 tertiles as a stand-in for classes\n",
    "\n",
    "        group_means = torch.stack([pred[cats == k].mean() for k in (0, 1, 2)])\n",
    "        group_vars = torch.stack(\n",
    "            [pred[cats == k].var(unbiased=True) for k in (0, 1, 2)],\n",
    "        )\n",
    "        group_ns = torch.tensor(\n",
    "            [int((cats == k).sum()) for k in (0, 1, 2)],\n",
    "            device=pred.device,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            self.log(\"val/anova_power\", self.anova_p(group_means, group_vars, group_ns))\n",
    "        except TypeError:\n",
    "            # If ANOVAPower expects raw groups instead of summary stats, adapt here.\n",
    "            pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "model_s = LitRegressorGrouped(in_dim=Xn.shape[1])\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    log_every_n_steps=5,\n",
    "    deterministic=True,\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "trainer.fit(model_s, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92bb869",
   "metadata": {},
   "source": [
    "\n",
    "> **Interpretation (SKEMPI).**\n",
    "> - *val/d*, *val/g*: standardized effect between predictions and truth; *g* is bias-corrected for small samples.\n",
    "> - *val/anova_power*: is between-class separation detectable at current *n*? Great for mutation class stratifications.\n",
    "> - The **threshold → d → n** workflow translates a biological delta (e.g., ΔΔG = 0.5 kcal/mol) to required sample size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f2e61",
   "metadata": {},
   "source": [
    "### 4.3 Convert biological threshold → **required n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose ΔΔG = 0.5 kcal/mol is biologically meaningful.\n",
    "# Estimate Cohen's d from observed SD in original units:\n",
    "with torch.no_grad():\n",
    "    # NOTE: use original-scale y for this calculation\n",
    "    y_val = y[te]\n",
    "    sd = y_val.std()\n",
    "    d_eff = torch.tensor(0.5) / sd\n",
    "\n",
    "# Sample size for 80% power at α=0.05 using the Metric interface:\n",
    "n_needed = TTestSampleSize()(effect_size=d_eff, alpha=0.05, power=0.80)\n",
    "n_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7375ca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5) Power-aware workflow (what to log & when)\n",
    "\n",
    "1. **Before training**: convert meaningful thresholds (ΔTm, ΔΔG, prevalence deltas) → **effect sizes**; compute **required n** with `*SampleSize` metrics.  \n",
    "2. **During validation**: log **effect sizes** and **power** alongside loss/AUC.  \n",
    "3. **After training**: frame claims like *“association is moderate (Cramér’s V≈0.3) and detectable (χ²-power≈0.86) at n=…”* rather than only AUROC/MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fa795",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5) Common pitfalls & best practices\n",
    "\n",
    "- **Confusing significance with importance**: small *p* with *tiny* effect may be irrelevant biologically; report effect sizes.\n",
    "- **Underpowered negatives**: failing to detect ≠ no effect. Always report power achieved for the claim’s threshold.\n",
    "- **Data leakage in power eval**: compute effect/power on held-out validation/test—not on the training set.\n",
    "- **Ignoring stratification**: subgroup effects (e.g., mutation classes) can differ; use ANOVA power and per-stratum reporting.\n",
    "- **Class imbalance**: pair χ² power with prevalence-aware metrics (precision/recall) and use proportion-power for planning.\n",
    "- **Multiple testing**: adjust α for multiple hypotheses or use hierarchical claims; document your correction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8ea91",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Decision checklist (copy/paste for your projects)\n",
    "\n",
    "1. **Define a biologically meaningful threshold** (ΔΔG, ΔTm, prevalence delta, r).  \n",
    "2. **Translate to effect size** (e.g., Cohen’s d or *r*).  \n",
    "3. **Before training**: compute **required n** for 80–90% power at α=0.05.  \n",
    "4. **During validation**: log AUROC/MSE **and** effect sizes **and** power.  \n",
    "5. **After**: write claims that include effect magnitude and detectability at reported *n*.  \n",
    "6. **If power < 0.8**: expand data, reduce noise, or narrow the claim; repeat steps 1–5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab8723",
   "metadata": {},
   "source": [
    "## 6) Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34215b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_learning_curve_regression(\n",
    "    X,\n",
    "    y,\n",
    "    model_ctor,\n",
    "    power_metric,\n",
    "    ns=(50, 100, 150, 200),\n",
    "    repeats=3,\n",
    "):\n",
    "    \"\"\"Subsampled n → power. power_metric(pred, y) should return a scalar tensor.\"\"\"\n",
    "    out, idx_all = [], torch.arange(len(X))\n",
    "    for n in ns:\n",
    "        for r in range(repeats):\n",
    "            idx = idx_all[torch.randperm(len(idx_all))[:n]]\n",
    "            tr, va = train_test_split(torch.arange(n), test_size=0.25, random_state=r)\n",
    "            train_loader = DataLoader(\n",
    "                TensorDataset(X[idx][tr], y[idx][tr]),\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                TensorDataset(X[idx][va], y[idx][va]),\n",
    "                batch_size=128,\n",
    "            )\n",
    "            model = model_ctor()\n",
    "            Trainer(\n",
    "                max_epochs=5,\n",
    "                logger=False,\n",
    "                enable_checkpointing=False,\n",
    "                deterministic=True,\n",
    "            ).fit(model, train_loader, val_loader)\n",
    "            with torch.no_grad():\n",
    "                pred = model(val_loader.dataset.tensors[0])\n",
    "                pwr = float(power_metric(pred, val_loader.dataset.tensors[1]))\n",
    "            out.append({\"n\": int(n), \"repeat\": r, \"power\": pwr})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "def threshold_to_d(delta_units, sd_units):\n",
    "    \"\"\"Convert a meaningful difference in original units (e.g., 2°C Tm, 0.5 kcal/mol ΔΔG) to Cohen's d.\"\"\"\n",
    "    return torch.tensor(delta_units, dtype=torch.float32) / torch.tensor(\n",
    "        sd_units,\n",
    "        dtype=torch.float32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb7b29",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Reporting check-list\n",
    "\n",
    "- **Data adequacy**: “Power to detect r≥0.35 at n=240 is 0.82 (α=0.05).”  \n",
    "- **Effect sizes**: “φ=0.28 (weak–moderate), Cramér’s V=0.31 (moderate).”  \n",
    "- **Limits**: “Power < 0.8 for ΔΔG=0.3 kcal/mol; claims restricted to ≥0.5 kcal/mol shifts.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1f430",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Appendix — FAQ\n",
    "\n",
    "**Q: Why compute Cohen’s d between predictions and ground truth?**  \n",
    "A: It’s a compact, unitless measure of separation. For regression, it complements MSE/R² by summarizing how distinguishable predictions are from the empirical distribution, and it feeds directly into t-test power/sample-size calculations.\n",
    "\n",
    "**Q: My dataset is tiny. Should I still do power analysis?**  \n",
    "A: Especially then. Power tells you what claims are feasible *now* and what additional *n* is needed for target claims.\n",
    "\n",
    "**Q: Are these operators differentiable?**  \n",
    "A: Many effect-size computations are differentiable; the power/sample-size solvers are typically used as diagnostics (not for gradient steps). Log them as metrics.\n",
    "\n",
    "**Q: How do I handle multiple strata?**  \n",
    "A: Use ANOVA (effect size *f*) for global separation; then do targeted pairwise tests (with correction) if you plan granular claims.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a02773",
   "metadata": {},
   "source": [
    "\n",
    "## Reporting templates (drop into papers / PRDs)\n",
    "\n",
    "- **Data adequacy**: *\"For r ≥ 0.35 we require n ≈ 240 for 80% power (α=0.05); our study uses n=260.\"*\n",
    "- **Binary association**: *\"AUROC=0.78; χ²-power=0.86; Cramér’s V=0.31 (moderate). Association is detectable at current n.\"*\n",
    "- **Regression**: *\"R²=0.42; correlation-power=0.81; Cohen’s d=0.62 (medium). Detectable effect at target threshold.\"*\n",
    "- **Limitations**: *\"Power < 0.8 for ΔΔG ≤ 0.3 kcal/mol; conclusions restricted to ≥ 0.5 kcal/mol shifts.\"*\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
